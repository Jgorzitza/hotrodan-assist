#!/usr/bin/env python3
"""
Optimized Inventory Analytics Engine
High-performance analytics for large-scale inventory management
Designed for 1000+ SKU production environments with parallel processing
"""

import os
import sys
import time
import json
import asyncio
import numpy as np
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict
import statistics
from scipy import stats
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

@dataclass
class InventorySkuDemand:
    """Inventory SKU demand data structure"""
    sku_id: str
    sku: str
    current_stock: int
    demand_history: List[float]
    lead_time: int
    service_level: float = 0.95
    cost_per_unit: float = 0.0
    reorder_cost: float = 0.0
    holding_cost_rate: float = 0.2

@dataclass
class VelocityDecile:
    """Velocity decile analysis result"""
    decile: int
    sku_count: int
    avg_velocity: float
    sku_ids: List[str]

@dataclass
class ReorderPoint:
    """Reorder point calculation result"""
    sku_id: str
    sku: str
    current_stock: int
    reorder_point: int
    safety_stock: int
    lead_time_demand: float
    service_level: float
    confidence: float

@dataclass
class DemandForecast:
    """Demand forecast result"""
    sku_id: str
    sku: str
    current_demand: float
    forecasted_demand: List[float]
    confidence: float
    trend: str
    seasonality: float
    next_reorder_date: str
    recommended_order_quantity: int

@dataclass
class VendorPerformance:
    """Vendor performance analysis"""
    vendor_id: str
    vendor_name: str
    total_skus: int
    average_lead_time: float
    on_time_delivery_rate: float
    quality_score: float
    cost_efficiency: float
    overall_score: float
    recommendations: List[str]

@dataclass
class InventoryInsight:
    """Inventory insight/opportunity"""
    id: str
    type: str  # "opportunity", "risk", "optimization"
    priority: str  # "high", "medium", "low"
    title: str
    description: str
    impact: str
    action: str
    sku_ids: List[str]
    estimated_value: float
    confidence: float

class OptimizedInventoryAnalytics:
    """
    High-performance inventory analytics engine with parallel processing
    """
    
    def __init__(self, max_workers: int = 8, cache_size: int = 1000):
        self.max_workers = max_workers
        self.cache_size = cache_size
        self.cache: Dict[str, Any] = {}
        self.performance_metrics = {
            'total_calculations': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'parallel_calculations': 0,
            'avg_processing_time': 0.0,
            'total_processing_time': 0.0
        }
    
    def _get_cache_key(self, operation: str, data_hash: str) -> str:
        """Generate cache key for operation"""
        return f"{operation}_{data_hash}"
    
    def _get_from_cache(self, key: str) -> Optional[Any]:
        """Get result from cache"""
        if key in self.cache:
            self.performance_metrics['cache_hits'] += 1
            return self.cache[key]
        
        self.performance_metrics['cache_misses'] += 1
        return None
    
    def _set_cache(self, key: str, result: Any):
        """Store result in cache"""
        if len(self.cache) >= self.cache_size:
            # Remove oldest entry
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        self.cache[key] = result
    
    def _calculate_velocity_deciles_parallel(self, sku_demands: List[InventorySkuDemand]) -> List[VelocityDecile]:
        """Calculate velocity deciles with parallel processing"""
        start_time = time.time()
        
        # Calculate velocities for all SKUs
        velocities = []
        for sku in sku_demands:
            if sku.demand_history:
                avg_velocity = np.mean(sku.demand_history)
                velocities.append((sku.sku_id, avg_velocity))
        
        if not velocities:
            return []
        
        # Sort by velocity
        velocities.sort(key=lambda x: x[1], reverse=True)
        
        # Calculate deciles
        total_skus = len(velocities)
        deciles = []
        
        for decile in range(1, 11):
            start_idx = int((decile - 1) * total_skus / 10)
            end_idx = int(decile * total_skus / 10)
            
            decile_skus = velocities[start_idx:end_idx]
            avg_velocity = np.mean([v[1] for v in decile_skus]) if decile_skus else 0
            
            deciles.append(VelocityDecile(
                decile=decile,
                sku_count=len(decile_skus),
                avg_velocity=avg_velocity,
                sku_ids=[v[0] for v in decile_skus]
            ))
        
        processing_time = time.time() - start_time
        self.performance_metrics['total_processing_time'] += processing_time
        self.performance_metrics['total_calculations'] += 1
        
        return deciles
    
    def _calculate_reorder_point(self, sku: InventorySkuDemand) -> ReorderPoint:
        """Calculate reorder point for a single SKU"""
        if not sku.demand_history:
            return ReorderPoint(
                sku_id=sku.sku_id,
                sku=sku.sku,
                current_stock=sku.current_stock,
                reorder_point=0,
                safety_stock=0,
                lead_time_demand=0,
                service_level=sku.service_level,
                confidence=0.0
            )
        
        # Calculate demand statistics
        demand_mean = np.mean(sku.demand_history)
        demand_std = np.std(sku.demand_history)
        
        # Lead time demand
        lead_time_demand = demand_mean * sku.lead_time
        
        # Safety stock calculation using z-score
        z_score = stats.norm.ppf(sku.service_level)
        safety_stock = z_score * demand_std * np.sqrt(sku.lead_time)
        
        # Reorder point
        reorder_point = int(lead_time_demand + safety_stock)
        
        # Confidence based on data quality
        confidence = min(1.0, len(sku.demand_history) / 30)  # More data = higher confidence
        
        return ReorderPoint(
            sku_id=sku.sku_id,
            sku=sku.sku,
            current_stock=sku.current_stock,
            reorder_point=reorder_point,
            safety_stock=int(safety_stock),
            lead_time_demand=lead_time_demand,
            service_level=sku.service_level,
            confidence=confidence
        )
    
    def _calculate_reorder_points_parallel(self, sku_demands: List[InventorySkuDemand]) -> List[ReorderPoint]:
        """Calculate reorder points with parallel processing"""
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self._calculate_reorder_point, sku) for sku in sku_demands]
            results = [future.result() for future in as_completed(futures)]
        
        processing_time = time.time() - start_time
        self.performance_metrics['total_processing_time'] += processing_time
        self.performance_metrics['total_calculations'] += 1
        self.performance_metrics['parallel_calculations'] += len(sku_demands)
        
        return results
    
    def _calculate_demand_forecast(self, sku: InventorySkuDemand, periods: int = 12) -> DemandForecast:
        """Calculate demand forecast for a single SKU"""
        if not sku.demand_history or len(sku.demand_history) < 3:
            return DemandForecast(
                sku_id=sku.sku_id,
                sku=sku.sku,
                current_demand=np.mean(sku.demand_history) if sku.demand_history else 0,
                forecasted_demand=[0] * periods,
                confidence=0.0,
                trend="stable",
                seasonality=0.0,
                next_reorder_date="",
                recommended_order_quantity=0
            )
        
        # Prepare data for forecasting
        demand_data = np.array(sku.demand_history)
        X = np.arange(len(demand_data)).reshape(-1, 1)
        y = demand_data
        
        # Linear regression for trend
        model = LinearRegression()
        model.fit(X, y)
        
        # Calculate trend
        trend_slope = model.coef_[0]
        if trend_slope > 0.1:
            trend = "increasing"
        elif trend_slope < -0.1:
            trend = "decreasing"
        else:
            trend = "stable"
        
        # Generate forecast
        future_X = np.arange(len(demand_data), len(demand_data) + periods).reshape(-1, 1)
        forecasted_demand = model.predict(future_X).tolist()
        
        # Calculate confidence based on R-squared
        r_squared = model.score(X, y)
        confidence = max(0.0, min(1.0, r_squared))
        
        # Simple seasonality detection (placeholder)
        seasonality = 0.0  # Could implement more sophisticated seasonality detection
        
        # Calculate next reorder date and quantity
        current_demand = np.mean(demand_data[-3:]) if len(demand_data) >= 3 else np.mean(demand_data)
        next_reorder_date = (datetime.now() + timedelta(days=sku.lead_time)).strftime("%Y-%m-%d")
        recommended_order_quantity = max(0, int(current_demand * sku.lead_time * 1.2))
        
        return DemandForecast(
            sku_id=sku.sku_id,
            sku=sku.sku,
            current_demand=current_demand,
            forecasted_demand=forecasted_demand,
            confidence=confidence,
            trend=trend,
            seasonality=seasonality,
            next_reorder_date=next_reorder_date,
            recommended_order_quantity=recommended_order_quantity
        )
    
    def _calculate_demand_forecasts_parallel(self, sku_demands: List[InventorySkuDemand], periods: int = 12) -> List[DemandForecast]:
        """Calculate demand forecasts with parallel processing"""
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self._calculate_demand_forecast, sku, periods) for sku in sku_demands]
            results = [future.result() for future in as_completed(futures)]
        
        processing_time = time.time() - start_time
        self.performance_metrics['total_processing_time'] += processing_time
        self.performance_metrics['total_calculations'] += 1
        self.performance_metrics['parallel_calculations'] += len(sku_demands)
        
        return results
    
    def _analyze_vendor_performance(self, sku_demands: List[InventorySkuDemand], vendor_data: Dict[str, Any]) -> List[VendorPerformance]:
        """Analyze vendor performance"""
        vendor_stats = defaultdict(lambda: {
            'skus': [],
            'lead_times': [],
            'delivery_rates': [],
            'quality_scores': [],
            'costs': []
        })
        
        # Group SKUs by vendor
        for sku in sku_demands:
            vendor_id = getattr(sku, 'vendor_id', 'unknown')
            vendor_stats[vendor_id]['skus'].append(sku.sku_id)
            vendor_stats[vendor_id]['lead_times'].append(sku.lead_time)
            vendor_stats[vendor_id]['costs'].append(sku.cost_per_unit)
        
        # Calculate vendor performance metrics
        vendor_performances = []
        for vendor_id, stats in vendor_stats.items():
            if not stats['skus']:
                continue
            
            avg_lead_time = np.mean(stats['lead_times'])
            avg_cost = np.mean(stats['costs'])
            total_skus = len(stats['skus'])
            
            # Calculate scores (simplified)
            lead_time_score = max(0, 1 - (avg_lead_time - 7) / 30)  # Penalty for long lead times
            cost_score = max(0, 1 - (avg_cost - 10) / 100)  # Penalty for high costs
            delivery_score = 0.9  # Placeholder - would need actual delivery data
            quality_score = 0.85  # Placeholder - would need actual quality data
            
            overall_score = (lead_time_score + cost_score + delivery_score + quality_score) / 4
            
            # Generate recommendations
            recommendations = []
            if avg_lead_time > 14:
                recommendations.append("Consider alternative vendors with shorter lead times")
            if avg_cost > 50:
                recommendations.append("Negotiate better pricing or find cost-effective alternatives")
            if overall_score < 0.7:
                recommendations.append("Overall vendor performance needs improvement")
            
            vendor_performances.append(VendorPerformance(
                vendor_id=vendor_id,
                vendor_name=vendor_data.get(vendor_id, {}).get('name', f'Vendor {vendor_id}'),
                total_skus=total_skus,
                average_lead_time=avg_lead_time,
                on_time_delivery_rate=delivery_score,
                quality_score=quality_score,
                cost_efficiency=cost_score,
                overall_score=overall_score,
                recommendations=recommendations
            ))
        
        return vendor_performances
    
    def _generate_insights(self, sku_demands: List[InventorySkuDemand], reorder_points: List[ReorderPoint], forecasts: List[DemandForecast]) -> List[InventoryInsight]:
        """Generate inventory insights and opportunities"""
        insights = []
        insight_id = 1
        
        # Low stock insights
        low_stock_skus = [rp for rp in reorder_points if rp.current_stock <= rp.reorder_point]
        if low_stock_skus:
            insights.append(InventoryInsight(
                id=f"insight_{insight_id}",
                type="risk",
                priority="high",
                title="Low Stock Alert",
                description=f"{len(low_stock_skus)} SKUs are at or below reorder point",
                impact="Potential stockouts and lost sales",
                action="Place reorder immediately",
                sku_ids=[sku.sku_id for sku in low_stock_skus],
                estimated_value=len(low_stock_skus) * 1000,  # Estimated value per SKU
                confidence=0.9
            ))
            insight_id += 1
        
        # High velocity opportunities
        high_velocity_skus = [sku for sku in sku_demands if sku.demand_history and np.mean(sku.demand_history) > 10]
        if high_velocity_skus:
            insights.append(InventoryInsight(
                id=f"insight_{insight_id}",
                type="opportunity",
                priority="medium",
                title="High Velocity Products",
                description=f"{len(high_velocity_skus)} SKUs show high demand velocity",
                impact="Opportunity to increase stock levels and sales",
                action="Consider increasing stock levels for high-velocity items",
                sku_ids=[sku.sku_id for sku in high_velocity_skus],
                estimated_value=len(high_velocity_skus) * 500,
                confidence=0.8
            ))
            insight_id += 1
        
        # Overstock risks
        overstock_skus = [sku for sku in sku_demands if sku.demand_history and np.mean(sku.demand_history) < 1 and sku.current_stock > 50]
        if overstock_skus:
            insights.append(InventoryInsight(
                id=f"insight_{insight_id}",
                type="risk",
                priority="medium",
                title="Overstock Risk",
                description=f"{len(overstock_skus)} SKUs may be overstocked",
                impact="Excess inventory holding costs",
                action="Consider reducing stock levels or running promotions",
                sku_ids=[sku.sku_id for sku in overstock_skus],
                estimated_value=len(overstock_skus) * -200,  # Negative value for cost
                confidence=0.7
            ))
            insight_id += 1
        
        return insights
    
    def analyze_inventory(self, sku_demands: List[InventorySkuDemand], vendor_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Comprehensive inventory analysis with parallel processing
        """
        start_time = time.time()
        
        if not sku_demands:
            return {
                'velocity_deciles': [],
                'reorder_points': [],
                'demand_forecasts': [],
                'vendor_performance': [],
                'insights': [],
                'performance_metrics': self.get_performance_metrics()
            }
        
        # Calculate all analytics in parallel where possible
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all major calculations
            velocity_future = executor.submit(self._calculate_velocity_deciles_parallel, sku_demands)
            reorder_future = executor.submit(self._calculate_reorder_points_parallel, sku_demands)
            forecast_future = executor.submit(self._calculate_demand_forecasts_parallel, sku_demands)
            
            # Wait for results
            velocity_deciles = velocity_future.result()
            reorder_points = reorder_future.result()
            demand_forecasts = forecast_future.result()
        
        # Calculate vendor performance and insights
        vendor_performance = self._analyze_vendor_performance(sku_demands, vendor_data or {})
        insights = self._generate_insights(sku_demands, reorder_points, demand_forecasts)
        
        # Update performance metrics
        total_time = time.time() - start_time
        self.performance_metrics['total_processing_time'] += total_time
        self.performance_metrics['total_calculations'] += 1
        self.performance_metrics['avg_processing_time'] = (
            self.performance_metrics['total_processing_time'] / 
            self.performance_metrics['total_calculations']
        )
        
        return {
            'velocity_deciles': velocity_deciles,
            'reorder_points': reorder_points,
            'demand_forecasts': demand_forecasts,
            'vendor_performance': vendor_performance,
            'insights': insights,
            'performance_metrics': self.get_performance_metrics(),
            'processing_time': total_time,
            'sku_count': len(sku_demands)
        }
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get current performance metrics"""
        cache_hit_rate = (
            self.performance_metrics['cache_hits'] / 
            max(self.performance_metrics['total_calculations'], 1) * 100
        )
        
        return {
            **self.performance_metrics,
            'cache_hit_rate': cache_hit_rate,
            'cache_size': len(self.cache),
            'max_workers': self.max_workers,
            'parallel_efficiency': (
                self.performance_metrics['parallel_calculations'] / 
                max(self.performance_metrics['total_calculations'], 1) * 100
            )
        }
    
    def clear_cache(self):
        """Clear the analytics cache"""
        self.cache.clear()
        print("âœ… Analytics cache cleared")
    
    def optimize_for_scale(self, sku_count: int):
        """Optimize configuration for large-scale operations"""
        if sku_count > 1000:
            self.max_workers = min(16, sku_count // 100)
            self.cache_size = min(5000, sku_count * 2)
        elif sku_count > 500:
            self.max_workers = min(8, sku_count // 50)
            self.cache_size = min(2000, sku_count * 2)
        
        print(f"âœ… Optimized for {sku_count} SKUs: "
              f"{self.max_workers} workers, "
              f"{self.cache_size} cache size")

# Example usage and testing
if __name__ == "__main__":
    # Create sample data
    sample_skus = []
    for i in range(100):
        sku = InventorySkuDemand(
            sku_id=f"SKU{i:03d}",
            sku=f"Product {i}",
            current_stock=np.random.randint(0, 100),
            demand_history=[np.random.poisson(5) for _ in range(30)],
            lead_time=np.random.randint(3, 21),
            service_level=0.95,
            cost_per_unit=np.random.uniform(10, 100),
            reorder_cost=50,
            holding_cost_rate=0.2
        )
        sample_skus.append(sku)
    
    # Initialize analytics engine
    analytics = OptimizedInventoryAnalytics(max_workers=8)
    
    # Run analysis
    print("ðŸš€ Running inventory analysis...")
    results = analytics.analyze_inventory(sample_skus)
    
    # Print results summary
    print(f"\nðŸ“Š Analysis Results:")
    print(f"  SKUs analyzed: {results['sku_count']}")
    print(f"  Processing time: {results['processing_time']:.2f}s")
    print(f"  Velocity deciles: {len(results['velocity_deciles'])}")
    print(f"  Reorder points: {len(results['reorder_points'])}")
    print(f"  Demand forecasts: {len(results['demand_forecasts'])}")
    print(f"  Vendor performance: {len(results['vendor_performance'])}")
    print(f"  Insights: {len(results['insights'])}")
    
    # Print performance metrics
    metrics = analytics.get_performance_metrics()
    print(f"\nâš¡ Performance Metrics:")
    print(f"  Cache hit rate: {metrics['cache_hit_rate']:.1f}%")
    print(f"  Parallel efficiency: {metrics['parallel_efficiency']:.1f}%")
    print(f"  Avg processing time: {metrics['avg_processing_time']:.3f}s")
