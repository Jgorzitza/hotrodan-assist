{
  "timestamp": "2025-09-29T00:34:08.006146",
  "audit_duration": "0:15:33.266745",
  "python_quality": {
    "black": {
      "status": "FAIL",
      "details": "Formatting issues found: --- /home/justin/llama_rag/app/connectors/shopify.py\t2025-09-28 22:56:20.765924+00:00\n+++ /home/justin/llama_rag/app/connectors/shopify.py\t2025-09-29 06:34:08.268777+00:00\n@@ -6,74 +6,77 @@\n from typing import Any, Dict, Optional\n from datetime import datetime\n \n from .base import BaseConnector, ConnectorConfig, ConnectorStatus, ConnectorError\n \n+\n class ShopifyConnector(BaseConnector):\n     def __init__(self, config: Optional[ConnectorConfig] = None):\n         if config is None:\n             config = self._create_default_config()\n         super().__init__(config)\n-    \n+\n     def _create_default_config(self) -> ConnectorConfig:\n-        shop = os.getenv('SHOPIFY_SHOP')\n-        access_token = os.getenv('SHOPIFY_ACCESS_TOKEN')\n-        use_mock = os.getenv('USE_MOCK_DATA', 'true').lower() == 'true'\n-        \n+        shop = os.getenv(\"SHOPIFY_SHOP\")\n+        access_token = os.getenv(\"SHOPIFY_ACCESS_TOKEN\")\n+        use_mock = os.getenv(\"USE_MOCK_DATA\", \"true\").lower() == \"true\"\n+\n         if not shop and not use_mock:\n             raise ConnectorError(\"SHOPIFY_SHOP environment variable required\")\n         if not access_token and not use_mock:\n             raise ConnectorError(\"SHOPIFY_ACCESS_TOKEN environment variable required\")\n-        \n+\n         return ConnectorConfig(\n             name=\"shopify\",\n             status=ConnectorStatus.MOCK if use_mock else ConnectorStatus.LIVE,\n             api_key=access_token,\n-            base_url=f\"https://{shop}.myshopify.com/admin/api/2023-10\" if shop else None,\n-            mock_data_path=\"app/connectors/mock_data/shopify\"\n+            base_url=(\n+                f\"https://{shop}.myshopify.com/admin/api/2023-10\" if shop else None\n+            ),\n+            mock_data_path=\"app/connectors/mock_data/shopify\",\n         )\n-    \n+\n     def test_connection(self) -> Dict[str, Any]:\n         try:\n             if self.is_mock_mode:\n                 return {\n                     \"status\": \"success\",\n                     \"message\": \"Mock mode - connection test skipped\",\n-                    \"shop\": \"mock-shop.myshopify.com\"\n+                    \"shop\": \"mock-shop.myshopify.com\",\n                 }\n-            \n+\n             # In real implementation, would test actual API\n             return {\n                 \"status\": \"success\",\n                 \"message\": \"Connection successful\",\n-                \"shop\": \"real-shop.myshopify.com\"\n+                \"shop\": \"real-shop.myshopify.com\",\n             }\n         except Exception as e:\n             return {\n                 \"status\": \"error\",\n                 \"message\": f\"Connection failed: {str(e)}\",\n-                \"error_type\": type(e).__name__\n+                \"error_type\": type(e).__name__,\n             }\n-    \n+\n     def get_health_status(self) -> Dict[str, Any]:\n         try:\n             test_result = self.test_connection()\n             return {\n                 \"healthy\": test_result[\"status\"] == \"success\",\n                 \"status\": test_result[\"status\"],\n                 \"message\": test_result[\"message\"],\n-                \"last_check\": datetime.now().isoformat()\n+                \"last_check\": datetime.now().isoformat(),\n             }\n         except Exception as e:\n             return {\n                 \"healthy\": False,\n                 \"status\": \"error\",\n                 \"message\": f\"Health check failed: {str(e)}\",\n-                \"last_check\": datetime.now().isoformat()\n+                \"last_check\": datetime.now().isoformat(),\n             }\n-    \n+\n     def get_orders(self, limit: int = 50) -> Dict[str, Any]:\n         if self.is_mock_mode:\n             return self._load_mock_data(\"orders\") or {\"orders\": []}\n-        \n+\n         # In real implementation, would call Shopify API\n         return {\"orders\": []}\n--- /home/justin/llama_rag/app/connectors/main.py\t2025-09-29 06:22:18.629218+00:00\n+++ /home/justin/llama_rag/app/connectors/main.py\t2025-09-29 06:34:08.294519+00:00\n@@ -6,11 +6,11 @@\n from datetime import datetime\n \n app = FastAPI(\n     title=\"MCP Connectors API\",\n     description=\"Mock MCP connectors for inventory integration\",\n-    version=\"1.0.0\"\n+    version=\"1.0.0\",\n )\n \n # Add CORS middleware\n app.add_middleware(\n     CORSMiddleware,\n@@ -21,96 +21,104 @@\n )\n \n # Mock data directory\n MOCK_DATA_DIR = os.path.join(os.path.dirname(__file__), \"mock_data\")\n \n+\n @app.get(\"/health\")\n async def health_check():\n     \"\"\"Health check endpoint\"\"\"\n     return {\n         \"status\": \"healthy\",\n         \"service\": \"MCP Connectors API\",\n         \"timestamp\": datetime.now().isoformat(),\n-        \"version\": \"1.0.0\"\n+        \"version\": \"1.0.0\",\n     }\n+\n \n @app.get(\"/\")\n async def root():\n     \"\"\"Root endpoint\"\"\"\n     return {\n         \"service\": \"MCP Connectors API\",\n         \"version\": \"1.0.0\",\n         \"status\": \"operational\",\n-        \"timestamp\": datetime.now().isoformat()\n+        \"timestamp\": datetime.now().isoformat(),\n     }\n+\n \n @app.get(\"/shopify/products\")\n async def get_shopify_products():\n     \"\"\"Mock Shopify products endpoint\"\"\"\n     try:\n         mock_file = os.path.join(MOCK_DATA_DIR, \"shopify_products.json\")\n         if os.path.exists(mock_file):\n-            with open(mock_file, 'r') as f:\n+            with open(mock_file, \"r\") as f:\n                 return json.load(f)\n         else:\n             # Return mock data if file doesn't exist\n             return {\n                 \"products\": [\n                     {\n                         \"id\": \"mock-1\",\n                         \"title\": \"Sample Product 1\",\n                         \"inventory_quantity\": 100,\n                         \"price\": 29.99,\n-                        \"vendor\": \"Mock Vendor 1\"\n+                        \"vendor\": \"Mock Vendor 1\",\n                     },\n                     {\n-                        \"id\": \"mock-2\", \n+                        \"id\": \"mock-2\",\n                         \"title\": \"Sample Product 2\",\n                         \"inventory_quantity\": 50,\n                         \"price\": 49.99,\n-                        \"vendor\": \"Mock Vendor 2\"\n-                    }\n+                        \"vendor\": \"Mock Vendor 2\",\n+                    },\n                 ]\n             }\n     except Exception as e:\n-        raise HTTPException(status_code=500, detail=f\"Error loading Shopify data: {str(e)}\")\n+        raise HTTPException(\n+            status_code=500, detail=f\"Error loading Shopify data: {str(e)}\"\n+        )\n+\n \n @app.get(\"/ga4/traffic-summary\")\n async def get_ga4_traffic():\n     \"\"\"Mock GA4 traffic summary endpoint\"\"\"\n     try:\n         mock_file = os.path.join(MOCK_DATA_DIR, \"ga4_traffic.json\")\n         if os.path.exists(mock_file):\n-            with open(mock_file, 'r') as f:\n+            with open(mock_file, \"r\") as f:\n                 return json.load(f)\n         else:\n             # Return mock data if file doesn't exist\n             return {\n                 \"sessions\": 1250,\n                 \"page_views\": 3400,\n                 \"bounce_rate\": 0.35,\n-                \"avg_session_duration\": 180\n+                \"avg_session_duration\": 180,\n             }\n     except Exception as e:\n         raise HTTPException(status_code=500, detail=f\"Error loading GA4 data: {str(e)}\")\n+\n \n @app.get(\"/gsc/search-queries\")\n async def get_gsc_queries():\n     \"\"\"Mock GSC search queries endpoint\"\"\"\n     try:\n         mock_file = os.path.join(MOCK_DATA_DIR, \"gsc_queries.json\")\n         if os.path.exists(mock_file):\n-            with open(mock_file, 'r') as f:\n+            with open(mock_file, \"r\") as f:\n                 return json.load(f)\n         else:\n             # Return mock data if file doesn't exist\n             return {\n                 \"queries\": [\n                     {\"query\": \"sample product\", \"clicks\": 45, \"impressions\": 120},\n-                    {\"query\": \"inventory management\", \"clicks\": 23, \"impressions\": 67}\n+                    {\"query\": \"inventory management\", \"clicks\": 23, \"impressions\": 67},\n                 ]\n             }\n     except Exception as e:\n         raise HTTPException(status_code=500, detail=f\"Error loading GSC data: {str(e)}\")\n \n+\n if __name__ == \"__main__\":\n     uvicorn.run(app, host=\"0.0.0.0\", port=8003)\n--- /home/justin/llama_rag/app/seo-api/analytics/__init__.py\t2025-09-29 04:03:06.209169+00:00\n+++ /home/justin/llama_rag/app/seo-api/analytics/__init__.py\t2025-09-29 06:34:08.297027+00:00\n@@ -1,32 +1,43 @@\n \"\"\"\n SEO Advanced Analytics Platform\n \n Comprehensive SEO analytics with predictive capabilities:\n - Predictive analytics and forecasting\n-- Real-time dashboard and monitoring  \n+- Real-time dashboard and monitoring\n - Intelligent alerting and anomaly detection\n - Advanced reporting and insights\n - Competitive intelligence\n \"\"\"\n \n from .main import SEOAdvancedAnalyticsPlatform\n-from .predictive_analytics import PredictiveSEOAnalytics, SEOForecast, CompetitiveAnalysis, SEOTrend\n+from .predictive_analytics import (\n+    PredictiveSEOAnalytics,\n+    SEOForecast,\n+    CompetitiveAnalysis,\n+    SEOTrend,\n+)\n from .dashboard_analytics import SEOAnalyticsDashboard, DashboardMetric, ChartData\n-from .intelligent_alerts import IntelligentAlertSystem, Alert, AlertRule, AlertSeverity, AlertType\n+from .intelligent_alerts import (\n+    IntelligentAlertSystem,\n+    Alert,\n+    AlertRule,\n+    AlertSeverity,\n+    AlertType,\n+)\n \n __version__ = \"1.0.0\"\n __all__ = [\n     \"SEOAdvancedAnalyticsPlatform\",\n-    \"PredictiveSEOAnalytics\", \n+    \"PredictiveSEOAnalytics\",\n     \"SEOForecast\",\n     \"CompetitiveAnalysis\",\n     \"SEOTrend\",\n     \"SEOAnalyticsDashboard\",\n     \"DashboardMetric\",\n     \"ChartData\",\n     \"IntelligentAlertSystem\",\n     \"Alert\",\n-    \"AlertRule\", \n+    \"AlertRule\",\n     \"AlertSeverity\",\n-    \"AlertType\"\n+    \"AlertType\",\n ]\n--- /home/justin/llama_rag/app/connectors/zoho.py\t2025-09-29 06:31:23.221240+00:00\n+++ /home/justin/llama_rag/app/connectors/zoho.py\t2025-09-29 06:34:08.452543+00:00\n@@ -2,286 +2,293 @@\n Production-optimized Zoho Mail API connector for MCP integrations.\n \"\"\"\n \n from datetime import datetime, timedelta\n from typing import Any, Dict, List, Optional\n-from .base import BaseConnector, ConnectorConfig, ConnectorError, ConnectorResponse, ConnectorStatus\n+from .base import (\n+    BaseConnector,\n+    ConnectorConfig,\n+    ConnectorError,\n+    ConnectorResponse,\n+    ConnectorStatus,\n+)\n \n \n class ZohoConnector(BaseConnector):\n     \"\"\"Production-optimized Zoho Mail API connector with resilience patterns.\"\"\"\n-    \n+\n     def __init__(self, config: ConnectorConfig) -> None:\n         super().__init__(config)\n         self.base_url = \"https://mail.zoho.com/api\" if config.endpoint else None\n-        self.headers = {\n-            \"Authorization\": f\"Zoho-oauthtoken {config.api_key}\",\n-            \"Content-Type\": \"application/json\",\n-            \"User-Agent\": \"MCP-Connector/1.0\"\n-        } if config.api_key else {}\n-    \n+        self.headers = (\n+            {\n+                \"Authorization\": f\"Zoho-oauthtoken {config.api_key}\",\n+                \"Content-Type\": \"application/json\",\n+                \"User-Agent\": \"MCP-Connector/1.0\",\n+            }\n+            if config.api_key\n+            else {}\n+        )\n+\n     async def health_check(self) -> ConnectorResponse:\n         \"\"\"Check if Zoho Mail API is accessible with production monitoring.\"\"\"\n         if self.config.use_mocks:\n             return self._create_response(\n                 ConnectorStatus.SUCCESS,\n                 {\n                     \"status\": \"healthy\",\n                     \"mode\": \"mock\",\n                     \"timestamp\": datetime.utcnow().isoformat(),\n-                    \"api_version\": \"v1\"\n+                    \"api_version\": \"v1\",\n                 },\n-                \"Zoho Mail connector in mock mode\"\n-            )\n-        \n+                \"Zoho Mail connector in mock mode\",\n+            )\n+\n         if not self.base_url:\n             return self._create_response(\n                 ConnectorStatus.ERROR,\n                 None,\n                 \"Zoho Mail endpoint not configured\",\n-                \"MISSING_ENDPOINT\"\n-            )\n-        \n+                \"MISSING_ENDPOINT\",\n+            )\n+\n         async def _health_check_operation() -> ConnectorResponse:\n             client = await self._get_http_client()\n             response = await client.get(\n-                f\"{self.base_url}/accounts\",\n-                headers=self.headers\n-            )\n-            \n+                f\"{self.base_url}/accounts\", headers=self.headers\n+            )\n+\n             if response.status_code == 200:\n                 account_data = response.json()\n                 return self._create_response(\n                     ConnectorStatus.SUCCESS,\n                     {\n                         \"accounts\": account_data.get(\"data\", []),\n                         \"mode\": \"live\",\n                         \"response_time_ms\": response.elapsed.total_seconds() * 1000,\n-                        \"timestamp\": datetime.utcnow().isoformat()\n+                        \"timestamp\": datetime.utcnow().isoformat(),\n                     },\n-                    \"Zoho Mail API accessible\"\n+                    \"Zoho Mail API accessible\",\n                 )\n             else:\n                 raise self._handle_http_error(response.status_code, response.text)\n-        \n+\n         try:\n-            return await self._execute_with_resilience(\"health_check\", _health_check_operation)\n+            return await self._execute_with_resilience(\n+                \"health_check\", _health_check_operation\n+            )\n         except ConnectorError:\n             raise\n         except Exception as e:\n             raise ConnectorError(\n                 f\"Unexpected error checking Zoho Mail health: {str(e)}\",\n-                original_error=e\n-            )\n-    \n+                original_error=e,\n+            )\n+\n     async def test_connection(self) -> ConnectorResponse:\n         \"\"\"Test the Zoho Mail connection with production monitoring.\"\"\"\n         if self.config.use_mocks:\n             return self._create_response(\n                 ConnectorStatus.SUCCESS,\n                 {\n                     \"test\": \"passed\",\n                     \"mode\": \"mock\",\n-                    \"timestamp\": datetime.utcnow().isoformat()\n+                    \"timestamp\": datetime.utcnow().isoformat(),\n                 },\n-                \"Zoho Mail connection test passed (mock mode)\"\n-            )\n-        \n+                \"Zoho Mail connection test passed (mock mode)\",\n+            )\n+\n         async def _test_connection_operation() -> ConnectorResponse:\n             client = await self._get_http_client()\n             response = await client.get(\n-                f\"{self.base_url}/accounts\",\n-                headers=self.headers\n-            )\n-            \n+                f\"{self.base_url}/accounts\", headers=self.headers\n+            )\n+\n             if response.status_code == 200:\n                 account_data = response.json().get(\"data\", [])\n                 return self._create_response(\n                     ConnectorStatus.SUCCESS,\n                     {\n                         \"account_count\": len(account_data),\n-                        \"accounts\": [acc.get(\"accountName\") for acc in account_data[:3]],\n+                        \"accounts\": [\n+                            acc.get(\"accountName\") for acc in account_data[:3]\n+                        ],\n                         \"mode\": \"live\",\n                         \"response_time_ms\": response.elapsed.total_seconds() * 1000,\n-                        \"timestamp\": datetime.utcnow().isoformat()\n+                        \"timestamp\": datetime.utcnow().isoformat(),\n                     },\n-                    \"Zoho Mail connection successful\"\n+                    \"Zoho Mail connection successful\",\n                 )\n             else:\n                 raise self._handle_http_error(response.status_code, response.text)\n-        \n+\n         try:\n-            return await self._execute_with_resilience(\"test_connection\", _test_connection_operation)\n+            return await self._execute_with_resilience(\n+                \"test_connection\", _test_connection_operation\n+            )\n         except ConnectorError:\n             raise\n         except Exception as e:\n             raise ConnectorError(\n-                f\"Zoho Mail connection test failed: {str(e)}\",\n-                original_error=e\n-            )\n-    \n+                f\"Zoho Mail connection test failed: {str(e)}\", original_error=e\n+            )\n+\n     async def get_inbound_emails(\n         self,\n         account_id: str,\n         limit: int = 50,\n         folder_id: str = \"inbox\",\n         from_date: Optional[datetime] = None,\n-        to_date: Optional[datetime] = None\n+        to_date: Optional[datetime] = None,\n     ) -> ConnectorResponse:\n         \"\"\"Fetch inbound emails from Zoho Mail with caching and production optimizations.\"\"\"\n         if self.config.use_mocks:\n             return self._create_response(\n                 ConnectorStatus.SUCCESS,\n                 self._get_mock_emails(),\n-                \"Inbound emails fetched (mock mode)\"\n-            )\n-        \n+                \"Inbound emails fetched (mock mode)\",\n+            )\n+\n         # Check cache first\n         cache_key = self._get_cache_key(\n             \"get_inbound_emails\",\n             account_id=account_id,\n             limit=limit,\n             folder_id=folder_id,\n             from_date=from_date.isoformat() if from_date else None,\n-            to_date=to_date.isoformat() if to_date else None\n+            to_date=to_date.isoformat() if to_date else None,\n         )\n-        \n+\n         cached_data = self._get_from_cache(cache_key)\n         if cached_data:\n             self.logger.info(\"Returning cached inbound emails data\")\n             return self._create_response(\n                 ConnectorStatus.SUCCESS,\n                 cached_data,\n-                \"Inbound emails fetched from cache\"\n-            )\n-        \n+                \"Inbound emails fetched from cache\",\n+            )\n+\n         async def _get_inbound_emails_operation() -> ConnectorResponse:\n             params = {\n                 \"accountId\": account_id,\n                 \"folderId\": folder_id,\n-                \"limit\": min(limit, 100)\n+                \"limit\": min(limit, 100),\n             }\n-            \n+\n             if from_date:\n                 params[\"fromDate\"] = from_date.isoformat()\n             if to_date:\n                 params[\"toDate\"] = to_date.isoformat()\n-            \n+\n             client = await self._get_http_client()\n             response = await client.get(\n-                f\"{self.base_url}/messages\",\n-                headers=self.headers,\n-                params=params\n-            )\n-            \n+                f\"{self.base_url}/messages\", headers=self.headers, params=params\n+            )\n+\n             if response.status_code == 200:\n                 emails_data = response.json()\n                 emails = emails_data.get(\"data\", [])\n-                \n+\n                 # Cache the results\n                 self._set_cache(cache_key, emails)\n-                \n+\n                 return self._create_response(\n                     ConnectorStatus.SUCCESS,\n                     {\n                         \"emails\": emails,\n                         \"count\": len(emails),\n                         \"response_time_ms\": response.elapsed.total_seconds() * 1000,\n-                        \"timestamp\": datetime.utcnow().isoformat()\n+                        \"timestamp\": datetime.utcnow().isoformat(),\n                     },\n-                    f\"Fetched {len(emails)} inbound emails\"\n+                    f\"Fetched {len(emails)} inbound emails\",\n                 )\n             else:\n                 raise self._handle_http_error(response.status_code, response.text)\n-        \n+\n         try:\n-            return await self._execute_with_resilience(\"get_inbound_emails\", _get_inbound_emails_operation)\n+            return await self._execute_with_resilience(\n+                \"get_inbound_emails\", _get_inbound_emails_operation\n+            )\n         except ConnectorError:\n             raise\n         except Exception as e:\n             raise ConnectorError(\n-                f\"Failed to fetch inbound emails: {str(e)}\",\n-                original_error=e\n-            )\n-    \n+                f\"Failed to fetch inbound emails: {str(e)}\", original_error=e\n+            )\n+\n     async def get_outbound_status(\n-        self,\n-        account_id: str,\n-        message_id: Optional[str] = None,\n-        limit: int = 50\n+        self, account_id: str, message_id: Optional[str] = None, limit: int = 50\n     ) -> ConnectorResponse:\n         \"\"\"Get outbound email status from Zoho Mail with production optimizations.\"\"\"\n         if self.config.use_mocks:\n             return self._create_response(\n                 ConnectorStatus.SUCCESS,\n                 self._get_mock_outbound_status(),\n-                \"Outbound status fetched (mock mode)\"\n-            )\n-        \n+                \"Outbound status fetched (mock mode)\",\n+            )\n+\n         # Check cache first\n         cache_key = self._get_cache_key(\n             \"get_outbound_status\",\n             account_id=account_id,\n             message_id=message_id,\n-            limit=limit\n+            limit=limit,\n         )\n-        \n+\n         cached_data = self._get_from_cache(cache_key)\n         if cached_data:\n             self.logger.info(\"Returning cached outbound status data\")\n             return self._create_response(\n                 ConnectorStatus.SUCCESS,\n                 cached_data,\n-                \"Outbound status fetched from cache\"\n-            )\n-        \n+                \"Outbound status fetched from cache\",\n+            )\n+\n         async def _get_outbound_status_operation() -> ConnectorResponse:\n-            params = {\n-                \"accountId\": account_id,\n-                \"limit\": min(limit, 100)\n-            }\n-            \n+            params = {\"accountId\": account_id, \"limit\": min(limit, 100)}\n+\n             if message_id:\n                 params[\"messageId\"] = message_id\n-            \n+\n             client = await self._get_http_client()\n             response = await client.get(\n                 f\"{self.base_url}/messages/outbound\",\n                 headers=self.headers,\n-                params=params\n-            )\n-            \n+                params=params,\n+            )\n+\n             if response.status_code == 200:\n                 status_data = response.json()\n                 statuses = status_data.get(\"data\", [])\n-                \n+\n                 # Cache the results\n                 self._set_cache(cache_key, statuses)\n-                \n+\n                 return self._create_response(\n                     ConnectorStatus.SUCCESS,\n                     {\n                         \"statuses\": statuses,\n                         \"count\": len(statuses),\n                         \"response_time_ms\": response.elapsed.total_seconds() * 1000,\n-                        \"timestamp\": datetime.utcnow().isoformat()\n+                        \"timestamp\": datetime.utcnow().isoformat(),\n                     },\n-                    f\"Fetched {len(statuses)} outbound statuses\"\n+                    f\"Fetched {len(statuses)} outbound statuses\",\n                 )\n             else:\n                 raise self._handle_http_error(response.status_code, response.text)\n-        \n+\n         try:\n-            return await self._execute_with_resilience(\"get_outbound_status\", _get_outbound_status_operation)\n+            return await self._execute_with_resilience(\n+                \"get_outbound_status\", _get_outbound_status_operation\n+            )\n         except ConnectorError:\n             raise\n         except Exception as e:\n             raise ConnectorError(\n-                f\"Failed to fetch outbound status: {str(e)}\",\n-                original_error=e\n-            )\n-    \n+                f\"Failed to fetch outbound status: {str(e)}\", original_error=e\n+            )\n+\n     def _get_mock_emails(self) -> List[Dict[str, Any]]:\n         \"\"\"Generate mock inbound emails data for development.\"\"\"\n         return [\n             {\n                 \"messageId\": \"msg_001\",\n@@ -290,42 +297,44 @@\n                 \"to\": \"orders@company.com\",\n                 \"receivedTime\": (datetime.utcnow() - timedelta(hours=2)).isoformat(),\n                 \"folderId\": \"inbox\",\n                 \"isRead\": False,\n                 \"priority\": \"high\",\n-                \"body\": \"New order #1001 has been placed for $99.99\"\n+                \"body\": \"New order #1001 has been placed for $99.99\",\n             },\n             {\n                 \"messageId\": \"msg_002\",\n                 \"subject\": \"Payment Confirmation\",\n                 \"from\": \"payments@stripe.com\",\n                 \"to\": \"finance@company.com\",\n                 \"receivedTime\": (datetime.utcnow() - timedelta(hours=1)).isoformat(),\n                 \"folderId\": \"inbox\",\n                 \"isRead\": True,\n                 \"priority\": \"normal\",\n-                \"body\": \"Payment of $149.99 has been processed successfully\"\n-            }\n+                \"body\": \"Payment of $149.99 has been processed successfully\",\n+            },\n         ]\n-    \n+\n     def _get_mock_outbound_status(self) -> List[Dict[str, Any]]:\n         \"\"\"Generate mock outbound status data for development.\"\"\"\n         return [\n             {\n                 \"messageId\": \"out_001\",\n                 \"subject\": \"Order Confirmation\",\n                 \"to\": \"customer@example.com\",\n                 \"sentTime\": (datetime.utcnow() - timedelta(hours=3)).isoformat(),\n                 \"status\": \"delivered\",\n-                \"deliveryTime\": (datetime.utcnow() - timedelta(hours=2, minutes=45)).isoformat(),\n-                \"readStatus\": \"read\"\n+                \"deliveryTime\": (\n+                    datetime.utcnow() - timedelta(hours=2, minutes=45)\n+                ).isoformat(),\n+                \"readStatus\": \"read\",\n             },\n             {\n                 \"messageId\": \"out_002\",\n                 \"subject\": \"Shipping Update\",\n                 \"to\": \"customer2@example.com\",\n                 \"sentTime\": (datetime.utcnow() - timedelta(hours=1)).isoformat(),\n                 \"status\": \"delivered\",\n                 \"deliveryTime\": (datetime.utcnow() - timedelta(minutes=45)).isoformat(),\n-                \"readStatus\": \"unread\"\n-            }\n+                \"readStatus\": \"unread\",\n+            },\n         ]\n--- /home/justin/llama_rag/app/seo-api/analytics/dashboard_analytics.py\t2025-09-29 06:32:16.985252+00:00\n+++ /home/justin/llama_rag/app/seo-api/analytics/dashboard_analytics.py\t2025-09-29 06:34:08.647190+00:00\n@@ -20,406 +20,470 @@\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n @dataclass\n class DashboardMetric:\n     \"\"\"Dashboard metric data point.\"\"\"\n+\n     metric_name: str\n     current_value: float\n     previous_value: float\n     change_percent: float\n     trend: str\n     status: str\n     target_value: Optional[float] = None\n     last_updated: str = \"\"\n \n+\n @dataclass\n class ChartData:\n     \"\"\"Chart data for visualization.\"\"\"\n+\n     chart_type: str\n     title: str\n     data: List[Dict[str, Any]]\n     x_axis: str\n     y_axis: str\n     colors: List[str]\n     created_at: str\n \n+\n @dataclass\n class AlertRule:\n     \"\"\"Alert rule configuration.\"\"\"\n+\n     rule_id: str\n     metric_name: str\n     condition: str\n     threshold: float\n     severity: str\n     enabled: bool\n     created_at: str\n \n+\n @dataclass\n class DashboardConfig:\n     \"\"\"Dashboard configuration.\"\"\"\n+\n     dashboard_id: str\n     name: str\n     widgets: List[Dict[str, Any]]\n     filters: Dict[str, Any]\n     refresh_interval: int\n     theme: str\n     created_at: str\n \n+\n class SEOAnalyticsDashboard:\n     \"\"\"Advanced SEO analytics dashboard.\"\"\"\n-    \n+\n     def __init__(self):\n         self.metrics = {}\n         self.charts = {}\n         self.alerts = []\n         self.configs = {}\n         self.logger = logging.getLogger(self.__class__.__name__)\n-    \n+\n     def add_metric(self, metric: DashboardMetric):\n         \"\"\"Add or update a dashboard metric.\"\"\"\n         self.metrics[metric.metric_name] = metric\n         self.logger.info(f\"Updated metric: {metric.metric_name}\")\n-    \n+\n     def get_metric(self, metric_name: str) -> Optional[DashboardMetric]:\n         \"\"\"Get a specific metric.\"\"\"\n         return self.metrics.get(metric_name)\n-    \n+\n     def get_all_metrics(self) -> List[DashboardMetric]:\n         \"\"\"Get all metrics.\"\"\"\n         return list(self.metrics.values())\n-    \n-    def create_chart(self, chart_type: str, title: str, data: List[Dict[str, Any]], \n-                    x_axis: str, y_axis: str, colors: List[str] = None) -> ChartData:\n+\n+    def create_chart(\n+        self,\n+        chart_type: str,\n+        title: str,\n+        data: List[Dict[str, Any]],\n+        x_axis: str,\n+        y_axis: str,\n+        colors: List[str] = None,\n+    ) -> ChartData:\n         \"\"\"Create a chart for the dashboard.\"\"\"\n-        \n+\n         if colors is None:\n             colors = [\"#3498db\", \"#e74c3c\", \"#2ecc71\", \"#f39c12\", \"#9b59b6\"]\n-        \n+\n         chart = ChartData(\n             chart_type=chart_type,\n             title=title,\n             data=data,\n             x_axis=x_axis,\n             y_axis=y_axis,\n             colors=colors,\n-            created_at=datetime.now().isoformat()\n-        )\n-        \n+            created_at=datetime.now().isoformat(),\n+        )\n+\n         chart_id = f\"{chart_type}_{len(self.charts)}\"\n         self.charts[chart_id] = chart\n-        \n+\n         self.logger.info(f\"Created chart: {title}\")\n         return chart\n-    \n+\n     def create_traffic_chart(self, days: int = 30) -> ChartData:\n         \"\"\"Create organic traffic trend chart.\"\"\"\n-        \n+\n         # Generate mock traffic data\n-        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(days, 0, -1)]\n+        dates = [\n+            (datetime.now() - timedelta(days=i)).strftime(\"%Y-%m-%d\")\n+            for i in range(days, 0, -1)\n+        ]\n         traffic_data = []\n-        \n+\n         base_traffic = 1000\n         for i, date in enumerate(dates):\n             # Add some realistic variation\n             variation = np.random.normal(0, 0.1)\n-            traffic = int(base_traffic * (1 + variation + (i * 0.02)))  # Slight growth trend\n-            \n-            traffic_data.append({\n-                \"date\": date,\n-                \"traffic\": traffic,\n-                \"sessions\": int(traffic * 1.2),\n-                \"bounce_rate\": round(np.random.uniform(0.4, 0.7), 2)\n-            })\n-        \n+            traffic = int(\n+                base_traffic * (1 + variation + (i * 0.02))\n+            )  # Slight growth trend\n+\n+            traffic_data.append(\n+                {\n+                    \"date\": date,\n+                    \"traffic\": traffic,\n+                    \"sessions\": int(traffic * 1.2),\n+                    \"bounce_rate\": round(np.random.uniform(0.4, 0.7), 2),\n+                }\n+            )\n+\n         return self.create_chart(\n             chart_type=\"line\",\n             title=\"Organic Traffic Trend\",\n             data=traffic_data,\n             x_axis=\"date\",\n             y_axis=\"traffic\",\n-            colors=[\"#3498db\"]\n-        )\n-    \n+            colors=[\"#3498db\"],\n+        )\n+\n     def create_ranking_chart(self, keywords: List[str]) -> ChartData:\n         \"\"\"Create keyword ranking chart.\"\"\"\n-        \n+\n         ranking_data = []\n         for keyword in keywords:\n             # Generate mock ranking data\n             current_rank = np.random.randint(1, 50)\n             previous_rank = current_rank + np.random.randint(-5, 5)\n-            \n-            ranking_data.append({\n-                \"keyword\": keyword,\n-                \"current_rank\": current_rank,\n-                \"previous_rank\": previous_rank,\n-                \"change\": current_rank - previous_rank,\n-                \"traffic_potential\": np.random.randint(100, 2000)\n-            })\n-        \n+\n+            ranking_data.append(\n+                {\n+                    \"keyword\": keyword,\n+                    \"current_rank\": current_rank,\n+                    \"previous_rank\": previous_rank,\n+                    \"change\": current_rank - previous_rank,\n+                    \"traffic_potential\": np.random.randint(100, 2000),\n+                }\n+            )\n+\n         return self.create_chart(\n             chart_type=\"bar\",\n             title=\"Keyword Rankings\",\n             data=ranking_data,\n             x_axis=\"keyword\",\n             y_axis=\"current_rank\",\n-            colors=[\"#2ecc71\", \"#e74c3c\"]\n-        )\n-    \n+            colors=[\"#2ecc71\", \"#e74c3c\"],\n+        )\n+\n     def create_competitor_chart(self, competitors: List[str]) -> ChartData:\n         \"\"\"Create competitor analysis chart.\"\"\"\n-        \n+\n         competitor_data = []\n         for competitor in competitors:\n             market_share = np.random.uniform(0.1, 0.4)\n             keyword_overlap = np.random.uniform(0.2, 0.8)\n-            \n-            competitor_data.append({\n-                \"competitor\": competitor,\n-                \"market_share\": round(market_share * 100, 1),\n-                \"keyword_overlap\": round(keyword_overlap * 100, 1),\n-                \"threat_level\": \"High\" if market_share > 0.3 else \"Medium\" if market_share > 0.15 else \"Low\"\n-            })\n-        \n+\n+            competitor_data.append(\n+                {\n+                    \"competitor\": competitor,\n+                    \"market_share\": round(market_share * 100, 1),\n+                    \"keyword_overlap\": round(keyword_overlap * 100, 1),\n+                    \"threat_level\": (\n+                        \"High\"\n+                        if market_share > 0.3\n+                        else \"Medium\" if market_share > 0.15 else \"Low\"\n+                    ),\n+                }\n+            )\n+\n         return self.create_chart(\n             chart_type=\"scatter\",\n             title=\"Competitor Analysis\",\n             data=competitor_data,\n             x_axis=\"market_share\",\n             y_axis=\"keyword_overlap\",\n-            colors=[\"#e74c3c\", \"#f39c12\", \"#2ecc71\"]\n-        )\n-    \n+            colors=[\"#e74c3c\", \"#f39c12\", \"#2ecc71\"],\n+        )\n+\n     def create_performance_chart(self, metrics: List[str]) -> ChartData:\n         \"\"\"Create performance metrics chart.\"\"\"\n-        \n+\n         performance_data = []\n         for metric in metrics:\n             current_value = np.random.uniform(50, 100)\n             previous_value = current_value + np.random.uniform(-10, 10)\n             change_percent = ((current_value - previous_value) / previous_value) * 100\n-            \n-            performance_data.append({\n-                \"metric\": metric,\n-                \"current_value\": round(current_value, 1),\n-                \"previous_value\": round(previous_value, 1),\n-                \"change_percent\": round(change_percent, 1),\n-                \"status\": \"improving\" if change_percent > 0 else \"declining\"\n-            })\n-        \n+\n+            performance_data.append(\n+                {\n+                    \"metric\": metric,\n+                    \"current_value\": round(current_value, 1),\n+                    \"previous_value\": round(previous_value, 1),\n+                    \"change_percent\": round(change_percent, 1),\n+                    \"status\": \"improving\" if change_percent > 0 else \"declining\",\n+                }\n+            )\n+\n         return self.create_chart(\n             chart_type=\"radar\",\n             title=\"Performance Metrics\",\n             data=performance_data,\n             x_axis=\"metric\",\n             y_axis=\"current_value\",\n-            colors=[\"#9b59b6\"]\n-        )\n-    \n+            colors=[\"#9b59b6\"],\n+        )\n+\n     def add_alert_rule(self, rule: AlertRule):\n         \"\"\"Add an alert rule.\"\"\"\n         self.alerts.append(rule)\n         self.logger.info(f\"Added alert rule: {rule.rule_id}\")\n-    \n+\n     def check_alerts(self) -> List[Dict[str, Any]]:\n         \"\"\"Check all alert rules and return triggered alerts.\"\"\"\n-        \n+\n         triggered_alerts = []\n-        \n+\n         for rule in self.alerts:\n             if not rule.enabled:\n                 continue\n-            \n+\n             metric = self.get_metric(rule.metric_name)\n             if not metric:\n                 continue\n-            \n+\n             # Check alert condition\n             triggered = False\n-            if rule.condition == \"greater_than\" and metric.current_value > rule.threshold:\n+            if (\n+                rule.condition == \"greater_than\"\n+                and metric.current_value > rule.threshold\n+            ):\n                 triggered = True\n-            elif rule.condition == \"less_than\" and metric.current_value < rule.threshold:\n+            elif (\n+                rule.condition == \"less_than\" and metric.current_value < rule.threshold\n+            ):\n                 triggered = True\n-            elif rule.condition == \"change_greater_than\" and abs(metric.change_percent) > rule.threshold:\n+            elif (\n+                rule.condition == \"change_greater_than\"\n+                and abs(metric.change_percent) > rule.threshold\n+            ):\n                 triggered = True\n-            \n+\n             if triggered:\n                 alert = {\n                     \"rule_id\": rule.rule_id,\n                     \"metric_name\": rule.metric_name,\n                     \"current_value\": metric.current_value,\n                     \"threshold\": rule.threshold,\n                     \"severity\": rule.severity,\n                     \"message\": f\"Alert: {rule.metric_name} {rule.condition} {rule.threshold}\",\n-                    \"triggered_at\": datetime.now().isoformat()\n+                    \"triggered_at\": datetime.now().isoformat(),\n                 }\n                 triggered_alerts.append(alert)\n-        \n+\n         return triggered_alerts\n-    \n-    def create_dashboard_config(self, name: str, widgets: List[Dict[str, Any]], \n-                              filters: Dict[str, Any] = None) -> DashboardConfig:\n+\n+    def create_dashboard_config(\n+        self, name: str, widgets: List[Dict[str, Any]], filters: Dict[str, Any] = None\n+    ) -> DashboardConfig:\n         \"\"\"Create a dashboard configuration.\"\"\"\n-        \n+\n         if filters is None:\n             filters = {\n                 \"date_range\": \"30d\",\n                 \"keywords\": [],\n                 \"competitors\": [],\n-                \"metrics\": []\n+                \"metrics\": [],\n             }\n-        \n+\n         config = DashboardConfig(\n             dashboard_id=f\"dashboard_{len(self.configs)}\",\n             name=name,\n             widgets=widgets,\n             filters=filters,\n             refresh_interval=300,  # 5 minutes\n             theme=\"light\",\n-            created_at=datetime.now().isoformat()\n-        )\n-        \n+            created_at=datetime.now().isoformat(),\n+        )\n+\n         self.configs[config.dashboard_id] = config\n         self.logger.info(f\"Created dashboard config: {name}\")\n-        \n+\n         return config\n-    \n+\n     def generate_dashboard_data(self) -> Dict[str, Any]:\n         \"\"\"Generate complete dashboard data.\"\"\"\n-        \n+\n         # Create sample metrics\n         sample_metrics = [\n             DashboardMetric(\n                 metric_name=\"organic_traffic\",\n                 current_value=15420,\n                 previous_value=14200,\n                 change_percent=8.6,\n                 trend=\"up\",\n                 status=\"good\",\n                 target_value=20000,\n-                last_updated=datetime.now().isoformat()\n+                last_updated=datetime.now().isoformat(),\n             ),\n             DashboardMetric(\n                 metric_name=\"average_rank\",\n                 current_value=12.3,\n                 previous_value=14.1,\n                 change_percent=-12.8,\n                 trend=\"up\",\n                 status=\"excellent\",\n                 target_value=10.0,\n-                last_updated=datetime.now().isoformat()\n+                last_updated=datetime.now().isoformat(),\n             ),\n             DashboardMetric(\n                 metric_name=\"click_through_rate\",\n                 current_value=3.2,\n                 previous_value=2.9,\n                 change_percent=10.3,\n                 trend=\"up\",\n                 status=\"good\",\n                 target_value=4.0,\n-                last_updated=datetime.now().isoformat()\n-            )\n+                last_updated=datetime.now().isoformat(),\n+            ),\n         ]\n-        \n+\n         for metric in sample_metrics:\n             self.add_metric(metric)\n-        \n+\n         # Create charts\n-        \n+\n         # Create alert rules\n         alert_rules = [\n             AlertRule(\n                 rule_id=\"traffic_drop\",\n                 metric_name=\"organic_traffic\",\n                 condition=\"change_greater_than\",\n                 threshold=-10.0,\n                 severity=\"warning\",\n                 enabled=True,\n-                created_at=datetime.now().isoformat()\n+                created_at=datetime.now().isoformat(),\n             ),\n             AlertRule(\n                 rule_id=\"rank_drop\",\n                 metric_name=\"average_rank\",\n                 condition=\"greater_than\",\n                 threshold=20.0,\n                 severity=\"critical\",\n                 enabled=True,\n-                created_at=datetime.now().isoformat()\n-            )\n+                created_at=datetime.now().isoformat(),\n+            ),\n         ]\n-        \n+\n         for rule in alert_rules:\n             self.add_alert_rule(rule)\n-        \n+\n         # Check alerts\n         triggered_alerts = self.check_alerts()\n-        \n+\n         # Create dashboard config\n         widgets = [\n-            {\"type\": \"metric\", \"metric\": \"organic_traffic\", \"position\": {\"x\": 0, \"y\": 0}},\n-            {\"type\": \"chart\", \"chart_id\": \"traffic_chart\", \"position\": {\"x\": 0, \"y\": 1}},\n-            {\"type\": \"chart\", \"chart_id\": \"ranking_chart\", \"position\": {\"x\": 1, \"y\": 0}},\n-            {\"type\": \"chart\", \"chart_id\": \"competitor_chart\", \"position\": {\"x\": 1, \"y\": 1}}\n+            {\n+                \"type\": \"metric\",\n+                \"metric\": \"organic_traffic\",\n+                \"position\": {\"x\": 0, \"y\": 0},\n+            },\n+            {\n+                \"type\": \"chart\",\n+                \"chart_id\": \"traffic_chart\",\n+                \"position\": {\"x\": 0, \"y\": 1},\n+            },\n+            {\n+                \"type\": \"chart\",\n+                \"chart_id\": \"ranking_chart\",\n+                \"position\": {\"x\": 1, \"y\": 0},\n+            },\n+            {\n+                \"type\": \"chart\",\n+                \"chart_id\": \"competitor_chart\",\n+                \"position\": {\"x\": 1, \"y\": 1},\n+            },\n         ]\n-        \n-        dashboard_config = self.create_dashboard_config(\"SEO Analytics Dashboard\", widgets)\n-        \n+\n+        dashboard_config = self.create_dashboard_config(\n+            \"SEO Analytics Dashboard\", widgets\n+        )\n+\n         return {\n             \"dashboard_id\": dashboard_config.dashboard_id,\n             \"name\": dashboard_config.name,\n             \"metrics\": [asdict(m) for m in self.get_all_metrics()],\n             \"charts\": {k: asdict(v) for k, v in self.charts.items()},\n             \"alerts\": triggered_alerts,\n             \"config\": asdict(dashboard_config),\n-            \"generated_at\": datetime.now().isoformat()\n+            \"generated_at\": datetime.now().isoformat(),\n         }\n-    \n+\n     def export_dashboard_data(self, format: str = \"json\") -> str:\n         \"\"\"Export dashboard data in specified format.\"\"\"\n-        \n+\n         data = self.generate_dashboard_data()\n-        \n+\n         if format == \"json\":\n             return json.dumps(data, indent=2, default=str)\n         elif format == \"csv\":\n             # Convert to CSV format\n             df = pd.DataFrame(data[\"metrics\"])\n             return df.to_csv(index=False)\n         else:\n             raise ValueError(f\"Unsupported format: {format}\")\n \n+\n def main():\n     \"\"\"Main function to demonstrate SEO analytics dashboard.\"\"\"\n-    \n+\n     # Initialize dashboard\n     dashboard = SEOAnalyticsDashboard()\n-    \n+\n     print(\"\ud83d\ude80 SEO Analytics Dashboard - DEMO\")\n     print(\"=\" * 40)\n-    \n+\n     # Generate dashboard data\n     data = dashboard.generate_dashboard_data()\n-    \n+\n     print(f\"Dashboard: {data['name']}\")\n     print(f\"Metrics: {len(data['metrics'])}\")\n     print(f\"Charts: {len(data['charts'])}\")\n     print(f\"Alerts: {len(data['alerts'])}\")\n-    \n+\n     # Export data\n     json_data = dashboard.export_dashboard_data(\"json\")\n-    \n+\n     with open(\"seo_dashboard_data.json\", \"w\") as f:\n         f.write(json_data)\n-    \n+\n     print(\"\\nFiles created:\")\n     print(\"- seo_dashboard_data.json\")\n-    \n+\n     # Print sample metrics\n     print(\"\\nSample Metrics:\")\n     for metric in data[\"metrics\"]:\n-        print(f\"- {metric['metric_name']}: {metric['current_value']} ({metric['change_percent']:+.1f}%)\")\n+        print(\n+            f\"- {metric['metric_name']}: {metric['current_value']} ({metric['change_percent']:+.1f}%)\"\n+        )\n+\n \n if __name__ == \"__main__\":\n     main()\n--- /home/justin/llama_rag/app/seo-api/analytics/main.py\t2025-09-29 06:31:23.225240+00:00\n+++ /home/justin/llama_rag/app/seo-api/analytics/main.py\t2025-09-29 06:34:08.664772+00:00\n@@ -17,31 +17,37 @@\n from datetime import datetime, timedelta\n from dataclasses import asdict\n \n from .predictive_analytics import PredictiveSEOAnalytics, AdvancedAnalyticsReport\n from .dashboard_analytics import SEOAnalyticsDashboard\n-from .intelligent_alerts import IntelligentAlertSystem, AlertRule, AlertType, AlertSeverity\n+from .intelligent_alerts import (\n+    IntelligentAlertSystem,\n+    AlertRule,\n+    AlertType,\n+    AlertSeverity,\n+)\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n class SEOAdvancedAnalyticsPlatform:\n     \"\"\"Main SEO Advanced Analytics Platform.\"\"\"\n-    \n+\n     def __init__(self):\n         self.predictive_analytics = PredictiveSEOAnalytics()\n         self.dashboard = SEOAnalyticsDashboard()\n         self.alert_system = IntelligentAlertSystem()\n         self.logger = logging.getLogger(self.__class__.__name__)\n-        \n+\n         # Initialize default alert rules\n         self._setup_default_alerts()\n-    \n+\n     def _setup_default_alerts(self):\n         \"\"\"Setup default alert rules.\"\"\"\n-        \n+\n         default_rules = [\n             AlertRule(\n                 rule_id=\"traffic_drop\",\n                 name=\"Organic Traffic Drop\",\n                 alert_type=AlertType.PERFORMANCE_DROP,\n@@ -52,11 +58,11 @@\n                 enabled=True,\n                 keywords=[],\n                 pages=[],\n                 cooldown_minutes=60,\n                 escalation_minutes=240,\n-                created_at=datetime.now().isoformat()\n+                created_at=datetime.now().isoformat(),\n             ),\n             AlertRule(\n                 rule_id=\"ranking_drop\",\n                 name=\"Keyword Ranking Drop\",\n                 alert_type=AlertType.RANKING_DROP,\n@@ -67,11 +73,11 @@\n                 enabled=True,\n                 keywords=[],\n                 pages=[],\n                 cooldown_minutes=30,\n                 escalation_minutes=120,\n-                created_at=datetime.now().isoformat()\n+                created_at=datetime.now().isoformat(),\n             ),\n             AlertRule(\n                 rule_id=\"traffic_spike\",\n                 name=\"Traffic Spike Detection\",\n                 alert_type=AlertType.TRAFFIC_SPIKE,\n@@ -82,267 +88,327 @@\n                 enabled=True,\n                 keywords=[],\n                 pages=[],\n                 cooldown_minutes=30,\n                 escalation_minutes=60,\n-                created_at=datetime.now().isoformat()\n-            )\n+                created_at=datetime.now().isoformat(),\n+            ),\n         ]\n-        \n+\n         for rule in default_rules:\n             self.alert_system.add_alert_rule(rule)\n-    \n+\n     def add_historical_data(self, keyword: str, data: List[Dict[str, Any]]):\n         \"\"\"Add historical data for predictive analytics.\"\"\"\n         self.predictive_analytics.add_historical_data(keyword, data)\n-    \n+\n     def add_metric_data(self, metric_name: str, value: float, timestamp: str = None):\n         \"\"\"Add metric data for monitoring and alerting.\"\"\"\n         self.alert_system.add_metric_data(metric_name, value, timestamp)\n         self.dashboard.add_metric_data(metric_name, value, timestamp)\n-    \n-    def generate_comprehensive_report(self, \n-                                    keywords: List[str], \n-                                    competitors: List[str],\n-                                    metrics: Dict[str, List[float]]) -> Dict[str, Any]:\n+\n+    def generate_comprehensive_report(\n+        self,\n+        keywords: List[str],\n+        competitors: List[str],\n+        metrics: Dict[str, List[float]],\n+    ) -> Dict[str, Any]:\n         \"\"\"Generate comprehensive analytics report.\"\"\"\n-        \n+\n         # Generate predictive analytics report\n         predictive_report = self.predictive_analytics.generate_advanced_report(\n             keywords, competitors, metrics\n         )\n-        \n+\n         # Generate dashboard data\n         dashboard_data = self.dashboard.generate_dashboard_data()\n-        \n+\n         # Check for alerts\n         rule_alerts = self.alert_system.check_alert_rules()\n         anomaly_alerts = self.alert_system.check_anomaly_alerts()\n-        \n+\n         # Generate alert summary\n         alert_summary = self.alert_system.generate_alert_summary()\n-        \n+\n         # Combine all data\n         comprehensive_report = {\n             \"report_id\": f\"seo_advanced_analytics_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n             \"generated_at\": datetime.now().isoformat(),\n             \"period\": {\n                 \"start\": (datetime.now() - timedelta(days=90)).isoformat(),\n-                \"end\": datetime.now().isoformat()\n+                \"end\": datetime.now().isoformat(),\n             },\n             \"predictive_analytics\": asdict(predictive_report),\n             \"dashboard\": dashboard_data,\n             \"alerts\": {\n                 \"rule_alerts\": [asdict(alert) for alert in rule_alerts],\n                 \"anomaly_alerts\": [asdict(alert) for alert in anomaly_alerts],\n-                \"summary\": alert_summary\n+                \"summary\": alert_summary,\n             },\n             \"insights\": self._generate_insights(predictive_report, alert_summary),\n-            \"recommendations\": self._generate_recommendations(predictive_report, alert_summary)\n+            \"recommendations\": self._generate_recommendations(\n+                predictive_report, alert_summary\n+            ),\n         }\n-        \n+\n         return comprehensive_report\n-    \n-    def _generate_insights(self, predictive_report: AdvancedAnalyticsReport, alert_summary: Dict[str, Any]) -> List[str]:\n+\n+    def _generate_insights(\n+        self, predictive_report: AdvancedAnalyticsReport, alert_summary: Dict[str, Any]\n+    ) -> List[str]:\n         \"\"\"Generate key insights from analytics data.\"\"\"\n-        \n+\n         insights = []\n-        \n+\n         # Predictive insights\n-        high_opportunity_keywords = [f for f in predictive_report.forecasts if f.opportunity_score > 0.7]\n+        high_opportunity_keywords = [\n+            f for f in predictive_report.forecasts if f.opportunity_score > 0.7\n+        ]\n         if high_opportunity_keywords:\n-            insights.append(f\"\ud83c\udfaf {len(high_opportunity_keywords)} high-opportunity keywords identified with strong growth potential\")\n-        \n+            insights.append(\n+                f\"\ud83c\udfaf {len(high_opportunity_keywords)} high-opportunity keywords identified with strong growth potential\"\n+            )\n+\n         # Alert insights\n         if alert_summary[\"active_alerts\"] > 0:\n-            insights.append(f\"\u26a0\ufe0f {alert_summary['active_alerts']} active alerts requiring attention\")\n-        \n-        critical_alerts = alert_summary[\"alerts_by_severity\"][\"critical\"] + alert_summary[\"alerts_by_severity\"][\"emergency\"]\n+            insights.append(\n+                f\"\u26a0\ufe0f {alert_summary['active_alerts']} active alerts requiring attention\"\n+            )\n+\n+        critical_alerts = (\n+            alert_summary[\"alerts_by_severity\"][\"critical\"]\n+            + alert_summary[\"alerts_by_severity\"][\"emergency\"]\n+        )\n         if critical_alerts > 0:\n-            insights.append(f\"\ud83d\udea8 {critical_alerts} critical alerts requiring immediate action\")\n-        \n+            insights.append(\n+                f\"\ud83d\udea8 {critical_alerts} critical alerts requiring immediate action\"\n+            )\n+\n         # Trend insights\n-        positive_trends = [t for t in predictive_report.trends if t.trend_direction == \"up\"]\n+        positive_trends = [\n+            t for t in predictive_report.trends if t.trend_direction == \"up\"\n+        ]\n         if positive_trends:\n-            insights.append(f\"\ud83d\udcc8 {len(positive_trends)} positive trends detected indicating growth momentum\")\n-        \n+            insights.append(\n+                f\"\ud83d\udcc8 {len(positive_trends)} positive trends detected indicating growth momentum\"\n+            )\n+\n         # Competitive insights\n-        high_threat_competitors = [c for c in predictive_report.competitive_analysis if c.threat_level == \"High\"]\n+        high_threat_competitors = [\n+            c\n+            for c in predictive_report.competitive_analysis\n+            if c.threat_level == \"High\"\n+        ]\n         if high_threat_competitors:\n-            insights.append(f\"\ud83c\udfc6 {len(high_threat_competitors)} high-threat competitors identified requiring strategic response\")\n-        \n+            insights.append(\n+                f\"\ud83c\udfc6 {len(high_threat_competitors)} high-threat competitors identified requiring strategic response\"\n+            )\n+\n         return insights\n-    \n-    def _generate_recommendations(self, predictive_report: AdvancedAnalyticsReport, alert_summary: Dict[str, Any]) -> List[str]:\n+\n+    def _generate_recommendations(\n+        self, predictive_report: AdvancedAnalyticsReport, alert_summary: Dict[str, Any]\n+    ) -> List[str]:\n         \"\"\"Generate actionable recommendations.\"\"\"\n-        \n+\n         recommendations = []\n-        \n+\n         # High-priority recommendations\n         if alert_summary[\"active_alerts\"] > 5:\n-            recommendations.append(\"\ud83d\udd27 Address active alerts to improve overall SEO performance\")\n-        \n+            recommendations.append(\n+                \"\ud83d\udd27 Address active alerts to improve overall SEO performance\"\n+            )\n+\n         if alert_summary[\"alerts_by_severity\"][\"critical\"] > 0:\n-            recommendations.append(\"\ud83d\udea8 Resolve critical alerts immediately to prevent further damage\")\n-        \n+            recommendations.append(\n+                \"\ud83d\udea8 Resolve critical alerts immediately to prevent further damage\"\n+            )\n+\n         # Keyword recommendations\n-        high_opportunity_keywords = [f for f in predictive_report.forecasts if f.opportunity_score > 0.7]\n+        high_opportunity_keywords = [\n+            f for f in predictive_report.forecasts if f.opportunity_score > 0.7\n+        ]\n         if high_opportunity_keywords:\n-            recommendations.append(\"\ud83c\udfaf Focus on high-opportunity keywords for maximum ROI\")\n-        \n+            recommendations.append(\n+                \"\ud83c\udfaf Focus on high-opportunity keywords for maximum ROI\"\n+            )\n+\n         # Content recommendations\n-        recommendations.extend([\n-            \"\ud83d\udcdd Develop comprehensive content clusters around target keywords\",\n-            \"\ud83d\udd17 Build high-quality backlinks to improve domain authority\",\n-            \"\u26a1 Optimize page loading speed and mobile experience\",\n-            \"\ud83d\udcca Monitor competitor activities and adapt strategies accordingly\"\n-        ])\n-        \n+        recommendations.extend(\n+            [\n+                \"\ud83d\udcdd Develop comprehensive content clusters around target keywords\",\n+                \"\ud83d\udd17 Build high-quality backlinks to improve domain authority\",\n+                \"\u26a1 Optimize page loading speed and mobile experience\",\n+                \"\ud83d\udcca Monitor competitor activities and adapt strategies accordingly\",\n+            ]\n+        )\n+\n         # Technical recommendations\n-        recommendations.extend([\n-            \"\ud83d\udd0d Implement structured data markup for better search visibility\",\n-            \"\ud83d\udcf1 Ensure mobile-first indexing compliance\",\n-            \"\ud83c\udf10 Optimize for featured snippets and voice search\",\n-            \"\ud83d\udcc8 Set up advanced tracking and monitoring systems\"\n-        ])\n-        \n+        recommendations.extend(\n+            [\n+                \"\ud83d\udd0d Implement structured data markup for better search visibility\",\n+                \"\ud83d\udcf1 Ensure mobile-first indexing compliance\",\n+                \"\ud83c\udf10 Optimize for featured snippets and voice search\",\n+                \"\ud83d\udcc8 Set up advanced tracking and monitoring systems\",\n+            ]\n+        )\n+\n         return recommendations\n-    \n+\n     def export_report(self, report: Dict[str, Any], format: str = \"json\") -> str:\n         \"\"\"Export report in specified format.\"\"\"\n-        \n+\n         if format == \"json\":\n             return json.dumps(report, indent=2, default=str)\n         elif format == \"markdown\":\n             return self._generate_markdown_report(report)\n         else:\n             raise ValueError(f\"Unsupported format: {format}\")\n-    \n+\n     def _generate_markdown_report(self, report: Dict[str, Any]) -> str:\n         \"\"\"Generate markdown report.\"\"\"\n-        \n+\n         md_content = f\"\"\"# SEO Advanced Analytics Report\n \n **Report ID**: {report['report_id']}  \n **Generated**: {report['generated_at']}  \n **Period**: {report['period']['start']} to {report['period']['end']}\n \n ## \ud83c\udfaf Key Insights\n \n \"\"\"\n-        \n-        for insight in report['insights']:\n+\n+        for insight in report[\"insights\"]:\n             md_content += f\"- {insight}\\n\"\n-        \n+\n         md_content += \"\\n## \ud83d\udcca Predictive Analytics\\n\\n\"\n-        \n+\n         # Forecasts\n         md_content += \"### Keyword Forecasts\\n\\n\"\n-        for forecast in report['predictive_analytics']['forecasts']:\n+        for forecast in report[\"predictive_analytics\"][\"forecasts\"]:\n             md_content += f\"**{forecast['keyword']}**\\n\"\n             md_content += f\"- Current Rank: {forecast['current_rank']}\\n\"\n             md_content += f\"- Predicted 30d: {forecast['predicted_rank_30d']}\\n\"\n             md_content += f\"- Predicted 90d: {forecast['predicted_rank_90d']}\\n\"\n             md_content += f\"- Opportunity Score: {forecast['opportunity_score']:.2f}\\n\"\n             md_content += f\"- Traffic Forecast: {forecast['traffic_forecast']}\\n\\n\"\n-        \n+\n         # Alerts\n         md_content += \"## \u26a0\ufe0f Active Alerts\\n\\n\"\n-        alert_summary = report['alerts']['summary']\n+        alert_summary = report[\"alerts\"][\"summary\"]\n         md_content += f\"- **Total Alerts**: {alert_summary['total_alerts']}\\n\"\n         md_content += f\"- **Active Alerts**: {alert_summary['active_alerts']}\\n\"\n         md_content += f\"- **Critical Alerts**: {alert_summary['alerts_by_severity']['critical']}\\n\"\n         md_content += f\"- **Warning Alerts**: {alert_summary['alerts_by_severity']['warning']}\\n\\n\"\n-        \n+\n         # Recommendations\n         md_content += \"## \ud83d\ude80 Recommendations\\n\\n\"\n-        for i, rec in enumerate(report['recommendations'], 1):\n+        for i, rec in enumerate(report[\"recommendations\"], 1):\n             md_content += f\"{i}. {rec}\\n\"\n-        \n+\n         return md_content\n-    \n+\n     def run_continuous_monitoring(self, interval_minutes: int = 5):\n         \"\"\"Run continuous monitoring and alerting.\"\"\"\n-        \n-        self.logger.info(f\"Starting continuous monitoring (interval: {interval_minutes} minutes)\")\n-        \n+\n+        self.logger.info(\n+            f\"Starting continuous monitoring (interval: {interval_minutes} minutes)\"\n+        )\n+\n         while True:\n             try:\n                 # Check for alerts\n                 rule_alerts = self.alert_system.check_alert_rules()\n                 anomaly_alerts = self.alert_system.check_anomaly_alerts()\n-                \n+\n                 if rule_alerts or anomaly_alerts:\n-                    self.logger.info(f\"Generated {len(rule_alerts)} rule alerts and {len(anomaly_alerts)} anomaly alerts\")\n-                \n+                    self.logger.info(\n+                        f\"Generated {len(rule_alerts)} rule alerts and {len(anomaly_alerts)} anomaly alerts\"\n+                    )\n+\n                 # Wait for next check\n                 asyncio.sleep(interval_minutes * 60)\n-                \n+\n             except KeyboardInterrupt:\n                 self.logger.info(\"Stopping continuous monitoring\")\n                 break\n             except Exception as e:\n                 self.logger.error(f\"Error in continuous monitoring: {e}\")\n                 asyncio.sleep(60)  # Wait 1 minute before retrying\n \n+\n def main():\n     \"\"\"Main function to demonstrate SEO Advanced Analytics Platform.\"\"\"\n-    \n+\n     # Initialize platform\n     platform = SEOAdvancedAnalyticsPlatform()\n-    \n+\n     print(\"\ud83d\ude80 SEO Advanced Analytics Platform - DEMO\")\n     print(\"=\" * 50)\n-    \n+\n     # Sample data\n-    keywords = [\"seo best practices\", \"content marketing\", \"keyword research\", \"link building\", \"technical seo\"]\n-    competitors = [\"competitor1.com\", \"competitor2.com\", \"competitor3.com\", \"competitor4.com\"]\n-    \n+    keywords = [\n+        \"seo best practices\",\n+        \"content marketing\",\n+        \"keyword research\",\n+        \"link building\",\n+        \"technical seo\",\n+    ]\n+    competitors = [\n+        \"competitor1.com\",\n+        \"competitor2.com\",\n+        \"competitor3.com\",\n+        \"competitor4.com\",\n+    ]\n+\n     # Sample metrics\n     metrics = {\n         \"organic_traffic\": [1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900],\n         \"average_rank\": [15, 14, 13, 12, 11, 10, 9, 8, 7, 6],\n         \"click_through_rate\": [2.5, 2.7, 2.9, 3.1, 3.3, 3.5, 3.7, 3.9, 4.1, 4.3],\n-        \"conversion_rate\": [1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1]\n+        \"conversion_rate\": [1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1],\n     }\n-    \n+\n     # Add sample metric data\n     for metric_name, values in metrics.items():\n         for i, value in enumerate(values):\n-            timestamp = (datetime.now() - timedelta(days=len(values)-i)).isoformat()\n+            timestamp = (datetime.now() - timedelta(days=len(values) - i)).isoformat()\n             platform.add_metric_data(metric_name, value, timestamp)\n-    \n+\n     # Generate comprehensive report\n     report = platform.generate_comprehensive_report(keywords, competitors, metrics)\n-    \n+\n     print(f\"Report ID: {report['report_id']}\")\n     print(f\"Forecasts: {len(report['predictive_analytics']['forecasts'])}\")\n-    print(f\"Competitive Analysis: {len(report['predictive_analytics']['competitive_analysis'])}\")\n+    print(\n+        f\"Competitive Analysis: {len(report['predictive_analytics']['competitive_analysis'])}\"\n+    )\n     print(f\"Trends: {len(report['predictive_analytics']['trends'])}\")\n     print(f\"Active Alerts: {report['alerts']['summary']['active_alerts']}\")\n     print(f\"Insights: {len(report['insights'])}\")\n     print(f\"Recommendations: {len(report['recommendations'])}\")\n-    \n+\n     # Export reports\n     json_report = platform.export_report(report, \"json\")\n     markdown_report = platform.export_report(report, \"markdown\")\n-    \n+\n     with open(\"seo_advanced_analytics_report.json\", \"w\") as f:\n         f.write(json_report)\n-    \n+\n     with open(\"seo_advanced_analytics_report.md\", \"w\") as f:\n         f.write(markdown_report)\n-    \n+\n     print(\"\\nFiles created:\")\n     print(\"- seo_advanced_analytics_report.json\")\n     print(\"- seo_advanced_analytics_report.md\")\n-    \n+\n     print(\"\\n\ud83c\udfaf Key Insights:\")\n-    for insight in report['insights']:\n+    for insight in report[\"insights\"]:\n         print(f\"  {insight}\")\n-    \n+\n     print(\"\\n\ud83d\ude80 Top Recommendations:\")\n-    for i, rec in enumerate(report['recommendations'][:5], 1):\n+    for i, rec in enumerate(report[\"recommendations\"][:5], 1):\n         print(f\"  {i}. {rec}\")\n+\n \n if __name__ == \"__main__\":\n     main()\n--- /home/justin/llama_rag/app/seo-api/analytics/predictive_analytics.py\t2025-09-29 06:31:23.221240+00:00\n+++ /home/justin/llama_rag/app/seo-api/analytics/predictive_analytics.py\t2025-09-29 06:34:08.735774+00:00\n@@ -19,56 +19,65 @@\n import logging\n from sklearn.linear_model import LinearRegression\n from sklearn.ensemble import RandomForestRegressor\n from sklearn.preprocessing import StandardScaler\n import warnings\n-warnings.filterwarnings('ignore')\n+\n+warnings.filterwarnings(\"ignore\")\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n @dataclass\n class SEOForecast:\n     \"\"\"SEO performance forecast.\"\"\"\n+\n     keyword: str\n     current_rank: int\n     predicted_rank_30d: int\n     predicted_rank_90d: int\n     confidence_score: float\n     traffic_forecast: int\n     opportunity_score: float\n     recommended_actions: List[str]\n     created_at: str\n \n+\n @dataclass\n class CompetitiveAnalysis:\n     \"\"\"Competitive SEO analysis.\"\"\"\n+\n     competitor_domain: str\n     market_share: float\n     keyword_overlap: float\n     content_gaps: List[str]\n     opportunity_score: float\n     threat_level: str\n     recommended_strategy: str\n     created_at: str\n \n+\n @dataclass\n class SEOTrend:\n     \"\"\"SEO trend analysis.\"\"\"\n+\n     metric: str\n     current_value: float\n     trend_direction: str\n     trend_strength: float\n     predicted_value_30d: float\n     predicted_value_90d: float\n     confidence: float\n     created_at: str\n \n+\n @dataclass\n class AdvancedAnalyticsReport:\n     \"\"\"Comprehensive SEO analytics report.\"\"\"\n+\n     report_id: str\n     period_start: str\n     period_end: str\n     forecasts: List[SEOForecast]\n     competitive_analysis: List[CompetitiveAnalysis]\n@@ -76,402 +85,483 @@\n     key_insights: List[str]\n     recommendations: List[str]\n     risk_alerts: List[str]\n     generated_at: str\n \n+\n class PredictiveSEOAnalytics:\n     \"\"\"Advanced SEO analytics with predictive capabilities.\"\"\"\n-    \n+\n     def __init__(self):\n         self.historical_data = {}\n         self.models = {}\n         self.scaler = StandardScaler()\n         self.logger = logging.getLogger(self.__class__.__name__)\n-    \n+\n     def add_historical_data(self, keyword: str, data: List[Dict[str, Any]]):\n         \"\"\"Add historical performance data for a keyword.\"\"\"\n         self.historical_data[keyword] = data\n         self.logger.info(f\"Added historical data for keyword: {keyword}\")\n-    \n+\n     def generate_forecast(self, keyword: str, days_ahead: int = 90) -> SEOForecast:\n         \"\"\"Generate SEO performance forecast for a keyword.\"\"\"\n-        \n+\n         if keyword not in self.historical_data:\n             # Generate mock forecast if no historical data\n             return self._generate_mock_forecast(keyword, days_ahead)\n-        \n+\n         historical_data = self.historical_data[keyword]\n-        \n+\n         # Prepare data for modeling\n         df = pd.DataFrame(historical_data)\n-        df['date'] = pd.to_datetime(df['date'])\n-        df = df.sort_values('date')\n-        \n+        df[\"date\"] = pd.to_datetime(df[\"date\"])\n+        df = df.sort_values(\"date\")\n+\n         # Feature engineering\n-        df['rank_change'] = df['rank'].diff()\n-        df['traffic_change'] = df['traffic'].diff()\n-        df['day_of_week'] = df['date'].dt.dayofweek\n-        df['month'] = df['date'].dt.month\n-        \n+        df[\"rank_change\"] = df[\"rank\"].diff()\n+        df[\"traffic_change\"] = df[\"traffic\"].diff()\n+        df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n+        df[\"month\"] = df[\"date\"].dt.month\n+\n         # Prepare features and target\n-        features = ['rank', 'traffic', 'rank_change', 'traffic_change', 'day_of_week', 'month']\n+        features = [\n+            \"rank\",\n+            \"traffic\",\n+            \"rank_change\",\n+            \"traffic_change\",\n+            \"day_of_week\",\n+            \"month\",\n+        ]\n         X = df[features].fillna(0)\n-        y_rank = df['rank'].values\n-        y_traffic = df['traffic'].values\n-        \n+        y_rank = df[\"rank\"].values\n+        y_traffic = df[\"traffic\"].values\n+\n         # Train models\n         rank_model = self._train_rank_model(X, y_rank)\n         traffic_model = self._train_traffic_model(X, y_traffic)\n-        \n+\n         # Generate predictions\n         last_data = X.iloc[-1:].values\n         predicted_rank_30d = self._predict_rank(rank_model, last_data, 30)\n         predicted_rank_90d = self._predict_rank(rank_model, last_data, 90)\n         predicted_traffic = self._predict_traffic(traffic_model, last_data, days_ahead)\n-        \n+\n         # Calculate confidence and opportunity scores\n         confidence_score = self._calculate_confidence(df)\n-        opportunity_score = self._calculate_opportunity_score(predicted_rank_90d, predicted_traffic)\n-        \n+        opportunity_score = self._calculate_opportunity_score(\n+            predicted_rank_90d, predicted_traffic\n+        )\n+\n         # Generate recommendations\n-        recommendations = self._generate_recommendations(keyword, predicted_rank_90d, predicted_traffic)\n-        \n+        recommendations = self._generate_recommendations(\n+            keyword, predicted_rank_90d, predicted_traffic\n+        )\n+\n         return SEOForecast(\n             keyword=keyword,\n-            current_rank=df['rank'].iloc[-1],\n+            current_rank=df[\"rank\"].iloc[-1],\n             predicted_rank_30d=int(predicted_rank_30d),\n             predicted_rank_90d=int(predicted_rank_90d),\n             confidence_score=confidence_score,\n             traffic_forecast=int(predicted_traffic),\n             opportunity_score=opportunity_score,\n             recommended_actions=recommendations,\n-            created_at=datetime.now().isoformat()\n-        )\n-    \n+            created_at=datetime.now().isoformat(),\n+        )\n+\n     def _generate_mock_forecast(self, keyword: str, days_ahead: int) -> SEOForecast:\n         \"\"\"Generate mock forecast for demonstration.\"\"\"\n-        \n+\n         # Mock data generation\n         current_rank = np.random.randint(5, 50)\n         rank_improvement = np.random.uniform(0.8, 1.2)\n         traffic_base = np.random.randint(100, 2000)\n         traffic_growth = np.random.uniform(1.1, 1.5)\n-        \n+\n         predicted_rank_30d = max(1, int(current_rank * rank_improvement))\n         predicted_rank_90d = max(1, int(current_rank * rank_improvement * 0.9))\n         predicted_traffic = int(traffic_base * (traffic_growth ** (days_ahead / 30)))\n-        \n+\n         confidence_score = np.random.uniform(0.6, 0.9)\n         opportunity_score = np.random.uniform(0.3, 0.8)\n-        \n+\n         recommendations = [\n             f\"Optimize content for '{keyword}'\",\n             \"Build high-quality backlinks\",\n             \"Improve page loading speed\",\n-            \"Enhance user experience signals\"\n+            \"Enhance user experience signals\",\n         ]\n-        \n+\n         return SEOForecast(\n             keyword=keyword,\n             current_rank=current_rank,\n             predicted_rank_30d=predicted_rank_30d,\n             predicted_rank_90d=predicted_rank_90d,\n             confidence_score=confidence_score,\n             traffic_forecast=predicted_traffic,\n             opportunity_score=opportunity_score,\n             recommended_actions=recommendations,\n-            created_at=datetime.now().isoformat()\n-        )\n-    \n+            created_at=datetime.now().isoformat(),\n+        )\n+\n     def _train_rank_model(self, X: pd.DataFrame, y: np.ndarray) -> Any:\n         \"\"\"Train ranking prediction model.\"\"\"\n         model = RandomForestRegressor(n_estimators=100, random_state=42)\n         model.fit(X, y)\n         return model\n-    \n+\n     def _train_traffic_model(self, X: pd.DataFrame, y: np.ndarray) -> Any:\n         \"\"\"Train traffic prediction model.\"\"\"\n         model = LinearRegression()\n         model.fit(X, y)\n         return model\n-    \n+\n     def _predict_rank(self, model: Any, last_data: np.ndarray, days: int) -> float:\n         \"\"\"Predict ranking for given days ahead.\"\"\"\n         # Simple prediction - in production, use time series forecasting\n         prediction = model.predict(last_data)[0]\n         # Apply trend adjustment based on days\n         trend_factor = 1 - (days / 365) * 0.1  # Slight improvement over time\n         return max(1, prediction * trend_factor)\n-    \n+\n     def _predict_traffic(self, model: Any, last_data: np.ndarray, days: int) -> float:\n         \"\"\"Predict traffic for given days ahead.\"\"\"\n         prediction = model.predict(last_data)[0]\n         # Apply growth factor based on days\n         growth_factor = 1 + (days / 365) * 0.2  # 20% annual growth\n         return max(0, prediction * growth_factor)\n-    \n+\n     def _calculate_confidence(self, df: pd.DataFrame) -> float:\n         \"\"\"Calculate confidence score based on data quality.\"\"\"\n         if len(df) < 7:\n             return 0.5\n-        \n+\n         # Calculate variance in rankings\n-        rank_variance = df['rank'].var()\n-        traffic_variance = df['traffic'].var()\n-        \n+        rank_variance = df[\"rank\"].var()\n+        traffic_variance = df[\"traffic\"].var()\n+\n         # Lower variance = higher confidence\n         rank_confidence = max(0.1, 1 - (rank_variance / 100))\n         traffic_confidence = max(0.1, 1 - (traffic_variance / 10000))\n-        \n+\n         return (rank_confidence + traffic_confidence) / 2\n-    \n-    def _calculate_opportunity_score(self, predicted_rank: int, predicted_traffic: int) -> float:\n+\n+    def _calculate_opportunity_score(\n+        self, predicted_rank: int, predicted_traffic: int\n+    ) -> float:\n         \"\"\"Calculate opportunity score based on predictions.\"\"\"\n         # Higher opportunity for better rankings and more traffic\n         rank_score = max(0, (50 - predicted_rank) / 50)  # Better rank = higher score\n         traffic_score = min(1, predicted_traffic / 1000)  # More traffic = higher score\n-        \n+\n         return (rank_score + traffic_score) / 2\n-    \n-    def _generate_recommendations(self, keyword: str, predicted_rank: int, predicted_traffic: int) -> List[str]:\n+\n+    def _generate_recommendations(\n+        self, keyword: str, predicted_rank: int, predicted_traffic: int\n+    ) -> List[str]:\n         \"\"\"Generate actionable recommendations.\"\"\"\n         recommendations = []\n-        \n+\n         if predicted_rank > 20:\n-            recommendations.append(f\"Focus on improving ranking for '{keyword}' - currently predicted at position {predicted_rank}\")\n-        \n+            recommendations.append(\n+                f\"Focus on improving ranking for '{keyword}' - currently predicted at position {predicted_rank}\"\n+            )\n+\n         if predicted_traffic < 500:\n-            recommendations.append(\"Increase content depth and quality to boost traffic\")\n-        \n-        recommendations.extend([\n-            \"Monitor competitor rankings and content strategies\",\n-            \"Optimize for featured snippets and voice search\",\n-            \"Build topic authority through comprehensive content clusters\"\n-        ])\n-        \n+            recommendations.append(\n+                \"Increase content depth and quality to boost traffic\"\n+            )\n+\n+        recommendations.extend(\n+            [\n+                \"Monitor competitor rankings and content strategies\",\n+                \"Optimize for featured snippets and voice search\",\n+                \"Build topic authority through comprehensive content clusters\",\n+            ]\n+        )\n+\n         return recommendations\n-    \n-    def analyze_competitive_landscape(self, competitors: List[str], target_keywords: List[str]) -> List[CompetitiveAnalysis]:\n+\n+    def analyze_competitive_landscape(\n+        self, competitors: List[str], target_keywords: List[str]\n+    ) -> List[CompetitiveAnalysis]:\n         \"\"\"Analyze competitive SEO landscape.\"\"\"\n-        \n+\n         analyses = []\n-        \n+\n         for competitor in competitors:\n             # Mock competitive analysis\n             market_share = np.random.uniform(0.1, 0.4)\n             keyword_overlap = np.random.uniform(0.2, 0.8)\n-            \n+\n             # Generate content gaps\n             content_gaps = [\n                 f\"Long-form content about {np.random.choice(target_keywords)}\",\n                 f\"Video content for {np.random.choice(target_keywords)}\",\n-                f\"Interactive tools for {np.random.choice(target_keywords)}\"\n+                f\"Interactive tools for {np.random.choice(target_keywords)}\",\n             ]\n-            \n+\n             opportunity_score = np.random.uniform(0.3, 0.9)\n-            threat_level = \"High\" if market_share > 0.3 else \"Medium\" if market_share > 0.15 else \"Low\"\n-            \n-            recommended_strategy = self._generate_competitive_strategy(competitor, market_share, keyword_overlap)\n-            \n+            threat_level = (\n+                \"High\"\n+                if market_share > 0.3\n+                else \"Medium\" if market_share > 0.15 else \"Low\"\n+            )\n+\n+            recommended_strategy = self._generate_competitive_strategy(\n+                competitor, market_share, keyword_overlap\n+            )\n+\n             analysis = CompetitiveAnalysis(\n                 competitor_domain=competitor,\n                 market_share=market_share,\n                 keyword_overlap=keyword_overlap,\n                 content_gaps=content_gaps,\n                 opportunity_score=opportunity_score,\n                 threat_level=threat_level,\n                 recommended_strategy=recommended_strategy,\n-                created_at=datetime.now().isoformat()\n-            )\n-            \n+                created_at=datetime.now().isoformat(),\n+            )\n+\n             analyses.append(analysis)\n-        \n+\n         return analyses\n-    \n-    def _generate_competitive_strategy(self, competitor: str, market_share: float, keyword_overlap: float) -> str:\n+\n+    def _generate_competitive_strategy(\n+        self, competitor: str, market_share: float, keyword_overlap: float\n+    ) -> str:\n         \"\"\"Generate competitive strategy recommendations.\"\"\"\n-        \n+\n         if market_share > 0.3:\n             return \"Direct competitor with high market share. Focus on differentiation and unique value propositions.\"\n         elif keyword_overlap > 0.6:\n-            return \"High keyword overlap. Compete on content quality and user experience.\"\n+            return (\n+                \"High keyword overlap. Compete on content quality and user experience.\"\n+            )\n         else:\n             return \"Low overlap opportunity. Target their weak keyword areas and build authority.\"\n-    \n+\n     def analyze_trends(self, metrics: Dict[str, List[float]]) -> List[SEOTrend]:\n         \"\"\"Analyze SEO trends across multiple metrics.\"\"\"\n-        \n+\n         trends = []\n-        \n+\n         for metric, values in metrics.items():\n             if len(values) < 2:\n                 continue\n-            \n+\n             # Calculate trend\n             x = np.arange(len(values))\n             slope, intercept = np.polyfit(x, values, 1)\n-            \n+\n             trend_direction = \"up\" if slope > 0 else \"down\" if slope < 0 else \"stable\"\n             trend_strength = abs(slope)\n-            \n+\n             # Generate predictions\n             predicted_30d = values[-1] + slope * 30\n             predicted_90d = values[-1] + slope * 90\n-            \n+\n             # Calculate confidence\n             confidence = min(0.9, max(0.1, 1 - (np.std(values) / np.mean(values))))\n-            \n+\n             trend = SEOTrend(\n                 metric=metric,\n                 current_value=values[-1],\n                 trend_direction=trend_direction,\n                 trend_strength=trend_strength,\n                 predicted_value_30d=predicted_30d,\n                 predicted_value_90d=predicted_90d,\n                 confidence=confidence,\n-                created_at=datetime.now().isoformat()\n-            )\n-            \n+                created_at=datetime.now().isoformat(),\n+            )\n+\n             trends.append(trend)\n-        \n+\n         return trends\n-    \n-    def generate_advanced_report(self, \n-                               keywords: List[str], \n-                               competitors: List[str],\n-                               metrics: Dict[str, List[float]]) -> AdvancedAnalyticsReport:\n+\n+    def generate_advanced_report(\n+        self,\n+        keywords: List[str],\n+        competitors: List[str],\n+        metrics: Dict[str, List[float]],\n+    ) -> AdvancedAnalyticsReport:\n         \"\"\"Generate comprehensive advanced analytics report.\"\"\"\n-        \n+\n         # Generate forecasts\n         forecasts = []\n         for keyword in keywords:\n             forecast = self.generate_forecast(keyword)\n             forecasts.append(forecast)\n-        \n+\n         # Generate competitive analysis\n         competitive_analysis = self.analyze_competitive_landscape(competitors, keywords)\n-        \n+\n         # Generate trends\n         trends = self.analyze_trends(metrics)\n-        \n+\n         # Generate insights and recommendations\n-        key_insights = self._generate_key_insights(forecasts, competitive_analysis, trends)\n-        recommendations = self._generate_recommendations_list(forecasts, competitive_analysis, trends)\n+        key_insights = self._generate_key_insights(\n+            forecasts, competitive_analysis, trends\n+        )\n+        recommendations = self._generate_recommendations_list(\n+            forecasts, competitive_analysis, trends\n+        )\n         risk_alerts = self._generate_risk_alerts(forecasts, trends)\n-        \n+\n         return AdvancedAnalyticsReport(\n             report_id=f\"seo_analytics_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n             period_start=(datetime.now() - timedelta(days=90)).isoformat(),\n             period_end=datetime.now().isoformat(),\n             forecasts=forecasts,\n             competitive_analysis=competitive_analysis,\n             trends=trends,\n             key_insights=key_insights,\n             recommendations=recommendations,\n             risk_alerts=risk_alerts,\n-            generated_at=datetime.now().isoformat()\n-        )\n-    \n-    def _generate_key_insights(self, forecasts: List[SEOForecast], \n-                             competitive_analysis: List[CompetitiveAnalysis],\n-                             trends: List[SEOTrend]) -> List[str]:\n+            generated_at=datetime.now().isoformat(),\n+        )\n+\n+    def _generate_key_insights(\n+        self,\n+        forecasts: List[SEOForecast],\n+        competitive_analysis: List[CompetitiveAnalysis],\n+        trends: List[SEOTrend],\n+    ) -> List[str]:\n         \"\"\"Generate key insights from analytics data.\"\"\"\n-        \n+\n         insights = []\n-        \n+\n         # Forecast insights\n         high_opportunity_keywords = [f for f in forecasts if f.opportunity_score > 0.7]\n         if high_opportunity_keywords:\n-            insights.append(f\"Found {len(high_opportunity_keywords)} high-opportunity keywords with strong growth potential\")\n-        \n+            insights.append(\n+                f\"Found {len(high_opportunity_keywords)} high-opportunity keywords with strong growth potential\"\n+            )\n+\n         # Competitive insights\n-        high_threat_competitors = [c for c in competitive_analysis if c.threat_level == \"High\"]\n+        high_threat_competitors = [\n+            c for c in competitive_analysis if c.threat_level == \"High\"\n+        ]\n         if high_threat_competitors:\n-            insights.append(f\"Identified {len(high_threat_competitors)} high-threat competitors requiring strategic response\")\n-        \n+            insights.append(\n+                f\"Identified {len(high_threat_competitors)} high-threat competitors requiring strategic response\"\n+            )\n+\n         # Trend insights\n         positive_trends = [t for t in trends if t.trend_direction == \"up\"]\n         if positive_trends:\n-            insights.append(f\"Detected {len(positive_trends)} positive trends indicating growth momentum\")\n-        \n+            insights.append(\n+                f\"Detected {len(positive_trends)} positive trends indicating growth momentum\"\n+            )\n+\n         return insights\n-    \n-    def _generate_recommendations_list(self, forecasts: List[SEOForecast],\n-                                     competitive_analysis: List[CompetitiveAnalysis],\n-                                     trends: List[SEOTrend]) -> List[str]:\n+\n+    def _generate_recommendations_list(\n+        self,\n+        forecasts: List[SEOForecast],\n+        competitive_analysis: List[CompetitiveAnalysis],\n+        trends: List[SEOTrend],\n+    ) -> List[str]:\n         \"\"\"Generate comprehensive recommendations.\"\"\"\n-        \n+\n         recommendations = []\n-        \n+\n         # Keyword recommendations\n-        recommendations.append(\"Focus on high-opportunity keywords with strong growth potential\")\n-        recommendations.append(\"Monitor competitor keyword strategies and identify content gaps\")\n-        \n+        recommendations.append(\n+            \"Focus on high-opportunity keywords with strong growth potential\"\n+        )\n+        recommendations.append(\n+            \"Monitor competitor keyword strategies and identify content gaps\"\n+        )\n+\n         # Content recommendations\n-        recommendations.append(\"Develop comprehensive content clusters around target keywords\")\n-        recommendations.append(\"Create interactive and multimedia content to improve engagement\")\n-        \n+        recommendations.append(\n+            \"Develop comprehensive content clusters around target keywords\"\n+        )\n+        recommendations.append(\n+            \"Create interactive and multimedia content to improve engagement\"\n+        )\n+\n         # Technical recommendations\n         recommendations.append(\"Optimize page loading speed and mobile experience\")\n-        recommendations.append(\"Implement structured data markup for better search visibility\")\n-        \n+        recommendations.append(\n+            \"Implement structured data markup for better search visibility\"\n+        )\n+\n         return recommendations\n-    \n-    def _generate_risk_alerts(self, forecasts: List[SEOForecast], trends: List[SEOTrend]) -> List[str]:\n+\n+    def _generate_risk_alerts(\n+        self, forecasts: List[SEOForecast], trends: List[SEOTrend]\n+    ) -> List[str]:\n         \"\"\"Generate risk alerts based on analytics.\"\"\"\n-        \n+\n         alerts = []\n-        \n+\n         # Declining performance alerts\n-        declining_keywords = [f for f in forecasts if f.predicted_rank_90d > f.current_rank + 5]\n+        declining_keywords = [\n+            f for f in forecasts if f.predicted_rank_90d > f.current_rank + 5\n+        ]\n         if declining_keywords:\n-            alerts.append(f\"Warning: {len(declining_keywords)} keywords showing declining performance\")\n-        \n+            alerts.append(\n+                f\"Warning: {len(declining_keywords)} keywords showing declining performance\"\n+            )\n+\n         # Negative trend alerts\n-        negative_trends = [t for t in trends if t.trend_direction == \"down\" and t.trend_strength > 0.1]\n+        negative_trends = [\n+            t for t in trends if t.trend_direction == \"down\" and t.trend_strength > 0.1\n+        ]\n         if negative_trends:\n-            alerts.append(f\"Alert: {len(negative_trends)} metrics showing negative trends\")\n-        \n+            alerts.append(\n+                f\"Alert: {len(negative_trends)} metrics showing negative trends\"\n+            )\n+\n         return alerts\n+\n \n def main():\n     \"\"\"Main function to demonstrate predictive SEO analytics.\"\"\"\n-    \n+\n     # Initialize analytics\n     analytics = PredictiveSEOAnalytics()\n-    \n+\n     # Sample keywords\n-    keywords = [\"seo best practices\", \"content marketing\", \"keyword research\", \"link building\"]\n-    \n+    keywords = [\n+        \"seo best practices\",\n+        \"content marketing\",\n+        \"keyword research\",\n+        \"link building\",\n+    ]\n+\n     # Sample competitors\n     competitors = [\"competitor1.com\", \"competitor2.com\", \"competitor3.com\"]\n-    \n+\n     # Sample metrics data\n     metrics = {\n         \"organic_traffic\": [1000, 1100, 1200, 1300, 1400, 1500],\n         \"average_rank\": [15, 14, 13, 12, 11, 10],\n-        \"click_through_rate\": [2.5, 2.7, 2.9, 3.1, 3.3, 3.5]\n+        \"click_through_rate\": [2.5, 2.7, 2.9, 3.1, 3.3, 3.5],\n     }\n-    \n+\n     # Generate advanced report\n     report = analytics.generate_advanced_report(keywords, competitors, metrics)\n-    \n+\n     print(\"\ud83d\ude80 SEO Advanced Analytics Platform - DEMO\")\n     print(\"=\" * 50)\n     print(f\"Report ID: {report.report_id}\")\n     print(f\"Period: {report.period_start} to {report.period_end}\")\n     print(f\"Forecasts Generated: {len(report.forecasts)}\")\n     print(f\"Competitive Analysis: {len(report.competitive_analysis)}\")\n     print(f\"Trends Analyzed: {len(report.trends)}\")\n     print(f\"Key Insights: {len(report.key_insights)}\")\n     print(f\"Recommendations: {len(report.recommendations)}\")\n     print(f\"Risk Alerts: {len(report.risk_alerts)}\")\n-    \n+\n     # Save report\n     with open(\"seo_advanced_analytics_report.json\", \"w\") as f:\n         f.write(json.dumps(asdict(report), indent=2, default=str))\n-    \n+\n     print(\"\\nFiles created:\")\n     print(\"- seo_advanced_analytics_report.json\")\n \n+\n if __name__ == \"__main__\":\n     main()\n--- /home/justin/llama_rag/app/seo-api/analytics/intelligent_alerts.py\t2025-09-29 06:31:23.225240+00:00\n+++ /home/justin/llama_rag/app/seo-api/analytics/intelligent_alerts.py\t2025-09-29 06:34:08.767421+00:00\n@@ -22,29 +22,35 @@\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n class AlertSeverity(Enum):\n     \"\"\"Alert severity levels.\"\"\"\n+\n     INFO = \"info\"\n     WARNING = \"warning\"\n     CRITICAL = \"critical\"\n     EMERGENCY = \"emergency\"\n \n+\n class AlertType(Enum):\n     \"\"\"Alert types.\"\"\"\n+\n     PERFORMANCE_DROP = \"performance_drop\"\n     RANKING_DROP = \"ranking_drop\"\n     TRAFFIC_SPIKE = \"traffic_spike\"\n     COMPETITOR_THREAT = \"competitor_threat\"\n     TECHNICAL_ISSUE = \"technical_issue\"\n     ANOMALY_DETECTED = \"anomaly_detected\"\n \n+\n @dataclass\n class Alert:\n     \"\"\"SEO alert data structure.\"\"\"\n+\n     alert_id: str\n     alert_type: AlertType\n     severity: AlertSeverity\n     title: str\n     description: str\n@@ -58,13 +64,15 @@\n     created_at: str\n     acknowledged: bool = False\n     resolved: bool = False\n     escalated: bool = False\n \n+\n @dataclass\n class AlertRule:\n     \"\"\"Alert rule configuration.\"\"\"\n+\n     rule_id: str\n     name: str\n     alert_type: AlertType\n     metric_name: str\n     condition: str\n@@ -75,162 +83,184 @@\n     pages: List[str]\n     cooldown_minutes: int\n     escalation_minutes: int\n     created_at: str\n \n+\n @dataclass\n class AnomalyDetection:\n     \"\"\"Anomaly detection result.\"\"\"\n+\n     metric_name: str\n     is_anomaly: bool\n     anomaly_score: float\n     expected_value: float\n     actual_value: float\n     confidence: float\n     detected_at: str\n \n+\n class IntelligentAlertSystem:\n     \"\"\"Intelligent SEO alerting system.\"\"\"\n-    \n+\n     def __init__(self):\n         self.alerts = []\n         self.rules = []\n         self.metric_history = {}\n         self.anomaly_models = {}\n         self.alert_cooldowns = {}\n         self.logger = logging.getLogger(self.__class__.__name__)\n-    \n+\n     def add_alert_rule(self, rule: AlertRule):\n         \"\"\"Add an alert rule.\"\"\"\n         self.rules.append(rule)\n         self.logger.info(f\"Added alert rule: {rule.name}\")\n-    \n+\n     def add_metric_data(self, metric_name: str, value: float, timestamp: str = None):\n         \"\"\"Add metric data point for analysis.\"\"\"\n         if timestamp is None:\n             timestamp = datetime.now().isoformat()\n-        \n+\n         if metric_name not in self.metric_history:\n-            self.metric_history[metric_name] = deque(maxlen=1000)  # Keep last 1000 data points\n-        \n-        self.metric_history[metric_name].append({\n-            \"value\": value,\n-            \"timestamp\": timestamp\n-        })\n-        \n+            self.metric_history[metric_name] = deque(\n+                maxlen=1000\n+            )  # Keep last 1000 data points\n+\n+        self.metric_history[metric_name].append(\n+            {\"value\": value, \"timestamp\": timestamp}\n+        )\n+\n         self.logger.debug(f\"Added metric data: {metric_name} = {value}\")\n-    \n-    def detect_anomalies(self, metric_name: str, window_size: int = 30) -> AnomalyDetection:\n+\n+    def detect_anomalies(\n+        self, metric_name: str, window_size: int = 30\n+    ) -> AnomalyDetection:\n         \"\"\"Detect anomalies in metric data using statistical methods.\"\"\"\n-        \n-        if metric_name not in self.metric_history or len(self.metric_history[metric_name]) < window_size:\n+\n+        if (\n+            metric_name not in self.metric_history\n+            or len(self.metric_history[metric_name]) < window_size\n+        ):\n             return AnomalyDetection(\n                 metric_name=metric_name,\n                 is_anomaly=False,\n                 anomaly_score=0.0,\n                 expected_value=0.0,\n                 actual_value=0.0,\n                 confidence=0.0,\n-                detected_at=datetime.now().isoformat()\n-            )\n-        \n+                detected_at=datetime.now().isoformat(),\n+            )\n+\n         # Get recent data\n         recent_data = list(self.metric_history[metric_name])[-window_size:]\n         values = [point[\"value\"] for point in recent_data]\n         current_value = values[-1]\n-        \n+\n         # Calculate statistical measures\n         mean_value = statistics.mean(values[:-1])  # Exclude current value\n         std_value = statistics.stdev(values[:-1]) if len(values) > 2 else 0\n-        \n+\n         # Calculate anomaly score (Z-score)\n         if std_value > 0:\n             z_score = abs((current_value - mean_value) / std_value)\n             anomaly_score = min(1.0, z_score / 3.0)  # Normalize to 0-1\n         else:\n             anomaly_score = 0.0\n-        \n+\n         # Determine if anomaly\n         is_anomaly = anomaly_score > 0.7  # Threshold for anomaly detection\n-        \n+\n         # Calculate confidence\n         confidence = min(0.95, anomaly_score * 1.2)\n-        \n+\n         return AnomalyDetection(\n             metric_name=metric_name,\n             is_anomaly=is_anomaly,\n             anomaly_score=anomaly_score,\n             expected_value=mean_value,\n             actual_value=current_value,\n             confidence=confidence,\n-            detected_at=datetime.now().isoformat()\n+            detected_at=datetime.now().isoformat(),\n         )\n-    \n+\n     def check_alert_rules(self) -> List[Alert]:\n         \"\"\"Check all alert rules and generate alerts.\"\"\"\n-        \n+\n         new_alerts = []\n-        \n+\n         for rule in self.rules:\n             if not rule.enabled:\n                 continue\n-            \n+\n             # Check cooldown\n             cooldown_key = f\"{rule.rule_id}_{rule.metric_name}\"\n             if cooldown_key in self.alert_cooldowns:\n                 last_alert = self.alert_cooldowns[cooldown_key]\n-                if datetime.now() - last_alert < timedelta(minutes=rule.cooldown_minutes):\n+                if datetime.now() - last_alert < timedelta(\n+                    minutes=rule.cooldown_minutes\n+                ):\n                     continue\n-            \n+\n             # Get current metric value\n-            if rule.metric_name not in self.metric_history or not self.metric_history[rule.metric_name]:\n+            if (\n+                rule.metric_name not in self.metric_history\n+                or not self.metric_history[rule.metric_name]\n+            ):\n                 continue\n-            \n+\n             current_data = self.metric_history[rule.metric_name][-1]\n             current_value = current_data[\"value\"]\n-            \n+\n             # Check rule condition\n             alert_triggered = False\n             change_percent = 0.0\n-            \n+\n             if rule.condition == \"greater_than\" and current_value > rule.threshold:\n                 alert_triggered = True\n             elif rule.condition == \"less_than\" and current_value < rule.threshold:\n                 alert_triggered = True\n             elif rule.condition == \"change_greater_than\":\n                 # Calculate change from previous value\n                 if len(self.metric_history[rule.metric_name]) > 1:\n                     previous_value = self.metric_history[rule.metric_name][-2][\"value\"]\n-                    change_percent = ((current_value - previous_value) / previous_value) * 100\n+                    change_percent = (\n+                        (current_value - previous_value) / previous_value\n+                    ) * 100\n                     if abs(change_percent) > rule.threshold:\n                         alert_triggered = True\n-            \n+\n             if alert_triggered:\n                 # Create alert\n                 alert = self._create_alert(rule, current_value, change_percent)\n                 new_alerts.append(alert)\n-                \n+\n                 # Update cooldown\n                 self.alert_cooldowns[cooldown_key] = datetime.now()\n-        \n+\n         return new_alerts\n-    \n-    def _create_alert(self, rule: AlertRule, current_value: float, change_percent: float) -> Alert:\n+\n+    def _create_alert(\n+        self, rule: AlertRule, current_value: float, change_percent: float\n+    ) -> Alert:\n         \"\"\"Create an alert from a triggered rule.\"\"\"\n-        \n-        alert_id = f\"alert_{len(self.alerts)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n-        \n+\n+        alert_id = (\n+            f\"alert_{len(self.alerts)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n+        )\n+\n         # Generate alert title and description\n         title = f\"{rule.alert_type.value.replace('_', ' ').title()}: {rule.metric_name}\"\n         description = f\"{rule.metric_name} is {rule.condition.replace('_', ' ')} {rule.threshold}. Current value: {current_value:.2f}\"\n-        \n+\n         if change_percent != 0:\n             description += f\" (Change: {change_percent:+.1f}%)\"\n-        \n+\n         # Generate recommendations\n-        recommendations = self._generate_alert_recommendations(rule, current_value, change_percent)\n-        \n+        recommendations = self._generate_alert_recommendations(\n+            rule, current_value, change_percent\n+        )\n+\n         alert = Alert(\n             alert_id=alert_id,\n             alert_type=rule.alert_type,\n             severity=rule.severity,\n             title=title,\n@@ -240,132 +270,152 @@\n             threshold_value=rule.threshold,\n             change_percent=change_percent,\n             affected_keywords=rule.keywords,\n             affected_pages=rule.pages,\n             recommendations=recommendations,\n-            created_at=datetime.now().isoformat()\n+            created_at=datetime.now().isoformat(),\n         )\n-        \n+\n         self.alerts.append(alert)\n         return alert\n-    \n-    def _generate_alert_recommendations(self, rule: AlertRule, current_value: float, change_percent: float) -> List[str]:\n+\n+    def _generate_alert_recommendations(\n+        self, rule: AlertRule, current_value: float, change_percent: float\n+    ) -> List[str]:\n         \"\"\"Generate recommendations based on alert type and severity.\"\"\"\n-        \n+\n         recommendations = []\n-        \n+\n         if rule.alert_type == AlertType.PERFORMANCE_DROP:\n-            recommendations.extend([\n-                \"Check for technical issues affecting page performance\",\n-                \"Review recent content changes that might impact SEO\",\n-                \"Analyze competitor activities and market changes\",\n-                \"Consider content optimization and link building strategies\"\n-            ])\n-        \n+            recommendations.extend(\n+                [\n+                    \"Check for technical issues affecting page performance\",\n+                    \"Review recent content changes that might impact SEO\",\n+                    \"Analyze competitor activities and market changes\",\n+                    \"Consider content optimization and link building strategies\",\n+                ]\n+            )\n+\n         elif rule.alert_type == AlertType.RANKING_DROP:\n-            recommendations.extend([\n-                \"Audit affected pages for SEO issues\",\n-                \"Check for duplicate content or thin content\",\n-                \"Review backlink profile for negative signals\",\n-                \"Optimize content for target keywords\"\n-            ])\n-        \n+            recommendations.extend(\n+                [\n+                    \"Audit affected pages for SEO issues\",\n+                    \"Check for duplicate content or thin content\",\n+                    \"Review backlink profile for negative signals\",\n+                    \"Optimize content for target keywords\",\n+                ]\n+            )\n+\n         elif rule.alert_type == AlertType.TRAFFIC_SPIKE:\n-            recommendations.extend([\n-                \"Investigate traffic sources to understand the spike\",\n-                \"Monitor for potential bot traffic or referral spam\",\n-                \"Check if the spike is from legitimate organic growth\",\n-                \"Prepare for potential server load issues\"\n-            ])\n-        \n+            recommendations.extend(\n+                [\n+                    \"Investigate traffic sources to understand the spike\",\n+                    \"Monitor for potential bot traffic or referral spam\",\n+                    \"Check if the spike is from legitimate organic growth\",\n+                    \"Prepare for potential server load issues\",\n+                ]\n+            )\n+\n         elif rule.alert_type == AlertType.COMPETITOR_THREAT:\n-            recommendations.extend([\n-                \"Analyze competitor's content and backlink strategies\",\n-                \"Identify content gaps and opportunities\",\n-                \"Develop competitive response strategy\",\n-                \"Monitor competitor activities more closely\"\n-            ])\n-        \n+            recommendations.extend(\n+                [\n+                    \"Analyze competitor's content and backlink strategies\",\n+                    \"Identify content gaps and opportunities\",\n+                    \"Develop competitive response strategy\",\n+                    \"Monitor competitor activities more closely\",\n+                ]\n+            )\n+\n         elif rule.alert_type == AlertType.TECHNICAL_ISSUE:\n-            recommendations.extend([\n-                \"Check website technical health and performance\",\n-                \"Review server logs for errors\",\n-                \"Test page loading speed and mobile responsiveness\",\n-                \"Verify structured data and schema markup\"\n-            ])\n-        \n+            recommendations.extend(\n+                [\n+                    \"Check website technical health and performance\",\n+                    \"Review server logs for errors\",\n+                    \"Test page loading speed and mobile responsiveness\",\n+                    \"Verify structured data and schema markup\",\n+                ]\n+            )\n+\n         return recommendations\n-    \n+\n     def check_anomaly_alerts(self) -> List[Alert]:\n         \"\"\"Check for anomaly-based alerts.\"\"\"\n-        \n+\n         anomaly_alerts = []\n-        \n+\n         for metric_name in self.metric_history:\n             anomaly = self.detect_anomalies(metric_name)\n-            \n+\n             if anomaly.is_anomaly and anomaly.confidence > 0.7:\n                 # Create anomaly alert\n                 alert_id = f\"anomaly_{len(self.alerts)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n-                \n+\n                 title = f\"Anomaly Detected: {metric_name}\"\n                 description = f\"Unusual pattern detected in {metric_name}. Expected: {anomaly.expected_value:.2f}, Actual: {anomaly.actual_value:.2f}\"\n-                \n+\n                 recommendations = [\n                     \"Investigate the cause of the unusual pattern\",\n                     \"Check for external factors affecting this metric\",\n                     \"Monitor closely for further anomalies\",\n-                    \"Consider adjusting alert thresholds if this is expected\"\n+                    \"Consider adjusting alert thresholds if this is expected\",\n                 ]\n-                \n+\n                 alert = Alert(\n                     alert_id=alert_id,\n                     alert_type=AlertType.ANOMALY_DETECTED,\n                     severity=AlertSeverity.WARNING,\n                     title=title,\n                     description=description,\n                     metric_name=metric_name,\n                     current_value=anomaly.actual_value,\n                     threshold_value=anomaly.expected_value,\n-                    change_percent=((anomaly.actual_value - anomaly.expected_value) / anomaly.expected_value) * 100,\n+                    change_percent=(\n+                        (anomaly.actual_value - anomaly.expected_value)\n+                        / anomaly.expected_value\n+                    )\n+                    * 100,\n                     affected_keywords=[],\n                     affected_pages=[],\n                     recommendations=recommendations,\n-                    created_at=datetime.now().isoformat()\n+                    created_at=datetime.now().isoformat(),\n                 )\n-                \n+\n                 anomaly_alerts.append(alert)\n                 self.alerts.append(alert)\n-        \n+\n         return anomaly_alerts\n-    \n+\n     def get_active_alerts(self) -> List[Alert]:\n         \"\"\"Get all active (non-resolved) alerts.\"\"\"\n         return [alert for alert in self.alerts if not alert.resolved]\n-    \n+\n     def get_alerts_by_severity(self, severity: AlertSeverity) -> List[Alert]:\n         \"\"\"Get alerts by severity level.\"\"\"\n-        return [alert for alert in self.alerts if alert.severity == severity and not alert.resolved]\n-    \n+        return [\n+            alert\n+            for alert in self.alerts\n+            if alert.severity == severity and not alert.resolved\n+        ]\n+\n     def acknowledge_alert(self, alert_id: str) -> bool:\n         \"\"\"Acknowledge an alert.\"\"\"\n         for alert in self.alerts:\n             if alert.alert_id == alert_id:\n                 alert.acknowledged = True\n                 self.logger.info(f\"Acknowledged alert: {alert_id}\")\n                 return True\n         return False\n-    \n+\n     def resolve_alert(self, alert_id: str) -> bool:\n         \"\"\"Resolve an alert.\"\"\"\n         for alert in self.alerts:\n             if alert.alert_id == alert_id:\n                 alert.resolved = True\n                 self.logger.info(f\"Resolved alert: {alert_id}\")\n                 return True\n         return False\n-    \n+\n     def escalate_alert(self, alert_id: str) -> bool:\n         \"\"\"Escalate an alert.\"\"\"\n         for alert in self.alerts:\n             if alert.alert_id == alert_id:\n                 alert.escalated = True\n@@ -374,51 +424,56 @@\n                     alert.severity = AlertSeverity.WARNING\n                 elif alert.severity == AlertSeverity.WARNING:\n                     alert.severity = AlertSeverity.CRITICAL\n                 elif alert.severity == AlertSeverity.CRITICAL:\n                     alert.severity = AlertSeverity.EMERGENCY\n-                \n+\n                 self.logger.info(f\"Escalated alert: {alert_id}\")\n                 return True\n         return False\n-    \n+\n     def generate_alert_summary(self) -> Dict[str, Any]:\n         \"\"\"Generate alert summary report.\"\"\"\n-        \n+\n         active_alerts = self.get_active_alerts()\n-        \n+\n         summary = {\n             \"total_alerts\": len(self.alerts),\n             \"active_alerts\": len(active_alerts),\n             \"resolved_alerts\": len([a for a in self.alerts if a.resolved]),\n             \"alerts_by_severity\": {\n                 \"info\": len(self.get_alerts_by_severity(AlertSeverity.INFO)),\n                 \"warning\": len(self.get_alerts_by_severity(AlertSeverity.WARNING)),\n                 \"critical\": len(self.get_alerts_by_severity(AlertSeverity.CRITICAL)),\n-                \"emergency\": len(self.get_alerts_by_severity(AlertSeverity.EMERGENCY))\n+                \"emergency\": len(self.get_alerts_by_severity(AlertSeverity.EMERGENCY)),\n             },\n             \"alerts_by_type\": {},\n-            \"recent_alerts\": [asdict(alert) for alert in active_alerts[-10:]],  # Last 10 alerts\n-            \"generated_at\": datetime.now().isoformat()\n+            \"recent_alerts\": [\n+                asdict(alert) for alert in active_alerts[-10:]\n+            ],  # Last 10 alerts\n+            \"generated_at\": datetime.now().isoformat(),\n         }\n-        \n+\n         # Count alerts by type\n         for alert in active_alerts:\n             alert_type = alert.alert_type.value\n-            summary[\"alerts_by_type\"][alert_type] = summary[\"alerts_by_type\"].get(alert_type, 0) + 1\n-        \n+            summary[\"alerts_by_type\"][alert_type] = (\n+                summary[\"alerts_by_type\"].get(alert_type, 0) + 1\n+            )\n+\n         return summary\n+\n \n def main():\n     \"\"\"Main function to demonstrate intelligent alert system.\"\"\"\n-    \n+\n     # Initialize alert system\n     alert_system = IntelligentAlertSystem()\n-    \n+\n     print(\"\ud83d\ude80 SEO Intelligent Alerts System - DEMO\")\n     print(\"=\" * 45)\n-    \n+\n     # Add sample alert rules\n     rules = [\n         AlertRule(\n             rule_id=\"traffic_drop\",\n             name=\"Traffic Drop Alert\",\n@@ -430,11 +485,11 @@\n             enabled=True,\n             keywords=[\"seo tips\", \"content marketing\"],\n             pages=[\"/blog/seo-tips\", \"/blog/content-marketing\"],\n             cooldown_minutes=60,\n             escalation_minutes=120,\n-            created_at=datetime.now().isoformat()\n+            created_at=datetime.now().isoformat(),\n         ),\n         AlertRule(\n             rule_id=\"ranking_drop\",\n             name=\"Ranking Drop Alert\",\n             alert_type=AlertType.RANKING_DROP,\n@@ -445,60 +500,63 @@\n             enabled=True,\n             keywords=[\"keyword research\", \"seo tools\"],\n             pages=[\"/tools/keyword-research\", \"/tools/seo-tools\"],\n             cooldown_minutes=30,\n             escalation_minutes=60,\n-            created_at=datetime.now().isoformat()\n-        )\n+            created_at=datetime.now().isoformat(),\n+        ),\n     ]\n-    \n+\n     for rule in rules:\n         alert_system.add_alert_rule(rule)\n-    \n+\n     # Add sample metric data\n     base_traffic = 1000\n     base_rank = 10\n-    \n+\n     for i in range(30):\n         # Simulate traffic with some drops\n         traffic = base_traffic + np.random.normal(0, 50)\n         if i == 25:  # Simulate a drop\n             traffic = base_traffic * 0.7\n-        \n+\n         # Simulate rankings with some drops\n         rank = base_rank + np.random.normal(0, 2)\n         if i == 28:  # Simulate a ranking drop\n             rank = base_rank + 15\n-        \n+\n         alert_system.add_metric_data(\"organic_traffic\", traffic)\n         alert_system.add_metric_data(\"average_rank\", rank)\n-    \n+\n     # Check for alerts\n     rule_alerts = alert_system.check_alert_rules()\n     anomaly_alerts = alert_system.check_anomaly_alerts()\n-    \n+\n     print(f\"Alert Rules: {len(alert_system.rules)}\")\n     print(f\"Rule-based Alerts: {len(rule_alerts)}\")\n     print(f\"Anomaly Alerts: {len(anomaly_alerts)}\")\n     print(f\"Total Alerts: {len(alert_system.alerts)}\")\n-    \n+\n     # Generate summary\n     summary = alert_system.generate_alert_summary()\n-    \n+\n     print(\"\\nAlert Summary:\")\n     print(f\"- Active Alerts: {summary['active_alerts']}\")\n     print(f\"- Resolved Alerts: {summary['resolved_alerts']}\")\n     print(f\"- By Severity: {summary['alerts_by_severity']}\")\n-    \n+\n     # Save alerts to file\n     with open(\"seo_alerts.json\", \"w\") as f:\n-        json.dump([asdict(alert) for alert in alert_system.alerts], f, indent=2, default=str)\n-    \n+        json.dump(\n+            [asdict(alert) for alert in alert_system.alerts], f, indent=2, default=str\n+        )\n+\n     with open(\"alert_summary.json\", \"w\") as f:\n         json.dump(summary, f, indent=2, default=str)\n-    \n+\n     print(\"\\nFiles created:\")\n     print(\"- seo_alerts.json\")\n     print(\"- alert_summary.json\")\n \n+\n if __name__ == \"__main__\":\n     main()\n--- /home/justin/llama_rag/app/seo-api/automation/ai_content_generator.py\t2025-09-29 06:31:23.225240+00:00\n+++ /home/justin/llama_rag/app/seo-api/automation/ai_content_generator.py\t2025-09-29 06:34:08.831184+00:00\n@@ -22,28 +22,32 @@\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n class ContentType(Enum):\n     BLOG_POST = \"blog_post\"\n     PRODUCT_PAGE = \"product_page\"\n     LANDING_PAGE = \"landing_page\"\n     GUIDE = \"guide\"\n     FAQ = \"faq\"\n     NEWS_ARTICLE = \"news_article\"\n \n+\n class ContentStatus(Enum):\n     DRAFT = \"draft\"\n     REVIEW = \"review\"\n     APPROVED = \"approved\"\n     PUBLISHED = \"published\"\n     ARCHIVED = \"archived\"\n \n+\n @dataclass\n class ContentBrief:\n     \"\"\"AI-generated content brief.\"\"\"\n+\n     id: str\n     title: str\n     content_type: ContentType\n     target_keywords: List[str]\n     primary_keyword: str\n@@ -60,13 +64,15 @@\n     target_audience: str\n     call_to_action: str\n     created_at: str\n     updated_at: str\n \n+\n @dataclass\n class GeneratedContent:\n     \"\"\"AI-generated content.\"\"\"\n+\n     id: str\n     brief_id: str\n     title: str\n     content: str\n     meta_description: str\n@@ -80,13 +86,15 @@\n     external_links: List[str]\n     status: ContentStatus\n     created_at: str\n     updated_at: str\n \n+\n @dataclass\n class ContentCalendar:\n     \"\"\"Content calendar entry.\"\"\"\n+\n     id: str\n     title: str\n     content_type: ContentType\n     brief_id: Optional[str]\n     content_id: Optional[str]\n@@ -96,84 +104,98 @@\n     priority: str\n     assigned_to: Optional[str]\n     notes: str\n     created_at: str\n \n+\n class AIContentGenerator:\n     \"\"\"AI-powered content generation system.\"\"\"\n-    \n+\n     def __init__(self, api_key: Optional[str] = None):\n         self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n         self.base_url = \"https://api.openai.com/v1\"\n         self.model = \"gpt-4\"\n         self.session: Optional[aiohttp.ClientSession] = None\n-    \n+\n     async def __aenter__(self):\n         self.session = aiohttp.ClientSession(\n             timeout=aiohttp.ClientTimeout(total=60),\n             headers={\n                 \"Authorization\": f\"Bearer {self.api_key}\",\n-                \"Content-Type\": \"application/json\"\n-            }\n+                \"Content-Type\": \"application/json\",\n+            },\n         )\n         return self\n-    \n+\n     async def __aexit__(self, exc_type, exc_val, exc_tb):\n         if self.session:\n             await self.session.close()\n-    \n-    async def generate_content_brief(self, \n-                                   topic: str,\n-                                   content_type: ContentType,\n-                                   target_keywords: List[str],\n-                                   competitor_analysis: Optional[Dict] = None,\n-                                   seo_opportunity: Optional[Dict] = None) -> ContentBrief:\n+\n+    async def generate_content_brief(\n+        self,\n+        topic: str,\n+        content_type: ContentType,\n+        target_keywords: List[str],\n+        competitor_analysis: Optional[Dict] = None,\n+        seo_opportunity: Optional[Dict] = None,\n+    ) -> ContentBrief:\n         \"\"\"Generate AI-powered content brief.\"\"\"\n-        \n-        prompt = self._build_brief_prompt(topic, content_type, target_keywords, competitor_analysis, seo_opportunity)\n-        \n+\n+        prompt = self._build_brief_prompt(\n+            topic, content_type, target_keywords, competitor_analysis, seo_opportunity\n+        )\n+\n         try:\n             response = await self._call_openai_api(prompt)\n-            brief_data = self._parse_brief_response(response, topic, content_type, target_keywords)\n+            brief_data = self._parse_brief_response(\n+                response, topic, content_type, target_keywords\n+            )\n             return brief_data\n         except Exception as e:\n             logger.error(f\"Error generating content brief: {str(e)}\")\n             # Return fallback brief\n             return self._create_fallback_brief(topic, content_type, target_keywords)\n-    \n+\n     async def generate_content(self, brief: ContentBrief) -> GeneratedContent:\n         \"\"\"Generate full content from brief.\"\"\"\n-        \n+\n         prompt = self._build_content_prompt(brief)\n-        \n+\n         try:\n             response = await self._call_openai_api(prompt)\n             content_data = self._parse_content_response(response, brief)\n             return content_data\n         except Exception as e:\n             logger.error(f\"Error generating content: {str(e)}\")\n             # Return fallback content\n             return self._create_fallback_content(brief)\n-    \n-    async def optimize_content(self, content: GeneratedContent, target_metrics: Dict[str, Any]) -> GeneratedContent:\n+\n+    async def optimize_content(\n+        self, content: GeneratedContent, target_metrics: Dict[str, Any]\n+    ) -> GeneratedContent:\n         \"\"\"Optimize content for better SEO and readability.\"\"\"\n-        \n+\n         prompt = self._build_optimization_prompt(content, target_metrics)\n-        \n+\n         try:\n             response = await self._call_openai_api(prompt)\n             optimized_content = self._parse_optimization_response(response, content)\n             return optimized_content\n         except Exception as e:\n             logger.error(f\"Error optimizing content: {str(e)}\")\n             return content\n-    \n-    def _build_brief_prompt(self, topic: str, content_type: ContentType, \n-                          target_keywords: List[str], competitor_analysis: Optional[Dict],\n-                          seo_opportunity: Optional[Dict]) -> str:\n+\n+    def _build_brief_prompt(\n+        self,\n+        topic: str,\n+        content_type: ContentType,\n+        target_keywords: List[str],\n+        competitor_analysis: Optional[Dict],\n+        seo_opportunity: Optional[Dict],\n+    ) -> str:\n         \"\"\"Build prompt for content brief generation.\"\"\"\n-        \n+\n         prompt = f\"\"\"Generate a comprehensive content brief for the following:\n \n Topic: {topic}\n Content Type: {content_type.value}\n Target Keywords: {', '.join(target_keywords)}\n@@ -213,22 +235,24 @@\n - Target 80+ SEO score\n - Tone should match content type and audience\n - Include clear call-to-action\n \n \"\"\"\n-        \n+\n         if competitor_analysis:\n-            prompt += f\"\\nCompetitor Analysis:\\n{json.dumps(competitor_analysis, indent=2)}\\n\"\n-        \n+            prompt += (\n+                f\"\\nCompetitor Analysis:\\n{json.dumps(competitor_analysis, indent=2)}\\n\"\n+            )\n+\n         if seo_opportunity:\n             prompt += f\"\\nSEO Opportunity:\\n{json.dumps(seo_opportunity, indent=2)}\\n\"\n-        \n+\n         return prompt\n-    \n+\n     def _build_content_prompt(self, brief: ContentBrief) -> str:\n         \"\"\"Build prompt for full content generation.\"\"\"\n-        \n+\n         prompt = f\"\"\"Write a comprehensive {brief.content_type.value} based on this content brief:\n \n Title: {brief.title}\n Meta Description: {brief.meta_description}\n H1: {brief.h1}\n@@ -256,16 +280,18 @@\n - Include relevant examples and case studies\n - End with a strong conclusion\n \n Please provide the content in HTML format with proper heading tags, paragraphs, and formatting.\n \"\"\"\n-        \n+\n         return prompt\n-    \n-    def _build_optimization_prompt(self, content: GeneratedContent, target_metrics: Dict[str, Any]) -> str:\n+\n+    def _build_optimization_prompt(\n+        self, content: GeneratedContent, target_metrics: Dict[str, Any]\n+    ) -> str:\n         \"\"\"Build prompt for content optimization.\"\"\"\n-        \n+\n         prompt = f\"\"\"Optimize this content for better SEO and readability:\n \n Current Content:\n Title: {content.title}\n Word Count: {content.word_count}\n@@ -287,47 +313,58 @@\n - Improving content structure and flow\n - Adding more internal and external links if needed\n \n Return the optimized content in the same HTML format.\n \"\"\"\n-        \n+\n         return prompt\n-    \n+\n     async def _call_openai_api(self, prompt: str) -> str:\n         \"\"\"Call OpenAI API for content generation.\"\"\"\n-        \n+\n         if not self.session:\n             raise Exception(\"Session not initialized\")\n-        \n+\n         payload = {\n             \"model\": self.model,\n             \"messages\": [\n-                {\"role\": \"system\", \"content\": \"You are an expert SEO content writer and strategist. Generate high-quality, SEO-optimized content that ranks well and provides value to readers.\"},\n-                {\"role\": \"user\", \"content\": prompt}\n+                {\n+                    \"role\": \"system\",\n+                    \"content\": \"You are an expert SEO content writer and strategist. Generate high-quality, SEO-optimized content that ranks well and provides value to readers.\",\n+                },\n+                {\"role\": \"user\", \"content\": prompt},\n             ],\n             \"max_tokens\": 4000,\n-            \"temperature\": 0.7\n+            \"temperature\": 0.7,\n         }\n-        \n-        async with self.session.post(f\"{self.base_url}/chat/completions\", json=payload) as response:\n+\n+        async with self.session.post(\n+            f\"{self.base_url}/chat/completions\", json=payload\n+        ) as response:\n             if response.status != 200:\n                 error_text = await response.text()\n                 raise Exception(f\"OpenAI API error: {response.status} - {error_text}\")\n-            \n+\n             data = await response.json()\n             return data[\"choices\"][0][\"message\"][\"content\"]\n-    \n-    def _parse_brief_response(self, response: str, topic: str, content_type: ContentType, target_keywords: List[str]) -> ContentBrief:\n+\n+    def _parse_brief_response(\n+        self,\n+        response: str,\n+        topic: str,\n+        content_type: ContentType,\n+        target_keywords: List[str],\n+    ) -> ContentBrief:\n         \"\"\"Parse AI response into ContentBrief object.\"\"\"\n-        \n+\n         try:\n             # Extract JSON from response\n-            json_start = response.find('{')\n-            json_end = response.rfind('}') + 1\n+            json_start = response.find(\"{\")\n+            json_end = response.rfind(\"}\") + 1\n             json_str = response[json_start:json_end]\n             brief_data = json.loads(json_str)\n-            \n+\n             return ContentBrief(\n                 id=f\"brief_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n                 title=brief_data.get(\"title\", f\"Content about {topic}\"),\n                 content_type=content_type,\n                 target_keywords=target_keywords,\n@@ -343,28 +380,32 @@\n                 seo_score_target=brief_data.get(\"seo_score_target\", 85),\n                 tone=brief_data.get(\"tone\", \"Professional\"),\n                 target_audience=brief_data.get(\"target_audience\", \"General audience\"),\n                 call_to_action=brief_data.get(\"call_to_action\", \"Learn more\"),\n                 created_at=datetime.now().isoformat(),\n-                updated_at=datetime.now().isoformat()\n+                updated_at=datetime.now().isoformat(),\n             )\n         except Exception as e:\n             logger.error(f\"Error parsing brief response: {str(e)}\")\n             return self._create_fallback_brief(topic, content_type, target_keywords)\n-    \n-    def _parse_content_response(self, response: str, brief: ContentBrief) -> GeneratedContent:\n+\n+    def _parse_content_response(\n+        self, response: str, brief: ContentBrief\n+    ) -> GeneratedContent:\n         \"\"\"Parse AI response into GeneratedContent object.\"\"\"\n-        \n+\n         # Extract content from HTML response\n         content = response.strip()\n-        \n+\n         # Calculate basic metrics\n         word_count = len(content.split())\n         readability_score = self._calculate_readability_score(content)\n         seo_score = self._calculate_seo_score(content, brief)\n-        keyword_density = self._calculate_keyword_density(content, brief.target_keywords)\n-        \n+        keyword_density = self._calculate_keyword_density(\n+            content, brief.target_keywords\n+        )\n+\n         return GeneratedContent(\n             id=f\"content_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n             brief_id=brief.id,\n             title=brief.title,\n             content=content,\n@@ -377,70 +418,81 @@\n             keyword_density=keyword_density,\n             internal_links=brief.internal_links,\n             external_links=brief.external_links,\n             status=ContentStatus.DRAFT,\n             created_at=datetime.now().isoformat(),\n-            updated_at=datetime.now().isoformat()\n-        )\n-    \n-    def _parse_optimization_response(self, response: str, original_content: GeneratedContent) -> GeneratedContent:\n+            updated_at=datetime.now().isoformat(),\n+        )\n+\n+    def _parse_optimization_response(\n+        self, response: str, original_content: GeneratedContent\n+    ) -> GeneratedContent:\n         \"\"\"Parse optimization response into GeneratedContent object.\"\"\"\n-        \n+\n         # Update content with optimized version\n         optimized_content = original_content\n         optimized_content.content = response.strip()\n         optimized_content.word_count = len(response.split())\n-        optimized_content.readability_score = self._calculate_readability_score(response)\n+        optimized_content.readability_score = self._calculate_readability_score(\n+            response\n+        )\n         optimized_content.seo_score = self._calculate_seo_score(response, None)\n         optimized_content.updated_at = datetime.now().isoformat()\n-        \n+\n         return optimized_content\n-    \n-    def _create_fallback_brief(self, topic: str, content_type: ContentType, target_keywords: List[str]) -> ContentBrief:\n+\n+    def _create_fallback_brief(\n+        self, topic: str, content_type: ContentType, target_keywords: List[str]\n+    ) -> ContentBrief:\n         \"\"\"Create fallback brief when AI generation fails.\"\"\"\n-        \n+\n         return ContentBrief(\n             id=f\"brief_fallback_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n             title=f\"Complete Guide to {topic}\",\n             content_type=content_type,\n             target_keywords=target_keywords,\n             primary_keyword=target_keywords[0] if target_keywords else topic,\n             meta_description=f\"Learn everything about {topic} with our comprehensive guide\",\n             h1=f\"Complete Guide to {topic}\",\n-            h2s=[f\"What is {topic}?\", f\"Benefits of {topic}\", f\"How to Use {topic}\", \"Best Practices\"],\n+            h2s=[\n+                f\"What is {topic}?\",\n+                f\"Benefits of {topic}\",\n+                f\"How to Use {topic}\",\n+                \"Best Practices\",\n+            ],\n             content_outline=[\n                 f\"Introduction to {topic}\",\n                 \"Key concepts and benefits\",\n                 \"Step-by-step implementation\",\n                 \"Common challenges and solutions\",\n-                \"Conclusion and next steps\"\n+                \"Conclusion and next steps\",\n             ],\n             internal_links=[\"/related-guide\", \"/contact\"],\n             external_links=[\"https://authority-site.com\"],\n             word_count_target=1500,\n             readability_target=65,\n             seo_score_target=85,\n             tone=\"Professional\",\n             target_audience=\"General audience\",\n             call_to_action=\"Get started today\",\n             created_at=datetime.now().isoformat(),\n-            updated_at=datetime.now().isoformat()\n-        )\n-    \n+            updated_at=datetime.now().isoformat(),\n+        )\n+\n     def _create_fallback_content(self, brief: ContentBrief) -> GeneratedContent:\n         \"\"\"Create fallback content when AI generation fails.\"\"\"\n-        \n+\n         content = f\"\"\"\n         <h1>{brief.h1}</h1>\n         <p>This is a comprehensive guide about {brief.primary_keyword}.</p>\n         \"\"\"\n-        \n+\n         for h2 in brief.h2s:\n             content += f\"<h2>{h2}</h2><p>Detailed information about {h2.lower()}.</p>\"\n-        \n+\n         content += f\"<p>{brief.call_to_action}</p>\"\n-        \n+\n         return GeneratedContent(\n             id=f\"content_fallback_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n             brief_id=brief.id,\n             title=brief.title,\n             content=content,\n@@ -453,80 +505,87 @@\n             keyword_density={brief.primary_keyword: 1.0},\n             internal_links=brief.internal_links,\n             external_links=brief.external_links,\n             status=ContentStatus.DRAFT,\n             created_at=datetime.now().isoformat(),\n-            updated_at=datetime.now().isoformat()\n-        )\n-    \n+            updated_at=datetime.now().isoformat(),\n+        )\n+\n     def _calculate_readability_score(self, content: str) -> float:\n         \"\"\"Calculate Flesch Reading Ease score.\"\"\"\n         # Simplified readability calculation\n-        sentences = content.count('.') + content.count('!') + content.count('?')\n+        sentences = content.count(\".\") + content.count(\"!\") + content.count(\"?\")\n         words = len(content.split())\n-        \n+\n         if sentences == 0 or words == 0:\n             return 50.0\n-        \n+\n         avg_sentence_length = words / sentences\n         avg_syllables = sum(len(word) for word in content.split()) / words\n-        \n+\n         score = 206.835 - (1.015 * avg_sentence_length) - (84.6 * avg_syllables)\n         return max(0, min(100, score))\n-    \n-    def _calculate_seo_score(self, content: str, brief: Optional[ContentBrief]) -> float:\n+\n+    def _calculate_seo_score(\n+        self, content: str, brief: Optional[ContentBrief]\n+    ) -> float:\n         \"\"\"Calculate SEO score.\"\"\"\n         score = 0.0\n-        \n+\n         # Basic SEO factors\n-        if '<h1>' in content:\n+        if \"<h1>\" in content:\n             score += 20\n-        if '<h2>' in content:\n+        if \"<h2>\" in content:\n             score += 15\n         if len(content) > 500:\n             score += 20\n         if len(content) > 1000:\n             score += 15\n-        if 'meta' in content.lower():\n+        if \"meta\" in content.lower():\n             score += 10\n-        if 'alt=' in content:\n+        if \"alt=\" in content:\n             score += 10\n-        if '<a href=' in content:\n+        if \"<a href=\" in content:\n             score += 10\n-        \n+\n         return min(100, score)\n-    \n-    def _calculate_keyword_density(self, content: str, keywords: List[str]) -> Dict[str, float]:\n+\n+    def _calculate_keyword_density(\n+        self, content: str, keywords: List[str]\n+    ) -> Dict[str, float]:\n         \"\"\"Calculate keyword density.\"\"\"\n         word_count = len(content.split())\n         if word_count == 0:\n             return {}\n-        \n+\n         density = {}\n         for keyword in keywords:\n             count = content.lower().count(keyword.lower())\n             density[keyword] = (count / word_count) * 100\n-        \n+\n         return density\n+\n \n class ContentCalendarManager:\n     \"\"\"Manages content calendar and publishing workflow.\"\"\"\n-    \n+\n     def __init__(self):\n         self.calendar_entries: List[ContentCalendar] = []\n-    \n-    def add_content_to_calendar(self, \n-                               title: str,\n-                               content_type: ContentType,\n-                               scheduled_date: str,\n-                               brief_id: Optional[str] = None,\n-                               content_id: Optional[str] = None,\n-                               priority: str = \"medium\",\n-                               assigned_to: Optional[str] = None,\n-                               notes: str = \"\") -> ContentCalendar:\n+\n+    def add_content_to_calendar(\n+        self,\n+        title: str,\n+        content_type: ContentType,\n+        scheduled_date: str,\n+        brief_id: Optional[str] = None,\n+        content_id: Optional[str] = None,\n+        priority: str = \"medium\",\n+        assigned_to: Optional[str] = None,\n+        notes: str = \"\",\n+    ) -> ContentCalendar:\n         \"\"\"Add content to calendar.\"\"\"\n-        \n+\n         entry = ContentCalendar(\n             id=f\"calendar_{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n             title=title,\n             content_type=content_type,\n             brief_id=brief_id,\n@@ -535,74 +594,83 @@\n             publish_date=None,\n             status=ContentStatus.DRAFT,\n             priority=priority,\n             assigned_to=assigned_to,\n             notes=notes,\n-            created_at=datetime.now().isoformat()\n-        )\n-        \n+            created_at=datetime.now().isoformat(),\n+        )\n+\n         self.calendar_entries.append(entry)\n         return entry\n-    \n+\n     def get_upcoming_content(self, days: int = 30) -> List[ContentCalendar]:\n         \"\"\"Get upcoming content for the next N days.\"\"\"\n-        \n+\n         cutoff_date = datetime.now() + timedelta(days=days)\n-        \n+\n         return [\n-            entry for entry in self.calendar_entries\n+            entry\n+            for entry in self.calendar_entries\n             if datetime.fromisoformat(entry.scheduled_date) <= cutoff_date\n         ]\n-    \n+\n     def update_content_status(self, content_id: str, status: ContentStatus) -> bool:\n         \"\"\"Update content status.\"\"\"\n-        \n+\n         for entry in self.calendar_entries:\n             if entry.content_id == content_id:\n                 entry.status = status\n                 if status == ContentStatus.PUBLISHED:\n                     entry.publish_date = datetime.now().isoformat()\n                 return True\n-        \n+\n         return False\n+\n \n async def main():\n     \"\"\"Main function to demonstrate AI content generation.\"\"\"\n-    \n+\n     # Sample data\n     topic = \"SEO Best Practices for E-commerce\"\n     content_type = ContentType.BLOG_POST\n-    target_keywords = [\"seo best practices\", \"e-commerce seo\", \"online store optimization\"]\n-    \n+    target_keywords = [\n+        \"seo best practices\",\n+        \"e-commerce seo\",\n+        \"online store optimization\",\n+    ]\n+\n     # Initialize AI content generator\n     async with AIContentGenerator() as generator:\n         # Generate content brief\n         print(\"Generating content brief...\")\n-        brief = await generator.generate_content_brief(topic, content_type, target_keywords)\n-        \n+        brief = await generator.generate_content_brief(\n+            topic, content_type, target_keywords\n+        )\n+\n         print(\"\u2705 Content brief generated!\")\n         print(f\"Title: {brief.title}\")\n         print(f\"Target Keywords: {brief.target_keywords}\")\n         print(f\"Word Count Target: {brief.word_count_target}\")\n-        \n+\n         # Generate full content\n         print(\"\\nGenerating full content...\")\n         content = await generator.generate_content(brief)\n-        \n+\n         print(\"\u2705 Content generated!\")\n         print(f\"Word Count: {content.word_count}\")\n         print(f\"Readability Score: {content.readability_score:.1f}\")\n         print(f\"SEO Score: {content.seo_score:.1f}\")\n-        \n+\n         # Save results\n         with open(\"ai_generated_brief.json\", \"w\") as f:\n             f.write(json.dumps(asdict(brief), indent=2))\n-        \n+\n         with open(\"ai_generated_content.json\", \"w\") as f:\n             f.write(json.dumps(asdict(content), indent=2))\n-        \n+\n         print(\"\\nFiles created:\")\n         print(\"- ai_generated_brief.json\")\n         print(\"- ai_generated_content.json\")\n \n+\n if __name__ == \"__main__\":\n     asyncio.run(main())\n--- /home/justin/llama_rag/app/seo-api/content_crawler.py\t2025-09-29 06:32:34.545259+00:00\n+++ /home/justin/llama_rag/app/seo-api/content_crawler.py\t2025-09-29 06:34:08.980801+00:00\n@@ -20,195 +20,203 @@\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n @dataclass\n class CrawlResult:\n     \"\"\"Result of crawling a single page.\"\"\"\n+\n     url: str\n     success: bool\n     content: Optional[str] = None\n     error: Optional[str] = None\n     response_time: float = 0.0\n     status_code: int = 0\n \n+\n @dataclass\n class SiteAuditReport:\n     \"\"\"Complete site audit report.\"\"\"\n+\n     base_url: str\n     pages_analyzed: int\n     total_pages: int\n     average_score: float\n     high_priority_pages: List[str]\n     optimization_reports: List[ContentOptimizationReport]\n     created_at: str\n \n+\n class ContentCrawler:\n     \"\"\"Crawls websites for content optimization analysis.\"\"\"\n-    \n+\n     def __init__(self, max_concurrent: int = 5, delay: float = 1.0):\n         self.max_concurrent = max_concurrent\n         self.delay = delay\n         self.optimizer = ContentOptimizer()\n         self.visited_urls: Set[str] = set()\n         self.session: Optional[aiohttp.ClientSession] = None\n-    \n+\n     async def __aenter__(self):\n         self.session = aiohttp.ClientSession(\n             timeout=aiohttp.ClientTimeout(total=30),\n-            headers={'User-Agent': 'SEO-Content-Optimizer/1.0 (Content Analysis Bot)'}\n+            headers={\"User-Agent\": \"SEO-Content-Optimizer/1.0 (Content Analysis Bot)\"},\n         )\n         return self\n-    \n+\n     async def __aexit__(self, exc_type, exc_val, exc_tb):\n         if self.session:\n             await self.session.close()\n-    \n+\n     async def crawl_site(self, base_url: str, max_pages: int = 10) -> SiteAuditReport:\n         \"\"\"Crawl a website and analyze content for optimization.\"\"\"\n-        \n+\n         logger.info(f\"Starting site crawl for {base_url}\")\n-        \n+\n         # Discover pages to crawl\n         pages_to_crawl = await self._discover_pages(base_url, max_pages)\n-        \n+\n         # Crawl pages concurrently\n         crawl_results = await self._crawl_pages(pages_to_crawl)\n-        \n+\n         # Analyze content\n         optimization_reports = []\n         for result in crawl_results:\n             if result.success and result.content:\n                 try:\n                     report = self.optimizer.optimize_content(result.content, result.url)\n                     optimization_reports.append(report)\n                 except Exception as e:\n                     logger.error(f\"Error analyzing {result.url}: {str(e)}\")\n-        \n+\n         # Generate site audit report\n-        audit_report = self._generate_audit_report(base_url, crawl_results, optimization_reports)\n-        \n+        audit_report = self._generate_audit_report(\n+            base_url, crawl_results, optimization_reports\n+        )\n+\n         logger.info(f\"Site crawl complete. Analyzed {len(optimization_reports)} pages\")\n-        \n+\n         return audit_report\n-    \n+\n     async def _discover_pages(self, base_url: str, max_pages: int) -> List[str]:\n         \"\"\"Discover pages to crawl on the site.\"\"\"\n         pages = [base_url]\n-        \n+\n         # Simple page discovery - in production, use sitemap or more sophisticated crawling\n         # For now, we'll just return the base URL and a few common paths\n         common_paths = [\n-            '/about',\n-            '/contact',\n-            '/blog',\n-            '/products',\n-            '/services',\n-            '/privacy',\n-            '/terms'\n+            \"/about\",\n+            \"/contact\",\n+            \"/blog\",\n+            \"/products\",\n+            \"/services\",\n+            \"/privacy\",\n+            \"/terms\",\n         ]\n-        \n+\n         for path in common_paths:\n             if len(pages) >= max_pages:\n                 break\n             full_url = urljoin(base_url, path)\n             pages.append(full_url)\n-        \n+\n         return pages[:max_pages]\n-    \n+\n     async def _crawl_pages(self, urls: List[str]) -> List[CrawlResult]:\n         \"\"\"Crawl multiple pages concurrently.\"\"\"\n         semaphore = asyncio.Semaphore(self.max_concurrent)\n-        \n+\n         async def crawl_single_page(url: str) -> CrawlResult:\n             async with semaphore:\n                 return await self._crawl_single_page(url)\n-        \n+\n         tasks = [crawl_single_page(url) for url in urls]\n         results = await asyncio.gather(*tasks, return_exceptions=True)\n-        \n+\n         # Handle exceptions\n         crawl_results = []\n         for i, result in enumerate(results):\n             if isinstance(result, Exception):\n-                crawl_results.append(CrawlResult(\n-                    url=urls[i],\n-                    success=False,\n-                    error=str(result)\n-                ))\n+                crawl_results.append(\n+                    CrawlResult(url=urls[i], success=False, error=str(result))\n+                )\n             else:\n                 crawl_results.append(result)\n-        \n+\n         return crawl_results\n-    \n+\n     async def _crawl_single_page(self, url: str) -> CrawlResult:\n         \"\"\"Crawl a single page.\"\"\"\n         start_time = time.time()\n-        \n+\n         try:\n             async with self.session.get(url) as response:\n                 content = await response.text()\n                 response_time = time.time() - start_time\n-                \n+\n                 return CrawlResult(\n                     url=url,\n                     success=True,\n                     content=content,\n                     response_time=response_time,\n-                    status_code=response.status\n+                    status_code=response.status,\n                 )\n-        \n+\n         except Exception as e:\n             response_time = time.time() - start_time\n             return CrawlResult(\n-                url=url,\n-                success=False,\n-                error=str(e),\n-                response_time=response_time\n+                url=url, success=False, error=str(e), response_time=response_time\n             )\n-    \n-    def _generate_audit_report(self, base_url: str, crawl_results: List[CrawlResult], \n-                             optimization_reports: List[ContentOptimizationReport]) -> SiteAuditReport:\n+\n+    def _generate_audit_report(\n+        self,\n+        base_url: str,\n+        crawl_results: List[CrawlResult],\n+        optimization_reports: List[ContentOptimizationReport],\n+    ) -> SiteAuditReport:\n         \"\"\"Generate site audit report.\"\"\"\n-        \n+\n         total_pages = len(crawl_results)\n         pages_analyzed = len(optimization_reports)\n-        \n+\n         # Calculate average score\n         if optimization_reports:\n-            average_score = sum(r.analysis.overall_score for r in optimization_reports) / len(optimization_reports)\n+            average_score = sum(\n+                r.analysis.overall_score for r in optimization_reports\n+            ) / len(optimization_reports)\n         else:\n             average_score = 0.0\n-        \n+\n         # Identify high-priority pages (score below 50)\n         high_priority_pages = [\n-            r.url for r in optimization_reports \n-            if r.analysis.overall_score < 50\n+            r.url for r in optimization_reports if r.analysis.overall_score < 50\n         ]\n-        \n+\n         return SiteAuditReport(\n             base_url=base_url,\n             pages_analyzed=pages_analyzed,\n             total_pages=total_pages,\n             average_score=average_score,\n             high_priority_pages=high_priority_pages,\n             optimization_reports=optimization_reports,\n-            created_at=time.strftime(\"%Y-%m-%d %H:%M:%S\")\n+            created_at=time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n         )\n-    \n+\n     def export_audit_report(self, report: SiteAuditReport, format: str = \"json\") -> str:\n         \"\"\"Export site audit report.\"\"\"\n         if format == \"json\":\n             import json\n             from dataclasses import asdict\n+\n             return json.dumps(asdict(report), indent=2, default=str)\n         elif format == \"markdown\":\n             return self._generate_markdown_audit_report(report)\n         else:\n             raise ValueError(f\"Unsupported format: {format}\")\n-    \n+\n     def _generate_markdown_audit_report(self, report: SiteAuditReport) -> str:\n         \"\"\"Generate markdown audit report.\"\"\"\n         md_content = f\"\"\"# Site Content Audit Report\n \n ## Site: {report.base_url}\n@@ -223,11 +231,11 @@\n ## High-Priority Pages (Score < 50)\n \n \"\"\"\n         for url in report.high_priority_pages:\n             md_content += f\"- {url}\\n\"\n-        \n+\n         md_content += \"\"\"\n ## Page-by-Page Analysis\n \n \"\"\"\n         for opt_report in report.optimization_reports:\n@@ -243,24 +251,26 @@\n \n **Issues:**\n \"\"\"\n             for issue in analysis.issues:\n                 md_content += f\"- \u274c {issue}\\n\"\n-            \n+\n             md_content += \"\"\"\n **Suggestions:**\n \"\"\"\n             high_priority = [s for s in opt_report.suggestions if s.priority == \"high\"]\n             for suggestion in high_priority:\n                 md_content += f\"- \ud83d\udd34 **{suggestion.title}**: {suggestion.description}\\n\"\n-            \n-            medium_priority = [s for s in opt_report.suggestions if s.priority == \"medium\"]\n+\n+            medium_priority = [\n+                s for s in opt_report.suggestions if s.priority == \"medium\"\n+            ]\n             for suggestion in medium_priority:\n                 md_content += f\"- \ud83d\udfe1 **{suggestion.title}**: {suggestion.description}\\n\"\n-            \n+\n             md_content += \"\\n---\\n\\n\"\n-        \n+\n         md_content += \"\"\"\n ## Recommendations\n \n 1. **Focus on high-priority pages first** - Address pages with scores below 50\n 2. **Improve content structure** - Add proper headings and improve readability\n@@ -272,48 +282,49 @@\n 1. Implement high-priority optimizations\n 2. Re-analyze pages after changes\n 3. Set up ongoing monitoring\n 4. Track performance improvements\n \"\"\"\n-        \n+\n         return md_content\n+\n \n async def main():\n     \"\"\"Main function to demonstrate content crawling.\"\"\"\n-    \n+\n     # Test URLs\n-    test_urls = [\n-        \"https://example.com\",\n-        \"https://httpbin.org/html\"\n-    ]\n-    \n+    test_urls = [\"https://example.com\", \"https://httpbin.org/html\"]\n+\n     for url in test_urls:\n         print(f\"\\nCrawling {url}...\")\n-        \n+\n         async with ContentCrawler(max_concurrent=3, delay=1.0) as crawler:\n             try:\n                 audit_report = await crawler.crawl_site(url, max_pages=5)\n-                \n+\n                 print(\"Site audit complete!\")\n                 print(f\"Pages analyzed: {audit_report.pages_analyzed}\")\n                 print(f\"Average score: {audit_report.average_score:.1f}/100\")\n                 print(f\"High-priority pages: {len(audit_report.high_priority_pages)}\")\n-                \n+\n                 # Export reports\n                 json_report = crawler.export_audit_report(audit_report, \"json\")\n                 markdown_report = crawler.export_audit_report(audit_report, \"markdown\")\n-                \n+\n                 # Save reports\n-                domain = urlparse(url).netloc.replace('.', '_')\n+                domain = urlparse(url).netloc.replace(\".\", \"_\")\n                 with open(f\"site_audit_{domain}.json\", \"w\") as f:\n                     f.write(json_report)\n-                \n+\n                 with open(f\"site_audit_{domain}.md\", \"w\") as f:\n                     f.write(markdown_report)\n-                \n-                print(f\"Reports saved: site_audit_{domain}.json, site_audit_{domain}.md\")\n-                \n+\n+                print(\n+                    f\"Reports saved: site_audit_{domain}.json, site_audit_{domain}.md\"\n+                )\n+\n             except Exception as e:\n                 print(f\"Error crawling {url}: {str(e)}\")\n \n+\n if __name__ == \"__main__\":\n     asyncio.run(main())\n--- /home/justin/llama_rag/app/seo-api/automation/competitor_analyzer.py\t2025-09-29 06:31:23.225240+00:00\n+++ /home/justin/llama_rag/app/seo-api/automation/competitor_analyzer.py\t2025-09-29 06:34:08.989237+00:00\n@@ -23,13 +23,15 @@\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n @dataclass\n class CompetitorContent:\n     \"\"\"Represents competitor content analysis.\"\"\"\n+\n     url: str\n     title: str\n     meta_description: str\n     h1: str\n     h2s: List[str]\n@@ -44,13 +46,15 @@\n     social_shares: int\n     estimated_traffic: int\n     domain_authority: int\n     created_at: str\n \n+\n @dataclass\n class ContentGap:\n     \"\"\"Represents a content gap opportunity.\"\"\"\n+\n     keyword: str\n     search_volume: int\n     competition: str\n     competitor_urls: List[str]\n     content_type: str\n@@ -59,88 +63,101 @@\n     suggested_outline: List[str]\n     estimated_difficulty: str\n     priority: str\n     created_at: str\n \n+\n @dataclass\n class CompetitorAnalysis:\n     \"\"\"Complete competitor analysis report.\"\"\"\n+\n     competitor_domain: str\n     total_pages_analyzed: int\n     content_gaps: List[ContentGap]\n     top_performing_content: List[CompetitorContent]\n     keyword_opportunities: List[Dict[str, Any]]\n     content_recommendations: List[Dict[str, Any]]\n     analysis_date: str\n \n+\n class CompetitorContentAnalyzer:\n     \"\"\"Analyzes competitor content for opportunities.\"\"\"\n-    \n+\n     def __init__(self, max_concurrent: int = 5, delay: float = 1.0):\n         self.max_concurrent = max_concurrent\n         self.delay = delay\n         self.session: Optional[aiohttp.ClientSession] = None\n         self.analyzed_urls: set = set()\n-    \n+\n     async def __aenter__(self):\n         self.session = aiohttp.ClientSession(\n             timeout=aiohttp.ClientTimeout(total=30),\n-            headers={'User-Agent': 'SEO-Competitor-Analyzer/1.0 (Content Analysis Bot)'}\n+            headers={\n+                \"User-Agent\": \"SEO-Competitor-Analyzer/1.0 (Content Analysis Bot)\"\n+            },\n         )\n         return self\n-    \n+\n     async def __aexit__(self, exc_type, exc_val, exc_tb):\n         if self.session:\n             await self.session.close()\n-    \n-    async def analyze_competitor(self, competitor_domain: str, max_pages: int = 20) -> CompetitorAnalysis:\n+\n+    async def analyze_competitor(\n+        self, competitor_domain: str, max_pages: int = 20\n+    ) -> CompetitorAnalysis:\n         \"\"\"Analyze competitor content comprehensively.\"\"\"\n-        \n+\n         logger.info(f\"Starting competitor analysis for {competitor_domain}\")\n-        \n+\n         # Discover competitor pages\n-        pages_to_analyze = await self._discover_competitor_pages(competitor_domain, max_pages)\n-        \n+        pages_to_analyze = await self._discover_competitor_pages(\n+            competitor_domain, max_pages\n+        )\n+\n         # Analyze each page\n         competitor_content = []\n         for page_url in pages_to_analyze:\n             try:\n                 content = await self._analyze_competitor_page(page_url)\n                 if content:\n                     competitor_content.append(content)\n                     await asyncio.sleep(self.delay)  # Rate limiting\n             except Exception as e:\n                 logger.error(f\"Error analyzing {page_url}: {str(e)}\")\n-        \n+\n         # Identify content gaps\n         content_gaps = self._identify_content_gaps(competitor_content)\n-        \n+\n         # Find top performing content\n         top_content = self._find_top_performing_content(competitor_content)\n-        \n+\n         # Identify keyword opportunities\n         keyword_opportunities = self._identify_keyword_opportunities(competitor_content)\n-        \n+\n         # Generate content recommendations\n-        content_recommendations = self._generate_content_recommendations(competitor_content, content_gaps)\n-        \n+        content_recommendations = self._generate_content_recommendations(\n+            competitor_content, content_gaps\n+        )\n+\n         return CompetitorAnalysis(\n             competitor_domain=competitor_domain,\n             total_pages_analyzed=len(competitor_content),\n             content_gaps=content_gaps,\n             top_performing_content=top_content,\n             keyword_opportunities=keyword_opportunities,\n             content_recommendations=content_recommendations,\n-            analysis_date=datetime.now().isoformat()\n-        )\n-    \n-    async def _discover_competitor_pages(self, domain: str, max_pages: int) -> List[str]:\n+            analysis_date=datetime.now().isoformat(),\n+        )\n+\n+    async def _discover_competitor_pages(\n+        self, domain: str, max_pages: int\n+    ) -> List[str]:\n         \"\"\"Discover pages to analyze on competitor site.\"\"\"\n-        \n+\n         pages = []\n         base_url = f\"https://{domain}\"\n-        \n+\n         # Common page patterns to check\n         common_paths = [\n             \"/\",\n             \"/about\",\n             \"/blog\",\n@@ -149,118 +166,119 @@\n             \"/contact\",\n             \"/privacy\",\n             \"/terms\",\n             \"/faq\",\n             \"/help\",\n-            \"/support\"\n+            \"/support\",\n         ]\n-        \n+\n         # Add common paths\n         for path in common_paths:\n             if len(pages) >= max_pages:\n                 break\n             pages.append(urljoin(base_url, path))\n-        \n+\n         # Try to discover blog posts\n         blog_pages = await self._discover_blog_pages(base_url, max_pages - len(pages))\n         pages.extend(blog_pages)\n-        \n+\n         return pages[:max_pages]\n-    \n+\n     async def _discover_blog_pages(self, base_url: str, max_pages: int) -> List[str]:\n         \"\"\"Discover blog posts and articles.\"\"\"\n-        \n+\n         blog_urls = []\n-        \n+\n         # Common blog patterns\n-        blog_patterns = [\n-            \"/blog/\",\n-            \"/articles/\",\n-            \"/news/\",\n-            \"/insights/\",\n-            \"/resources/\"\n-        ]\n-        \n+        blog_patterns = [\"/blog/\", \"/articles/\", \"/news/\", \"/insights/\", \"/resources/\"]\n+\n         for pattern in blog_patterns:\n             if len(blog_urls) >= max_pages:\n                 break\n-            \n+\n             try:\n                 blog_url = urljoin(base_url, pattern)\n                 async with self.session.get(blog_url) as response:\n                     if response.status == 200:\n                         content = await response.text()\n                         # Extract article links\n                         article_links = self._extract_article_links(content, base_url)\n-                        blog_urls.extend(article_links[:max_pages - len(blog_urls)])\n+                        blog_urls.extend(article_links[: max_pages - len(blog_urls)])\n             except Exception as e:\n                 logger.error(f\"Error discovering blog pages for {pattern}: {str(e)}\")\n-        \n+\n         return blog_urls[:max_pages]\n-    \n+\n     def _extract_article_links(self, html_content: str, base_url: str) -> List[str]:\n         \"\"\"Extract article links from HTML content.\"\"\"\n-        \n+\n         # Simple regex to find article links\n         link_pattern = r'<a[^>]*href=[\"\\']([^\"\\']*)[\"\\'][^>]*>'\n         links = re.findall(link_pattern, html_content)\n-        \n+\n         article_links = []\n         for link in links:\n             full_url = urljoin(base_url, link)\n             if self._is_article_url(full_url):\n                 article_links.append(full_url)\n-        \n+\n         return article_links[:10]  # Limit to first 10\n-    \n+\n     def _is_article_url(self, url: str) -> bool:\n         \"\"\"Check if URL is likely an article.\"\"\"\n-        \n+\n         article_indicators = [\n-            '/blog/', '/article/', '/post/', '/news/',\n-            '/insights/', '/resources/', '/guide/',\n-            '/tutorial/', '/how-to/', '/tips/'\n+            \"/blog/\",\n+            \"/article/\",\n+            \"/post/\",\n+            \"/news/\",\n+            \"/insights/\",\n+            \"/resources/\",\n+            \"/guide/\",\n+            \"/tutorial/\",\n+            \"/how-to/\",\n+            \"/tips/\",\n         ]\n-        \n+\n         return any(indicator in url.lower() for indicator in article_indicators)\n-    \n+\n     async def _analyze_competitor_page(self, url: str) -> Optional[CompetitorContent]:\n         \"\"\"Analyze a single competitor page.\"\"\"\n-        \n+\n         if url in self.analyzed_urls:\n             return None\n-        \n+\n         try:\n             async with self.session.get(url) as response:\n                 if response.status != 200:\n                     return None\n-                \n+\n                 html_content = await response.text()\n                 self.analyzed_urls.add(url)\n-                \n+\n                 # Extract content elements\n                 title = self._extract_title(html_content)\n                 meta_description = self._extract_meta_description(html_content)\n                 h1 = self._extract_h1(html_content)\n                 h2s = self._extract_h2s(html_content)\n                 content = self._extract_text_content(html_content)\n-                \n+\n                 # Calculate metrics\n                 word_count = len(content.split())\n                 content_type = self._determine_content_type(url, title, content)\n                 target_keywords = self._extract_keywords(content, title)\n-                \n+\n                 # Extract links\n                 internal_links = self._extract_internal_links(html_content, url)\n                 external_links = self._extract_external_links(html_content, url)\n                 images = self._extract_images(html_content)\n-                \n+\n                 # Estimate performance metrics\n                 social_shares = self._estimate_social_shares(url)\n                 estimated_traffic = self._estimate_traffic(word_count, content_type)\n                 domain_authority = self._estimate_domain_authority(url)\n-                \n+\n                 return CompetitorContent(\n                     url=url,\n                     title=title,\n                     meta_description=meta_description,\n                     h1=h1,\n@@ -274,195 +292,262 @@\n                     external_links=external_links,\n                     images=images,\n                     social_shares=social_shares,\n                     estimated_traffic=estimated_traffic,\n                     domain_authority=domain_authority,\n-                    created_at=datetime.now().isoformat()\n+                    created_at=datetime.now().isoformat(),\n                 )\n-        \n+\n         except Exception as e:\n             logger.error(f\"Error analyzing page {url}: {str(e)}\")\n             return None\n-    \n+\n     def _extract_title(self, html: str) -> str:\n         \"\"\"Extract page title.\"\"\"\n-        title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)\n-        return re.sub(r'<[^>]+>', '', title_match.group(1)).strip() if title_match else \"\"\n-    \n+        title_match = re.search(\n+            r\"<title[^>]*>(.*?)</title>\", html, re.IGNORECASE | re.DOTALL\n+        )\n+        return (\n+            re.sub(r\"<[^>]+>\", \"\", title_match.group(1)).strip() if title_match else \"\"\n+        )\n+\n     def _extract_meta_description(self, html: str) -> str:\n         \"\"\"Extract meta description.\"\"\"\n-        desc_match = re.search(r'<meta[^>]*name=[\"\\']description[\"\\'][^>]*content=[\"\\']([^\"\\']*)[\"\\']', html, re.IGNORECASE)\n+        desc_match = re.search(\n+            r'<meta[^>]*name=[\"\\']description[\"\\'][^>]*content=[\"\\']([^\"\\']*)[\"\\']',\n+            html,\n+            re.IGNORECASE,\n+        )\n         return desc_match.group(1).strip() if desc_match else \"\"\n-    \n+\n     def _extract_h1(self, html: str) -> str:\n         \"\"\"Extract H1 tag.\"\"\"\n-        h1_match = re.search(r'<h1[^>]*>(.*?)</h1>', html, re.IGNORECASE | re.DOTALL)\n-        return re.sub(r'<[^>]+>', '', h1_match.group(1)).strip() if h1_match else \"\"\n-    \n+        h1_match = re.search(r\"<h1[^>]*>(.*?)</h1>\", html, re.IGNORECASE | re.DOTALL)\n+        return re.sub(r\"<[^>]+>\", \"\", h1_match.group(1)).strip() if h1_match else \"\"\n+\n     def _extract_h2s(self, html: str) -> List[str]:\n         \"\"\"Extract H2 tags.\"\"\"\n-        h2_matches = re.findall(r'<h2[^>]*>(.*?)</h2>', html, re.IGNORECASE | re.DOTALL)\n-        return [re.sub(r'<[^>]+>', '', h2).strip() for h2 in h2_matches]\n-    \n+        h2_matches = re.findall(r\"<h2[^>]*>(.*?)</h2>\", html, re.IGNORECASE | re.DOTALL)\n+        return [re.sub(r\"<[^>]+>\", \"\", h2).strip() for h2 in h2_matches]\n+\n     def _extract_text_content(self, html: str) -> str:\n         \"\"\"Extract main text content from HTML.\"\"\"\n         # Remove script and style tags\n-        content = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.IGNORECASE | re.DOTALL)\n-        content = re.sub(r'<style[^>]*>.*?</style>', '', content, flags=re.IGNORECASE | re.DOTALL)\n-        \n+        content = re.sub(\n+            r\"<script[^>]*>.*?</script>\", \"\", html, flags=re.IGNORECASE | re.DOTALL\n+        )\n+        content = re.sub(\n+            r\"<style[^>]*>.*?</style>\", \"\", content, flags=re.IGNORECASE | re.DOTALL\n+        )\n+\n         # Remove HTML tags\n-        content = re.sub(r'<[^>]+>', ' ', content)\n-        \n+        content = re.sub(r\"<[^>]+>\", \" \", content)\n+\n         # Clean up whitespace\n-        content = re.sub(r'\\s+', ' ', content).strip()\n-        \n+        content = re.sub(r\"\\s+\", \" \", content).strip()\n+\n         return content\n-    \n+\n     def _determine_content_type(self, url: str, title: str, content: str) -> str:\n         \"\"\"Determine content type based on URL and content.\"\"\"\n-        \n+\n         url_lower = url.lower()\n         title_lower = title.lower()\n-        \n-        if any(word in url_lower for word in ['blog', 'article', 'post']):\n-            return 'blog_post'\n-        elif any(word in url_lower for word in ['product', 'item']):\n-            return 'product_page'\n-        elif any(word in url_lower for word in ['about', 'company']):\n-            return 'about_page'\n-        elif any(word in url_lower for word in ['contact', 'support']):\n-            return 'contact_page'\n-        elif any(word in title_lower for word in ['guide', 'tutorial', 'how-to']):\n-            return 'guide'\n-        elif any(word in title_lower for word in ['faq', 'questions']):\n-            return 'faq'\n+\n+        if any(word in url_lower for word in [\"blog\", \"article\", \"post\"]):\n+            return \"blog_post\"\n+        elif any(word in url_lower for word in [\"product\", \"item\"]):\n+            return \"product_page\"\n+        elif any(word in url_lower for word in [\"about\", \"company\"]):\n+            return \"about_page\"\n+        elif any(word in url_lower for word in [\"contact\", \"support\"]):\n+            return \"contact_page\"\n+        elif any(word in title_lower for word in [\"guide\", \"tutorial\", \"how-to\"]):\n+            return \"guide\"\n+        elif any(word in title_lower for word in [\"faq\", \"questions\"]):\n+            return \"faq\"\n         else:\n-            return 'general_page'\n-    \n+            return \"general_page\"\n+\n     def _extract_keywords(self, content: str, title: str) -> List[str]:\n         \"\"\"Extract potential target keywords from content.\"\"\"\n-        \n+\n         # Simple keyword extraction\n-        words = re.findall(r'\\b\\w+\\b', content.lower())\n+        words = re.findall(r\"\\b\\w+\\b\", content.lower())\n         word_freq = Counter(words)\n-        \n+\n         # Filter out common words and short words\n-        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those'}\n-        \n+        stop_words = {\n+            \"the\",\n+            \"a\",\n+            \"an\",\n+            \"and\",\n+            \"or\",\n+            \"but\",\n+            \"in\",\n+            \"on\",\n+            \"at\",\n+            \"to\",\n+            \"for\",\n+            \"of\",\n+            \"with\",\n+            \"by\",\n+            \"is\",\n+            \"are\",\n+            \"was\",\n+            \"were\",\n+            \"be\",\n+            \"been\",\n+            \"being\",\n+            \"have\",\n+            \"has\",\n+            \"had\",\n+            \"do\",\n+            \"does\",\n+            \"did\",\n+            \"will\",\n+            \"would\",\n+            \"could\",\n+            \"should\",\n+            \"may\",\n+            \"might\",\n+            \"must\",\n+            \"can\",\n+            \"this\",\n+            \"that\",\n+            \"these\",\n+            \"those\",\n+        }\n+\n         keywords = []\n         for word, count in word_freq.most_common(20):\n             if len(word) > 3 and word not in stop_words and count > 2:\n                 keywords.append(word)\n-        \n+\n         return keywords[:10]\n-    \n+\n     def _extract_internal_links(self, html: str, base_url: str) -> List[str]:\n         \"\"\"Extract internal links from HTML.\"\"\"\n-        link_matches = re.findall(r'<a[^>]*href=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE)\n+        link_matches = re.findall(\n+            r'<a[^>]*href=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE\n+        )\n         internal_links = []\n         base_domain = urlparse(base_url).netloc\n-        \n+\n         for link in link_matches:\n-            if link.startswith('/') or urlparse(link).netloc == base_domain:\n+            if link.startswith(\"/\") or urlparse(link).netloc == base_domain:\n                 internal_links.append(urljoin(base_url, link))\n-        \n+\n         return internal_links[:10]\n-    \n+\n     def _extract_external_links(self, html: str, base_url: str) -> List[str]:\n         \"\"\"Extract external links from HTML.\"\"\"\n-        link_matches = re.findall(r'<a[^>]*href=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE)\n+        link_matches = re.findall(\n+            r'<a[^>]*href=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE\n+        )\n         external_links = []\n         base_domain = urlparse(base_url).netloc\n-        \n+\n         for link in link_matches:\n             parsed_link = urlparse(link)\n             if parsed_link.netloc and parsed_link.netloc != base_domain:\n                 external_links.append(link)\n-        \n+\n         return external_links[:10]\n-    \n+\n     def _extract_images(self, html: str) -> List[str]:\n         \"\"\"Extract image URLs from HTML.\"\"\"\n-        img_matches = re.findall(r'<img[^>]*src=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE)\n+        img_matches = re.findall(\n+            r'<img[^>]*src=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE\n+        )\n         return img_matches[:10]\n-    \n+\n     def _extract_publish_date(self, html: str) -> Optional[str]:\n         \"\"\"Extract publish date from HTML.\"\"\"\n         # Look for common date patterns\n         date_patterns = [\n             r'<meta[^>]*property=[\"\\']article:published_time[\"\\'][^>]*content=[\"\\']([^\"\\']*)[\"\\']',\n             r'<time[^>]*datetime=[\"\\']([^\"\\']*)[\"\\']',\n-            r'<span[^>]*class=[\"\\'][^\"\\']*date[^\"\\']*[\"\\'][^>]*>([^<]*)</span>'\n+            r'<span[^>]*class=[\"\\'][^\"\\']*date[^\"\\']*[\"\\'][^>]*>([^<]*)</span>',\n         ]\n-        \n+\n         for pattern in date_patterns:\n             match = re.search(pattern, html, re.IGNORECASE)\n             if match:\n                 return match.group(1)\n-        \n+\n         return None\n-    \n+\n     def _estimate_social_shares(self, url: str) -> int:\n         \"\"\"Estimate social shares (mock implementation).\"\"\"\n         # In production, integrate with social media APIs\n         return hash(url) % 1000\n-    \n+\n     def _estimate_traffic(self, word_count: int, content_type: str) -> int:\n         \"\"\"Estimate traffic based on content characteristics.\"\"\"\n         base_traffic = 100\n-        \n+\n         # Adjust based on word count\n         if word_count > 2000:\n             base_traffic *= 2\n         elif word_count > 1000:\n             base_traffic *= 1.5\n-        \n+\n         # Adjust based on content type\n-        if content_type == 'blog_post':\n+        if content_type == \"blog_post\":\n             base_traffic *= 1.5\n-        elif content_type == 'guide':\n+        elif content_type == \"guide\":\n             base_traffic *= 2\n-        \n+\n         return int(base_traffic)\n-    \n+\n     def _estimate_domain_authority(self, url: str) -> int:\n         \"\"\"Estimate domain authority (mock implementation).\"\"\"\n         # In production, integrate with Moz API or similar\n         domain = urlparse(url).netloc\n         return hash(domain) % 100\n-    \n-    def _identify_content_gaps(self, competitor_content: List[CompetitorContent]) -> List[ContentGap]:\n+\n+    def _identify_content_gaps(\n+        self, competitor_content: List[CompetitorContent]\n+    ) -> List[ContentGap]:\n         \"\"\"Identify content gaps and opportunities.\"\"\"\n-        \n+\n         gaps = []\n-        \n+\n         # Analyze keywords across all content\n         all_keywords = []\n         for content in competitor_content:\n             all_keywords.extend(content.target_keywords)\n-        \n+\n         keyword_freq = Counter(all_keywords)\n-        \n+\n         # Find high-frequency keywords that could be opportunities\n         for keyword, count in keyword_freq.most_common(20):\n             if count >= 2:  # Keyword appears in multiple pieces of content\n                 # Find competitor URLs using this keyword\n                 competitor_urls = [\n-                    content.url for content in competitor_content\n+                    content.url\n+                    for content in competitor_content\n                     if keyword in content.target_keywords\n                 ]\n-                \n+\n                 # Determine content type based on competitor usage\n-                content_type = self._determine_optimal_content_type(competitor_urls, keyword)\n-                \n+                content_type = self._determine_optimal_content_type(\n+                    competitor_urls, keyword\n+                )\n+\n                 # Calculate opportunity score\n-                opportunity_score = self._calculate_opportunity_score(keyword, count, competitor_urls)\n-                \n+                opportunity_score = self._calculate_opportunity_score(\n+                    keyword, count, competitor_urls\n+                )\n+\n                 # Generate suggestions\n                 suggested_title = self._generate_suggested_title(keyword, content_type)\n-                suggested_outline = self._generate_suggested_outline(keyword, content_type)\n-                \n+                suggested_outline = self._generate_suggested_outline(\n+                    keyword, content_type\n+                )\n+\n                 gap = ContentGap(\n                     keyword=keyword,\n                     search_volume=count * 100,  # Mock search volume\n                     competition=\"medium\",\n                     competitor_urls=competitor_urls,\n@@ -470,212 +555,239 @@\n                     opportunity_score=opportunity_score,\n                     suggested_title=suggested_title,\n                     suggested_outline=suggested_outline,\n                     estimated_difficulty=\"medium\",\n                     priority=\"high\" if opportunity_score > 0.7 else \"medium\",\n-                    created_at=datetime.now().isoformat()\n+                    created_at=datetime.now().isoformat(),\n                 )\n-                \n+\n                 gaps.append(gap)\n-        \n+\n         return gaps[:10]  # Return top 10 opportunities\n-    \n+\n     def _determine_optimal_content_type(self, urls: List[str], keyword: str) -> str:\n         \"\"\"Determine optimal content type for keyword.\"\"\"\n-        \n+\n         # Analyze URL patterns\n-        blog_count = sum(1 for url in urls if 'blog' in url.lower())\n-        product_count = sum(1 for url in urls if 'product' in url.lower())\n-        guide_count = sum(1 for url in urls if 'guide' in url.lower())\n-        \n+        blog_count = sum(1 for url in urls if \"blog\" in url.lower())\n+        product_count = sum(1 for url in urls if \"product\" in url.lower())\n+        guide_count = sum(1 for url in urls if \"guide\" in url.lower())\n+\n         if blog_count > product_count and blog_count > guide_count:\n-            return 'blog_post'\n+            return \"blog_post\"\n         elif product_count > blog_count and product_count > guide_count:\n-            return 'product_page'\n+            return \"product_page\"\n         elif guide_count > blog_count and guide_count > product_count:\n-            return 'guide'\n+            return \"guide\"\n         else:\n-            return 'blog_post'  # Default\n-    \n-    def _calculate_opportunity_score(self, keyword: str, frequency: int, urls: List[str]) -> float:\n+            return \"blog_post\"  # Default\n+\n+    def _calculate_opportunity_score(\n+        self, keyword: str, frequency: int, urls: List[str]\n+    ) -> float:\n         \"\"\"Calculate opportunity score for keyword.\"\"\"\n-        \n+\n         score = 0.0\n-        \n+\n         # Frequency score (0-0.4)\n         score += min(0.4, frequency * 0.1)\n-        \n+\n         # URL diversity score (0-0.3)\n         unique_domains = len(set(urlparse(url).netloc for url in urls))\n         score += min(0.3, unique_domains * 0.1)\n-        \n+\n         # Keyword length score (0-0.3)\n         if len(keyword.split()) > 1:  # Long-tail keyword\n             score += 0.3\n         else:\n             score += 0.1\n-        \n+\n         return min(1.0, score)\n-    \n+\n     def _generate_suggested_title(self, keyword: str, content_type: str) -> str:\n         \"\"\"Generate suggested title for keyword.\"\"\"\n-        \n-        if content_type == 'blog_post':\n+\n+        if content_type == \"blog_post\":\n             return f\"Complete Guide to {keyword.title()}\"\n-        elif content_type == 'product_page':\n+        elif content_type == \"product_page\":\n             return f\"Best {keyword.title()} - Product Review\"\n-        elif content_type == 'guide':\n+        elif content_type == \"guide\":\n             return f\"How to {keyword.title()} - Step by Step Guide\"\n         else:\n             return f\"Everything You Need to Know About {keyword.title()}\"\n-    \n+\n     def _generate_suggested_outline(self, keyword: str, content_type: str) -> List[str]:\n         \"\"\"Generate suggested content outline.\"\"\"\n-        \n-        if content_type == 'blog_post':\n+\n+        if content_type == \"blog_post\":\n             return [\n                 f\"Introduction to {keyword}\",\n                 f\"What is {keyword}?\",\n                 f\"Benefits of {keyword}\",\n                 f\"How to Use {keyword}\",\n                 f\"Best Practices for {keyword}\",\n-                \"Conclusion and Next Steps\"\n+                \"Conclusion and Next Steps\",\n             ]\n-        elif content_type == 'guide':\n+        elif content_type == \"guide\":\n             return [\n                 f\"Getting Started with {keyword}\",\n                 f\"Step 1: Understanding {keyword}\",\n                 f\"Step 2: Setting Up {keyword}\",\n                 f\"Step 3: Advanced {keyword} Techniques\",\n                 f\"Troubleshooting {keyword}\",\n-                \"Resources and Further Reading\"\n+                \"Resources and Further Reading\",\n             ]\n         else:\n             return [\n                 f\"Overview of {keyword}\",\n                 \"Key Features\",\n                 \"Benefits and Use Cases\",\n                 \"Implementation Guide\",\n-                \"Best Practices\"\n+                \"Best Practices\",\n             ]\n-    \n-    def _find_top_performing_content(self, competitor_content: List[CompetitorContent]) -> List[CompetitorContent]:\n+\n+    def _find_top_performing_content(\n+        self, competitor_content: List[CompetitorContent]\n+    ) -> List[CompetitorContent]:\n         \"\"\"Find top performing content based on metrics.\"\"\"\n-        \n+\n         # Sort by estimated traffic and social shares\n         sorted_content = sorted(\n             competitor_content,\n             key=lambda x: (x.estimated_traffic + x.social_shares),\n-            reverse=True\n-        )\n-        \n+            reverse=True,\n+        )\n+\n         return sorted_content[:5]  # Top 5 performing pieces\n-    \n-    def _identify_keyword_opportunities(self, competitor_content: List[CompetitorContent]) -> List[Dict[str, Any]]:\n+\n+    def _identify_keyword_opportunities(\n+        self, competitor_content: List[CompetitorContent]\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Identify keyword opportunities from competitor analysis.\"\"\"\n-        \n+\n         opportunities = []\n-        \n+\n         # Analyze keyword patterns\n         keyword_usage = defaultdict(list)\n         for content in competitor_content:\n             for keyword in content.target_keywords:\n                 keyword_usage[keyword].append(content)\n-        \n+\n         # Find opportunities\n         for keyword, content_list in keyword_usage.items():\n             if len(content_list) >= 2:  # Keyword used in multiple pieces\n                 opportunity = {\n                     \"keyword\": keyword,\n                     \"competitor_count\": len(content_list),\n-                    \"avg_word_count\": sum(c.word_count for c in content_list) / len(content_list),\n+                    \"avg_word_count\": sum(c.word_count for c in content_list)\n+                    / len(content_list),\n                     \"content_types\": list(set(c.content_type for c in content_list)),\n                     \"opportunity_score\": len(content_list) * 0.2,\n-                    \"suggested_approach\": self._suggest_keyword_approach(keyword, content_list)\n+                    \"suggested_approach\": self._suggest_keyword_approach(\n+                        keyword, content_list\n+                    ),\n                 }\n                 opportunities.append(opportunity)\n-        \n-        return sorted(opportunities, key=lambda x: x[\"opportunity_score\"], reverse=True)[:10]\n-    \n-    def _suggest_keyword_approach(self, keyword: str, content_list: List[CompetitorContent]) -> str:\n+\n+        return sorted(\n+            opportunities, key=lambda x: x[\"opportunity_score\"], reverse=True\n+        )[:10]\n+\n+    def _suggest_keyword_approach(\n+        self, keyword: str, content_list: List[CompetitorContent]\n+    ) -> str:\n         \"\"\"Suggest approach for targeting keyword.\"\"\"\n-        \n+\n         avg_word_count = sum(c.word_count for c in content_list) / len(content_list)\n         content_types = set(c.content_type for c in content_list)\n-        \n+\n         if avg_word_count > 2000:\n             return f\"Create comprehensive, long-form content about {keyword}\"\n-        elif 'blog_post' in content_types:\n+        elif \"blog_post\" in content_types:\n             return f\"Write detailed blog post about {keyword}\"\n-        elif 'guide' in content_types:\n+        elif \"guide\" in content_types:\n             return f\"Create step-by-step guide for {keyword}\"\n         else:\n             return f\"Develop content strategy around {keyword}\"\n-    \n-    def _generate_content_recommendations(self, competitor_content: List[CompetitorContent], content_gaps: List[ContentGap]) -> List[Dict[str, Any]]:\n+\n+    def _generate_content_recommendations(\n+        self,\n+        competitor_content: List[CompetitorContent],\n+        content_gaps: List[ContentGap],\n+    ) -> List[Dict[str, Any]]:\n         \"\"\"Generate content recommendations based on analysis.\"\"\"\n-        \n+\n         recommendations = []\n-        \n+\n         # Analyze content performance patterns\n         high_performing = [c for c in competitor_content if c.estimated_traffic > 500]\n-        \n+\n         if high_performing:\n             # Recommend similar content\n-            avg_word_count = sum(c.word_count for c in high_performing) / len(high_performing)\n+            avg_word_count = sum(c.word_count for c in high_performing) / len(\n+                high_performing\n+            )\n             common_keywords = self._find_common_keywords(high_performing)\n-            \n-            recommendations.append({\n-                \"type\": \"content_length\",\n-                \"title\": \"Optimize Content Length\",\n-                \"description\": f\"High-performing content averages {avg_word_count:.0f} words\",\n-                \"action\": f\"Aim for {int(avg_word_count)}-{int(avg_word_count * 1.2)} words in your content\",\n-                \"priority\": \"high\"\n-            })\n-            \n+\n+            recommendations.append(\n+                {\n+                    \"type\": \"content_length\",\n+                    \"title\": \"Optimize Content Length\",\n+                    \"description\": f\"High-performing content averages {avg_word_count:.0f} words\",\n+                    \"action\": f\"Aim for {int(avg_word_count)}-{int(avg_word_count * 1.2)} words in your content\",\n+                    \"priority\": \"high\",\n+                }\n+            )\n+\n             if common_keywords:\n-                recommendations.append({\n-                    \"type\": \"keyword_strategy\",\n-                    \"title\": \"Focus on High-Performing Keywords\",\n-                    \"description\": f\"Keywords that perform well: {', '.join(common_keywords[:5])}\",\n-                    \"action\": \"Incorporate these keywords into your content strategy\",\n-                    \"priority\": \"high\"\n-                })\n-        \n+                recommendations.append(\n+                    {\n+                        \"type\": \"keyword_strategy\",\n+                        \"title\": \"Focus on High-Performing Keywords\",\n+                        \"description\": f\"Keywords that perform well: {', '.join(common_keywords[:5])}\",\n+                        \"action\": \"Incorporate these keywords into your content strategy\",\n+                        \"priority\": \"high\",\n+                    }\n+                )\n+\n         # Recommend content gaps\n         for gap in content_gaps[:3]:\n-            recommendations.append({\n-                \"type\": \"content_gap\",\n-                \"title\": f\"Create Content About {gap.keyword.title()}\",\n-                \"description\": f\"Opportunity score: {gap.opportunity_score:.2f}\",\n-                \"action\": f\"Create {gap.content_type} with title: {gap.suggested_title}\",\n-                \"priority\": gap.priority\n-            })\n-        \n+            recommendations.append(\n+                {\n+                    \"type\": \"content_gap\",\n+                    \"title\": f\"Create Content About {gap.keyword.title()}\",\n+                    \"description\": f\"Opportunity score: {gap.opportunity_score:.2f}\",\n+                    \"action\": f\"Create {gap.content_type} with title: {gap.suggested_title}\",\n+                    \"priority\": gap.priority,\n+                }\n+            )\n+\n         return recommendations\n+\n \n async def main():\n     \"\"\"Main function to demonstrate competitor analysis.\"\"\"\n-    \n+\n     # Test competitor analysis\n     competitor_domain = \"example.com\"\n-    \n+\n     async with CompetitorContentAnalyzer() as analyzer:\n         print(f\"Analyzing competitor: {competitor_domain}\")\n-        \n+\n         analysis = await analyzer.analyze_competitor(competitor_domain, max_pages=5)\n-        \n+\n         print(\"\u2705 Competitor analysis complete!\")\n         print(f\"Pages analyzed: {analysis.total_pages_analyzed}\")\n         print(f\"Content gaps found: {len(analysis.content_gaps)}\")\n         print(f\"Keyword opportunities: {len(analysis.keyword_opportunities)}\")\n         print(f\"Content recommendations: {len(analysis.content_recommendations)}\")\n-        \n+\n         # Save analysis\n         with open(\"competitor_analysis.json\", \"w\") as f:\n             f.write(json.dumps(asdict(analysis), indent=2, default=str))\n-        \n+\n         print(\"\\nFiles created:\")\n         print(\"- competitor_analysis.json\")\n \n+\n if __name__ == \"__main__\":\n     asyncio.run(main())\n--- /home/justin/llama_rag/app/seo-api/automation/main.py\t2025-09-29 06:31:23.225240+00:00\n+++ /home/justin/llama_rag/app/seo-api/automation/main.py\t2025-09-29 06:34:09.007889+00:00\n@@ -24,389 +24,424 @@\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n class ContentAutomationOrchestrator:\n     \"\"\"Main orchestrator for content automation system.\"\"\"\n-    \n+\n     def __init__(self):\n         self.ai_generator = None\n         self.competitor_analyzer = None\n         self.calendar_manager = ContentCalendarManager()\n         self.performance_tracker = None\n         self.scheduler = ContentScheduler(self.calendar_manager)\n         self.is_running = False\n-    \n+\n     async def initialize(self):\n         \"\"\"Initialize all automation components.\"\"\"\n-        \n+\n         logger.info(\"Initializing Content Automation System...\")\n-        \n+\n         # Initialize AI content generator\n         self.ai_generator = AIContentGenerator()\n         await self.ai_generator.__aenter__()\n-        \n+\n         # Initialize competitor analyzer\n         self.competitor_analyzer = CompetitorContentAnalyzer()\n         await self.competitor_analyzer.__aenter__()\n-        \n+\n         # Initialize performance tracker\n         self.performance_tracker = SEOPerformanceTracker()\n         await self.performance_tracker.__aenter__()\n-        \n+\n         logger.info(\"Content Automation System initialized successfully\")\n-    \n+\n     async def cleanup(self):\n         \"\"\"Cleanup all automation components.\"\"\"\n-        \n+\n         logger.info(\"Cleaning up Content Automation System...\")\n-        \n+\n         if self.ai_generator:\n             await self.ai_generator.__aexit__(None, None, None)\n-        \n+\n         if self.competitor_analyzer:\n             await self.competitor_analyzer.__aexit__(None, None, None)\n-        \n+\n         if self.performance_tracker:\n             await self.performance_tracker.__aexit__(None, None, None)\n-        \n+\n         logger.info(\"Content Automation System cleaned up\")\n-    \n-    async def generate_content_from_opportunity(self, seo_opportunity: Dict[str, Any]) -> Dict[str, Any]:\n+\n+    async def generate_content_from_opportunity(\n+        self, seo_opportunity: Dict[str, Any]\n+    ) -> Dict[str, Any]:\n         \"\"\"Generate content from SEO opportunity.\"\"\"\n-        \n-        logger.info(f\"Generating content from opportunity: {seo_opportunity.get('keyword', 'Unknown')}\")\n-        \n+\n+        logger.info(\n+            f\"Generating content from opportunity: {seo_opportunity.get('keyword', 'Unknown')}\"\n+        )\n+\n         try:\n             # Extract opportunity data\n             keyword = seo_opportunity.get(\"keyword\", \"\")\n             content_type = ContentType(seo_opportunity.get(\"content_type\", \"blog_post\"))\n             target_keywords = seo_opportunity.get(\"target_keywords\", [keyword])\n-            \n+\n             # Generate content brief\n             brief = await self.ai_generator.generate_content_brief(\n                 topic=keyword,\n                 content_type=content_type,\n                 target_keywords=target_keywords,\n-                seo_opportunity=seo_opportunity\n-            )\n-            \n+                seo_opportunity=seo_opportunity,\n+            )\n+\n             # Generate full content\n             content = await self.ai_generator.generate_content(brief)\n-            \n+\n             # Add to content calendar\n             calendar_entry = self.calendar_manager.add_content_entry(\n                 title=content.title,\n                 content_type=content_type,\n                 scheduled_date=(datetime.now() + timedelta(days=7)).isoformat(),\n                 priority=ContentPriority.HIGH,\n                 assigned_to=\"ai_content_generator\",\n                 tags=[\"ai_generated\", \"seo_opportunity\"],\n                 seo_keywords=target_keywords,\n                 target_audience=brief.target_audience,\n-                estimated_read_time=brief.word_count_target // 200,  # Estimate read time\n-                word_count_target=brief.word_count_target\n-            )\n-            \n+                estimated_read_time=brief.word_count_target\n+                // 200,  # Estimate read time\n+                word_count_target=brief.word_count_target,\n+            )\n+\n             # Link brief and content to calendar entry\n             calendar_entry.brief_id = brief.id\n             calendar_entry.content_id = content.id\n-            \n+\n             logger.info(f\"Generated content: {content.title}\")\n-            \n+\n             return {\n                 \"success\": True,\n                 \"brief\": asdict(brief),\n                 \"content\": asdict(content),\n-                \"calendar_entry\": asdict(calendar_entry)\n+                \"calendar_entry\": asdict(calendar_entry),\n             }\n-            \n+\n         except Exception as e:\n             logger.error(f\"Error generating content from opportunity: {str(e)}\")\n-            return {\n-                \"success\": False,\n-                \"error\": str(e)\n-            }\n-    \n-    async def analyze_competitor_and_generate_content(self, competitor_domain: str, max_pages: int = 10) -> Dict[str, Any]:\n+            return {\"success\": False, \"error\": str(e)}\n+\n+    async def analyze_competitor_and_generate_content(\n+        self, competitor_domain: str, max_pages: int = 10\n+    ) -> Dict[str, Any]:\n         \"\"\"Analyze competitor and generate content based on gaps.\"\"\"\n-        \n+\n         logger.info(f\"Analyzing competitor: {competitor_domain}\")\n-        \n+\n         try:\n             # Analyze competitor\n             competitor_analysis = await self.competitor_analyzer.analyze_competitor(\n                 competitor_domain, max_pages\n             )\n-            \n+\n             # Generate content for top opportunities\n             generated_content = []\n             for gap in competitor_analysis.content_gaps[:3]:  # Top 3 opportunities\n-                content_result = await self.generate_content_from_opportunity({\n-                    \"keyword\": gap.keyword,\n-                    \"content_type\": gap.content_type,\n-                    \"target_keywords\": [gap.keyword],\n-                    \"opportunity_score\": gap.opportunity_score,\n-                    \"competitor_urls\": gap.competitor_urls\n-                })\n-                \n+                content_result = await self.generate_content_from_opportunity(\n+                    {\n+                        \"keyword\": gap.keyword,\n+                        \"content_type\": gap.content_type,\n+                        \"target_keywords\": [gap.keyword],\n+                        \"opportunity_score\": gap.opportunity_score,\n+                        \"competitor_urls\": gap.competitor_urls,\n+                    }\n+                )\n+\n                 if content_result[\"success\"]:\n                     generated_content.append(content_result)\n-            \n-            logger.info(f\"Generated {len(generated_content)} pieces of content from competitor analysis\")\n-            \n+\n+            logger.info(\n+                f\"Generated {len(generated_content)} pieces of content from competitor analysis\"\n+            )\n+\n             return {\n                 \"success\": True,\n                 \"competitor_analysis\": asdict(competitor_analysis),\n-                \"generated_content\": generated_content\n+                \"generated_content\": generated_content,\n             }\n-            \n+\n         except Exception as e:\n             logger.error(f\"Error analyzing competitor and generating content: {str(e)}\")\n-            return {\n-                \"success\": False,\n-                \"error\": str(e)\n-            }\n-    \n+            return {\"success\": False, \"error\": str(e)}\n+\n     async def optimize_content_performance(self, content_id: str) -> Dict[str, Any]:\n         \"\"\"Optimize content based on performance data.\"\"\"\n-        \n+\n         logger.info(f\"Optimizing content performance: {content_id}\")\n-        \n+\n         try:\n             # Get current performance\n             performance = self.performance_tracker.performance_data.get(content_id)\n             if not performance:\n                 return {\n                     \"success\": False,\n-                    \"error\": \"No performance data found for content\"\n+                    \"error\": \"No performance data found for content\",\n                 }\n-            \n+\n             # Get optimization recommendations\n             recommendations = [\n-                r for r in self.performance_tracker.recommendations\n+                r\n+                for r in self.performance_tracker.recommendations\n                 if r.content_id == content_id\n             ]\n-            \n+\n             # Apply optimizations\n             optimization_results = []\n             for recommendation in recommendations:\n                 if recommendation.priority == \"high\":\n                     # Apply high-priority optimizations\n-                    optimization_result = await self._apply_optimization(content_id, recommendation)\n+                    optimization_result = await self._apply_optimization(\n+                        content_id, recommendation\n+                    )\n                     optimization_results.append(optimization_result)\n-            \n-            logger.info(f\"Applied {len(optimization_results)} optimizations to content {content_id}\")\n-            \n+\n+            logger.info(\n+                f\"Applied {len(optimization_results)} optimizations to content {content_id}\"\n+            )\n+\n             return {\n                 \"success\": True,\n                 \"performance\": asdict(performance),\n                 \"recommendations\": [asdict(r) for r in recommendations],\n-                \"optimization_results\": optimization_results\n+                \"optimization_results\": optimization_results,\n             }\n-            \n+\n         except Exception as e:\n             logger.error(f\"Error optimizing content performance: {str(e)}\")\n-            return {\n-                \"success\": False,\n-                \"error\": str(e)\n-            }\n-    \n-    async def _apply_optimization(self, content_id: str, recommendation: Any) -> Dict[str, Any]:\n+            return {\"success\": False, \"error\": str(e)}\n+\n+    async def _apply_optimization(\n+        self, content_id: str, recommendation: Any\n+    ) -> Dict[str, Any]:\n         \"\"\"Apply specific optimization recommendation.\"\"\"\n-        \n+\n         # Mock optimization application\n         # In production, this would integrate with CMS/WordPress/etc.\n-        \n+\n         logger.info(f\"Applying optimization: {recommendation.title}\")\n-        \n+\n         # Simulate optimization delay\n         await asyncio.sleep(1)\n-        \n+\n         return {\n             \"recommendation_id\": recommendation.content_id,\n             \"applied\": True,\n             \"expected_improvement\": recommendation.expected_improvement,\n-            \"applied_at\": datetime.now().isoformat()\n+            \"applied_at\": datetime.now().isoformat(),\n         }\n-    \n+\n     async def run_automation_cycle(self) -> Dict[str, Any]:\n         \"\"\"Run complete automation cycle.\"\"\"\n-        \n+\n         logger.info(\"Starting automation cycle...\")\n-        \n+\n         cycle_results = {\n             \"started_at\": datetime.now().isoformat(),\n             \"content_generated\": 0,\n             \"content_optimized\": 0,\n             \"competitors_analyzed\": 0,\n-            \"errors\": []\n+            \"errors\": [],\n         }\n-        \n+\n         try:\n             # 1. Analyze top competitors\n             competitors = [\"competitor1.com\", \"competitor2.com\"]\n             for competitor in competitors:\n                 try:\n-                    result = await self.analyze_competitor_and_generate_content(competitor, max_pages=5)\n+                    result = await self.analyze_competitor_and_generate_content(\n+                        competitor, max_pages=5\n+                    )\n                     if result[\"success\"]:\n                         cycle_results[\"competitors_analyzed\"] += 1\n-                        cycle_results[\"content_generated\"] += len(result[\"generated_content\"])\n+                        cycle_results[\"content_generated\"] += len(\n+                            result[\"generated_content\"]\n+                        )\n                     else:\n-                        cycle_results[\"errors\"].append(f\"Competitor analysis failed for {competitor}: {result.get('error', 'Unknown error')}\")\n+                        cycle_results[\"errors\"].append(\n+                            f\"Competitor analysis failed for {competitor}: {result.get('error', 'Unknown error')}\"\n+                        )\n                 except Exception as e:\n-                    cycle_results[\"errors\"].append(f\"Error analyzing {competitor}: {str(e)}\")\n-            \n+                    cycle_results[\"errors\"].append(\n+                        f\"Error analyzing {competitor}: {str(e)}\"\n+                    )\n+\n             # 2. Optimize underperforming content\n             underperforming_content = [\n-                content_id for content_id, performance in self.performance_tracker.performance_data.items()\n+                content_id\n+                for content_id, performance in self.performance_tracker.performance_data.items()\n                 if performance.performance_score < 50\n             ]\n-            \n+\n             for content_id in underperforming_content[:3]:  # Top 3 underperformers\n                 try:\n                     result = await self.optimize_content_performance(content_id)\n                     if result[\"success\"]:\n                         cycle_results[\"content_optimized\"] += 1\n                     else:\n-                        cycle_results[\"errors\"].append(f\"Content optimization failed for {content_id}: {result.get('error', 'Unknown error')}\")\n+                        cycle_results[\"errors\"].append(\n+                            f\"Content optimization failed for {content_id}: {result.get('error', 'Unknown error')}\"\n+                        )\n                 except Exception as e:\n-                    cycle_results[\"errors\"].append(f\"Error optimizing {content_id}: {str(e)}\")\n-            \n+                    cycle_results[\"errors\"].append(\n+                        f\"Error optimizing {content_id}: {str(e)}\"\n+                    )\n+\n             # 3. Schedule upcoming content\n             upcoming_content = self.calendar_manager.get_upcoming_content(7)\n             for entry in upcoming_content:\n                 if entry.status.value == \"draft\":\n                     # Auto-schedule for publishing\n                     self.calendar_manager.schedule_content(\n-                        entry.id,\n-                        (datetime.now() + timedelta(days=1)).isoformat()\n-                    )\n-            \n+                        entry.id, (datetime.now() + timedelta(days=1)).isoformat()\n+                    )\n+\n             cycle_results[\"completed_at\"] = datetime.now().isoformat()\n             cycle_results[\"success\"] = len(cycle_results[\"errors\"]) == 0\n-            \n-            logger.info(f\"Automation cycle completed: {cycle_results['content_generated']} content generated, {cycle_results['content_optimized']} optimized\")\n-            \n+\n+            logger.info(\n+                f\"Automation cycle completed: {cycle_results['content_generated']} content generated, {cycle_results['content_optimized']} optimized\"\n+            )\n+\n         except Exception as e:\n             cycle_results[\"errors\"].append(f\"Automation cycle error: {str(e)}\")\n             cycle_results[\"success\"] = False\n             logger.error(f\"Automation cycle failed: {str(e)}\")\n-        \n+\n         return cycle_results\n-    \n+\n     async def start_automation_service(self):\n         \"\"\"Start the automation service with continuous operation.\"\"\"\n-        \n+\n         logger.info(\"Starting Content Automation Service...\")\n-        \n+\n         self.is_running = True\n-        \n+\n         # Start scheduler\n         scheduler_task = asyncio.create_task(self.scheduler.start_scheduler())\n-        \n+\n         # Run automation cycles\n         cycle_count = 0\n         while self.is_running:\n             try:\n                 cycle_count += 1\n                 logger.info(f\"Starting automation cycle #{cycle_count}\")\n-                \n+\n                 cycle_results = await self.run_automation_cycle()\n-                \n+\n                 # Log cycle results\n                 if cycle_results[\"success\"]:\n                     logger.info(f\"Cycle #{cycle_count} completed successfully\")\n                 else:\n-                    logger.warning(f\"Cycle #{cycle_count} completed with errors: {cycle_results['errors']}\")\n-                \n+                    logger.warning(\n+                        f\"Cycle #{cycle_count} completed with errors: {cycle_results['errors']}\"\n+                    )\n+\n                 # Wait before next cycle (e.g., 1 hour)\n                 await asyncio.sleep(3600)\n-                \n+\n             except Exception as e:\n                 logger.error(f\"Error in automation cycle #{cycle_count}: {str(e)}\")\n                 await asyncio.sleep(300)  # Wait 5 minutes before retry\n-        \n+\n         # Cancel scheduler task\n         scheduler_task.cancel()\n         logger.info(\"Content Automation Service stopped\")\n-    \n+\n     def stop_automation_service(self):\n         \"\"\"Stop the automation service.\"\"\"\n-        \n+\n         self.is_running = False\n         logger.info(\"Stopping Content Automation Service...\")\n-    \n+\n     def get_automation_status(self) -> Dict[str, Any]:\n         \"\"\"Get current automation system status.\"\"\"\n-        \n+\n         return {\n             \"is_running\": self.is_running,\n             \"calendar_entries\": len(self.calendar_manager.calendar.entries),\n             \"performance_tracked\": len(self.performance_tracker.performance_data),\n             \"recommendations\": len(self.performance_tracker.recommendations),\n             \"upcoming_content\": len(self.calendar_manager.get_upcoming_content(7)),\n-            \"draft_content\": len(self.calendar_manager.get_content_by_status(ContentStatus.DRAFT)),\n-            \"published_content\": len(self.calendar_manager.get_content_by_status(ContentStatus.PUBLISHED))\n+            \"draft_content\": len(\n+                self.calendar_manager.get_content_by_status(ContentStatus.DRAFT)\n+            ),\n+            \"published_content\": len(\n+                self.calendar_manager.get_content_by_status(ContentStatus.PUBLISHED)\n+            ),\n         }\n-    \n+\n     def export_automation_data(self, format: str = \"json\") -> str:\n         \"\"\"Export all automation data.\"\"\"\n-        \n+\n         data = {\n             \"calendar\": asdict(self.calendar_manager.calendar),\n             \"performance_data\": {\n-                k: asdict(v) for k, v in self.performance_tracker.performance_data.items()\n+                k: asdict(v)\n+                for k, v in self.performance_tracker.performance_data.items()\n             },\n-            \"recommendations\": [asdict(r) for r in self.performance_tracker.recommendations],\n-            \"exported_at\": datetime.now().isoformat()\n+            \"recommendations\": [\n+                asdict(r) for r in self.performance_tracker.recommendations\n+            ],\n+            \"exported_at\": datetime.now().isoformat(),\n         }\n-        \n+\n         if format == \"json\":\n             return json.dumps(data, indent=2, default=str)\n         else:\n             raise ValueError(f\"Unsupported format: {format}\")\n \n+\n async def main():\n     \"\"\"Main function to demonstrate content automation.\"\"\"\n-    \n+\n     # Initialize orchestrator\n     orchestrator = ContentAutomationOrchestrator()\n-    \n+\n     try:\n         await orchestrator.initialize()\n-        \n+\n         # Run sample automation cycle\n         print(\"Running sample automation cycle...\")\n         cycle_results = await orchestrator.run_automation_cycle()\n-        \n+\n         print(\"\u2705 Automation cycle completed!\")\n         print(f\"Content generated: {cycle_results['content_generated']}\")\n         print(f\"Content optimized: {cycle_results['content_optimized']}\")\n         print(f\"Competitors analyzed: {cycle_results['competitors_analyzed']}\")\n-        \n-        if cycle_results['errors']:\n+\n+        if cycle_results[\"errors\"]:\n             print(f\"Errors: {len(cycle_results['errors'])}\")\n-            for error in cycle_results['errors']:\n+            for error in cycle_results[\"errors\"]:\n                 print(f\"  - {error}\")\n-        \n+\n         # Get status\n         status = orchestrator.get_automation_status()\n         print(\"\\nAutomation Status:\")\n         print(f\"Calendar entries: {status['calendar_entries']}\")\n         print(f\"Performance tracked: {status['performance_tracked']}\")\n         print(f\"Recommendations: {status['recommendations']}\")\n-        \n+\n         # Export data\n         automation_data = orchestrator.export_automation_data(\"json\")\n         with open(\"content_automation_data.json\", \"w\") as f:\n             f.write(automation_data)\n-        \n+\n         print(\"\\nFiles created:\")\n         print(\"- content_automation_data.json\")\n-        \n+\n     finally:\n         await orchestrator.cleanup()\n \n+\n if __name__ == \"__main__\":\n     asyncio.run(main())\n--- /home/justin/llama_rag/app/seo-api/monitoring.py\t2025-09-29 06:31:23.221240+00:00\n+++ /home/justin/llama_rag/app/seo-api/monitoring.py\t2025-09-29 06:34:09.022196+00:00\n@@ -16,28 +16,31 @@\n logger = logging.getLogger(__name__)\n \n \n class AlertType(Enum):\n     \"\"\"Types of SEO alerts.\"\"\"\n+\n     NEW_OPPORTUNITY = \"new_opportunity\"\n     RANKING_CHANGE = \"ranking_change\"\n     COMPETITOR_UPDATE = \"competitor_update\"\n     TECHNICAL_ISSUE = \"technical_issue\"\n     PERFORMANCE_DROP = \"performance_drop\"\n \n \n class AlertSeverity(Enum):\n     \"\"\"Alert severity levels.\"\"\"\n+\n     LOW = \"low\"\n     MEDIUM = \"medium\"\n     HIGH = \"high\"\n     CRITICAL = \"critical\"\n \n \n @dataclass\n class SEOAlert:\n     \"\"\"SEO alert data structure.\"\"\"\n+\n     id: str\n     type: AlertType\n     severity: AlertSeverity\n     title: str\n     description: str\n@@ -51,10 +54,11 @@\n \n \n @dataclass\n class MonitoringConfig:\n     \"\"\"Configuration for SEO monitoring.\"\"\"\n+\n     check_interval_minutes: int = 60\n     competitor_check_interval_hours: int = 24\n     alert_cooldown_hours: int = 1\n     max_concurrent_checks: int = 3\n     enable_real_time_alerts: bool = True\n@@ -63,177 +67,186 @@\n     email_recipients: List[str] = field(default_factory=list)\n \n \n class SEOMonitoringSystem:\n     \"\"\"Real-time SEO monitoring and alerting system.\"\"\"\n-    \n-    def __init__(self, config: MonitoringConfig, storage_path: str = \"storage/seo/monitoring\"):\n+\n+    def __init__(\n+        self, config: MonitoringConfig, storage_path: str = \"storage/seo/monitoring\"\n+    ):\n         self.config = config\n         self.storage_path = Path(storage_path)\n         self.storage_path.mkdir(parents=True, exist_ok=True)\n-        \n+\n         # Monitoring state\n         self.active_alerts: List[SEOAlert] = []\n         self.monitoring_active = False\n-        \n+\n         # Alert handlers\n         self.alert_handlers: List[Callable[[SEOAlert], None]] = []\n-        \n+\n         # Load existing data\n         self._load_alerts()\n-    \n+\n     def start_monitoring(self):\n         \"\"\"Start the monitoring system.\"\"\"\n         if self.monitoring_active:\n             logger.warning(\"Monitoring already active\")\n             return\n-        \n+\n         self.monitoring_active = True\n         logger.info(\"SEO monitoring system started\")\n-    \n+\n     def stop_monitoring(self):\n         \"\"\"Stop the monitoring system.\"\"\"\n         self.monitoring_active = False\n         logger.info(\"SEO monitoring system stopped\")\n-    \n+\n     def _add_alert(self, alert: SEOAlert):\n         \"\"\"Add a new alert to the system.\"\"\"\n         # Check cooldown to avoid spam\n         if self._should_suppress_alert(alert):\n             logger.info(f\"Suppressing alert {alert.id} due to cooldown\")\n             return\n-        \n+\n         self.active_alerts.append(alert)\n         self._save_alerts()\n-        \n+\n         # Notify handlers\n         for handler in self.alert_handlers:\n             try:\n                 handler(alert)\n             except Exception as e:\n                 logger.error(f\"Error in alert handler: {e}\")\n-        \n+\n         logger.info(f\"Alert created: {alert.title}\")\n-    \n+\n     def _should_suppress_alert(self, alert: SEOAlert) -> bool:\n         \"\"\"Check if alert should be suppressed due to cooldown.\"\"\"\n-        cooldown_threshold = datetime.now() - timedelta(hours=self.config.alert_cooldown_hours)\n-        \n+        cooldown_threshold = datetime.now() - timedelta(\n+            hours=self.config.alert_cooldown_hours\n+        )\n+\n         # Check for similar recent alerts\n         for existing_alert in self.active_alerts:\n-            if (existing_alert.type == alert.type and \n-                existing_alert.domain == alert.domain and\n-                datetime.fromisoformat(existing_alert.created_at) > cooldown_threshold):\n+            if (\n+                existing_alert.type == alert.type\n+                and existing_alert.domain == alert.domain\n+                and datetime.fromisoformat(existing_alert.created_at)\n+                > cooldown_threshold\n+            ):\n                 return True\n-        \n+\n         return False\n-    \n+\n     def add_alert_handler(self, handler: Callable[[SEOAlert], None]):\n         \"\"\"Add an alert handler function.\"\"\"\n         self.alert_handlers.append(handler)\n-    \n+\n     def get_active_alerts(self, limit: int = 50) -> List[SEOAlert]:\n         \"\"\"Get active alerts.\"\"\"\n         return sorted(\n             [alert for alert in self.active_alerts if not alert.resolved],\n             key=lambda x: datetime.fromisoformat(x.created_at),\n-            reverse=True\n+            reverse=True,\n         )[:limit]\n-    \n+\n     def acknowledge_alert(self, alert_id: str):\n         \"\"\"Acknowledge an alert.\"\"\"\n         for alert in self.active_alerts:\n             if alert.id == alert_id:\n                 alert.acknowledged = True\n                 self._save_alerts()\n                 break\n-    \n+\n     def resolve_alert(self, alert_id: str):\n         \"\"\"Resolve an alert.\"\"\"\n         for alert in self.active_alerts:\n             if alert.id == alert_id:\n                 alert.resolved = True\n                 self._save_alerts()\n                 break\n-    \n+\n     def _save_alerts(self):\n         \"\"\"Save alerts to storage.\"\"\"\n         file_path = self.storage_path / \"alerts.json\"\n-        \n+\n         alerts_data = []\n         for alert in self.active_alerts:\n-            alerts_data.append({\n-                'id': alert.id,\n-                'type': alert.type.value,\n-                'severity': alert.severity.value,\n-                'title': alert.title,\n-                'description': alert.description,\n-                'domain': alert.domain,\n-                'affected_keywords': alert.affected_keywords,\n-                'metric_changes': alert.metric_changes,\n-                'created_at': alert.created_at,\n-                'acknowledged': alert.acknowledged,\n-                'resolved': alert.resolved,\n-                'metadata': alert.metadata\n-            })\n-        \n-        with open(file_path, 'w') as f:\n+            alerts_data.append(\n+                {\n+                    \"id\": alert.id,\n+                    \"type\": alert.type.value,\n+                    \"severity\": alert.severity.value,\n+                    \"title\": alert.title,\n+                    \"description\": alert.description,\n+                    \"domain\": alert.domain,\n+                    \"affected_keywords\": alert.affected_keywords,\n+                    \"metric_changes\": alert.metric_changes,\n+                    \"created_at\": alert.created_at,\n+                    \"acknowledged\": alert.acknowledged,\n+                    \"resolved\": alert.resolved,\n+                    \"metadata\": alert.metadata,\n+                }\n+            )\n+\n+        with open(file_path, \"w\") as f:\n             json.dump(alerts_data, f, indent=2)\n-    \n+\n     def _load_alerts(self):\n         \"\"\"Load alerts from storage.\"\"\"\n         file_path = self.storage_path / \"alerts.json\"\n-        \n+\n         if file_path.exists():\n             try:\n-                with open(file_path, 'r') as f:\n+                with open(file_path, \"r\") as f:\n                     alerts_data = json.load(f)\n-                \n+\n                 for data in alerts_data:\n                     alert = SEOAlert(\n-                        id=data['id'],\n-                        type=AlertType(data['type']),\n-                        severity=AlertSeverity(data['severity']),\n-                        title=data['title'],\n-                        description=data['description'],\n-                        domain=data['domain'],\n-                        affected_keywords=data['affected_keywords'],\n-                        metric_changes=data['metric_changes'],\n-                        created_at=data['created_at'],\n-                        acknowledged=data.get('acknowledged', False),\n-                        resolved=data.get('resolved', False),\n-                        metadata=data.get('metadata', {})\n+                        id=data[\"id\"],\n+                        type=AlertType(data[\"type\"]),\n+                        severity=AlertSeverity(data[\"severity\"]),\n+                        title=data[\"title\"],\n+                        description=data[\"description\"],\n+                        domain=data[\"domain\"],\n+                        affected_keywords=data[\"affected_keywords\"],\n+                        metric_changes=data[\"metric_changes\"],\n+                        created_at=data[\"created_at\"],\n+                        acknowledged=data.get(\"acknowledged\", False),\n+                        resolved=data.get(\"resolved\", False),\n+                        metadata=data.get(\"metadata\", {}),\n                     )\n                     self.active_alerts.append(alert)\n             except Exception as e:\n                 logger.error(f\"Error loading alerts: {e}\")\n \n \n # Example usage and testing\n if __name__ == \"__main__\":\n     logging.basicConfig(level=logging.INFO)\n-    \n+\n     # Create monitoring system\n     config = MonitoringConfig(\n         check_interval_minutes=5,\n         competitor_check_interval_hours=1,\n-        enable_real_time_alerts=True\n+        enable_real_time_alerts=True,\n     )\n-    \n+\n     monitoring = SEOMonitoringSystem(config)\n-    \n+\n     # Add alert handler\n     def alert_handler(alert: SEOAlert):\n         print(f\"\ud83d\udea8 ALERT: {alert.title} - {alert.description}\")\n-    \n+\n     monitoring.add_alert_handler(alert_handler)\n-    \n+\n     # Start monitoring\n     monitoring.start_monitoring()\n-    \n+\n     print(\"\u2705 Real-time monitoring system is ready for production!\")\n-    \n+\n     # Print active alerts\n     alerts = monitoring.get_active_alerts()\n     print(f\"\\nActive alerts: {len(alerts)}\")\n     for alert in alerts:\n         print(f\"- {alert.title} ({alert.severity.value})\")\n--- /home/justin/llama_rag/app/seo-api/automation/content_calendar.py\t2025-09-29 06:31:23.225240+00:00\n+++ /home/justin/llama_rag/app/seo-api/automation/content_calendar.py\t2025-09-29 06:34:09.026287+00:00\n@@ -21,36 +21,41 @@\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n class ContentStatus(Enum):\n     DRAFT = \"draft\"\n     REVIEW = \"review\"\n     APPROVED = \"approved\"\n     SCHEDULED = \"scheduled\"\n     PUBLISHED = \"published\"\n     ARCHIVED = \"archived\"\n \n+\n class ContentPriority(Enum):\n     LOW = \"low\"\n     MEDIUM = \"medium\"\n     HIGH = \"high\"\n     URGENT = \"urgent\"\n+\n \n class ContentType(Enum):\n     BLOG_POST = \"blog_post\"\n     PRODUCT_PAGE = \"product_page\"\n     LANDING_PAGE = \"landing_page\"\n     GUIDE = \"guide\"\n     FAQ = \"faq\"\n     NEWS_ARTICLE = \"news_article\"\n     SOCIAL_POST = \"social_post\"\n \n+\n @dataclass\n class ContentCalendarEntry:\n     \"\"\"Content calendar entry.\"\"\"\n+\n     id: str\n     title: str\n     content_type: ContentType\n     brief_id: Optional[str]\n     content_id: Optional[str]\n@@ -66,23 +71,27 @@\n     estimated_read_time: int\n     word_count_target: int\n     created_at: str\n     updated_at: str\n \n+\n @dataclass\n class PublishingWorkflow:\n     \"\"\"Publishing workflow configuration.\"\"\"\n+\n     id: str\n     name: str\n     steps: List[Dict[str, Any]]\n     auto_approve: bool\n     notification_emails: List[str]\n     created_at: str\n \n+\n @dataclass\n class ContentPerformance:\n     \"\"\"Content performance metrics.\"\"\"\n+\n     content_id: str\n     views: int\n     engagement_rate: float\n     bounce_rate: float\n     time_on_page: float\n@@ -91,48 +100,53 @@\n     organic_traffic: int\n     conversion_rate: float\n     seo_score: float\n     last_updated: str\n \n+\n @dataclass\n class ContentCalendar:\n     \"\"\"Main content calendar management system.\"\"\"\n+\n     entries: List[ContentCalendarEntry]\n     workflows: List[PublishingWorkflow]\n     performance_data: Dict[str, ContentPerformance]\n     team_members: List[Dict[str, Any]]\n     created_at: str\n     updated_at: str\n \n+\n class ContentCalendarManager:\n     \"\"\"Manages content calendar and publishing workflow.\"\"\"\n-    \n+\n     def __init__(self):\n         self.calendar = ContentCalendar(\n             entries=[],\n             workflows=[],\n             performance_data={},\n             team_members=[],\n             created_at=datetime.now().isoformat(),\n-            updated_at=datetime.now().isoformat()\n+            updated_at=datetime.now().isoformat(),\n         )\n         self._load_default_workflows()\n-    \n-    def add_content_entry(self,\n-                         title: str,\n-                         content_type: ContentType,\n-                         scheduled_date: str,\n-                         priority: ContentPriority = ContentPriority.MEDIUM,\n-                         assigned_to: Optional[str] = None,\n-                         tags: List[str] = None,\n-                         notes: str = \"\",\n-                         seo_keywords: List[str] = None,\n-                         target_audience: str = \"General audience\",\n-                         estimated_read_time: int = 5,\n-                         word_count_target: int = 1500) -> ContentCalendarEntry:\n+\n+    def add_content_entry(\n+        self,\n+        title: str,\n+        content_type: ContentType,\n+        scheduled_date: str,\n+        priority: ContentPriority = ContentPriority.MEDIUM,\n+        assigned_to: Optional[str] = None,\n+        tags: List[str] = None,\n+        notes: str = \"\",\n+        seo_keywords: List[str] = None,\n+        target_audience: str = \"General audience\",\n+        estimated_read_time: int = 5,\n+        word_count_target: int = 1500,\n+    ) -> ContentCalendarEntry:\n         \"\"\"Add new content entry to calendar.\"\"\"\n-        \n+\n         entry = ContentCalendarEntry(\n             id=str(uuid.uuid4()),\n             title=title,\n             content_type=content_type,\n             brief_id=None,\n@@ -147,186 +161,202 @@\n             seo_keywords=seo_keywords or [],\n             target_audience=target_audience,\n             estimated_read_time=estimated_read_time,\n             word_count_target=word_count_target,\n             created_at=datetime.now().isoformat(),\n-            updated_at=datetime.now().isoformat()\n-        )\n-        \n+            updated_at=datetime.now().isoformat(),\n+        )\n+\n         self.calendar.entries.append(entry)\n         self.calendar.updated_at = datetime.now().isoformat()\n-        \n+\n         logger.info(f\"Added content entry: {title}\")\n         return entry\n-    \n+\n     def update_content_status(self, content_id: str, status: ContentStatus) -> bool:\n         \"\"\"Update content status.\"\"\"\n-        \n+\n         for entry in self.calendar.entries:\n             if entry.id == content_id:\n                 entry.status = status\n                 entry.updated_at = datetime.now().isoformat()\n-                \n+\n                 if status == ContentStatus.PUBLISHED:\n                     entry.publish_date = datetime.now().isoformat()\n-                \n+\n                 self.calendar.updated_at = datetime.now().isoformat()\n                 logger.info(f\"Updated content {content_id} status to {status.value}\")\n                 return True\n-        \n+\n         return False\n-    \n+\n     def get_upcoming_content(self, days: int = 30) -> List[ContentCalendarEntry]:\n         \"\"\"Get upcoming content for the next N days.\"\"\"\n-        \n+\n         cutoff_date = datetime.now() + timedelta(days=days)\n-        \n+\n         upcoming = []\n         for entry in self.calendar.entries:\n             entry_date = datetime.fromisoformat(entry.scheduled_date)\n             if entry_date <= cutoff_date and entry.status != ContentStatus.PUBLISHED:\n                 upcoming.append(entry)\n-        \n+\n         return sorted(upcoming, key=lambda x: x.scheduled_date)\n-    \n-    def get_content_by_status(self, status: ContentStatus) -> List[ContentCalendarEntry]:\n+\n+    def get_content_by_status(\n+        self, status: ContentStatus\n+    ) -> List[ContentCalendarEntry]:\n         \"\"\"Get content by status.\"\"\"\n-        \n+\n         return [entry for entry in self.calendar.entries if entry.status == status]\n-    \n+\n     def get_content_by_assignee(self, assignee: str) -> List[ContentCalendarEntry]:\n         \"\"\"Get content assigned to specific person.\"\"\"\n-        \n-        return [entry for entry in self.calendar.entries if entry.assigned_to == assignee]\n-    \n-    def get_content_by_priority(self, priority: ContentPriority) -> List[ContentCalendarEntry]:\n+\n+        return [\n+            entry for entry in self.calendar.entries if entry.assigned_to == assignee\n+        ]\n+\n+    def get_content_by_priority(\n+        self, priority: ContentPriority\n+    ) -> List[ContentCalendarEntry]:\n         \"\"\"Get content by priority level.\"\"\"\n-        \n+\n         return [entry for entry in self.calendar.entries if entry.priority == priority]\n-    \n+\n     def schedule_content(self, content_id: str, scheduled_date: str) -> bool:\n         \"\"\"Schedule content for specific date.\"\"\"\n-        \n+\n         for entry in self.calendar.entries:\n             if entry.id == content_id:\n                 entry.scheduled_date = scheduled_date\n                 entry.status = ContentStatus.SCHEDULED\n                 entry.updated_at = datetime.now().isoformat()\n                 self.calendar.updated_at = datetime.now().isoformat()\n                 logger.info(f\"Scheduled content {content_id} for {scheduled_date}\")\n                 return True\n-        \n+\n         return False\n-    \n-    def get_content_calendar_view(self, start_date: str, end_date: str) -> Dict[str, List[ContentCalendarEntry]]:\n+\n+    def get_content_calendar_view(\n+        self, start_date: str, end_date: str\n+    ) -> Dict[str, List[ContentCalendarEntry]]:\n         \"\"\"Get calendar view for date range.\"\"\"\n-        \n+\n         start = datetime.fromisoformat(start_date)\n         end = datetime.fromisoformat(end_date)\n-        \n+\n         calendar_view = {}\n         current_date = start\n-        \n+\n         while current_date <= end:\n             date_str = current_date.strftime(\"%Y-%m-%d\")\n             calendar_view[date_str] = []\n-            \n+\n             for entry in self.calendar.entries:\n                 entry_date = datetime.fromisoformat(entry.scheduled_date)\n                 if entry_date.date() == current_date.date():\n                     calendar_view[date_str].append(entry)\n-            \n+\n             current_date += timedelta(days=1)\n-        \n+\n         return calendar_view\n-    \n+\n     def get_content_analytics(self) -> Dict[str, Any]:\n         \"\"\"Get content calendar analytics.\"\"\"\n-        \n+\n         total_entries = len(self.calendar.entries)\n         published_entries = len(self.get_content_by_status(ContentStatus.PUBLISHED))\n         draft_entries = len(self.get_content_by_status(ContentStatus.DRAFT))\n         scheduled_entries = len(self.get_content_by_status(ContentStatus.SCHEDULED))\n-        \n+\n         # Content type distribution\n         content_types = {}\n         for entry in self.calendar.entries:\n             content_type = entry.content_type.value\n             content_types[content_type] = content_types.get(content_type, 0) + 1\n-        \n+\n         # Priority distribution\n         priorities = {}\n         for entry in self.calendar.entries:\n             priority = entry.priority.value\n             priorities[priority] = priorities.get(priority, 0) + 1\n-        \n+\n         # Upcoming content\n         upcoming = self.get_upcoming_content(7)  # Next 7 days\n-        \n+\n         return {\n             \"total_entries\": total_entries,\n             \"published_entries\": published_entries,\n             \"draft_entries\": draft_entries,\n             \"scheduled_entries\": scheduled_entries,\n             \"content_types\": content_types,\n             \"priorities\": priorities,\n             \"upcoming_this_week\": len(upcoming),\n-            \"completion_rate\": (published_entries / total_entries * 100) if total_entries > 0 else 0\n+            \"completion_rate\": (\n+                (published_entries / total_entries * 100) if total_entries > 0 else 0\n+            ),\n         }\n-    \n-    def create_publishing_workflow(self,\n-                                 name: str,\n-                                 steps: List[Dict[str, Any]],\n-                                 auto_approve: bool = False,\n-                                 notification_emails: List[str] = None) -> PublishingWorkflow:\n+\n+    def create_publishing_workflow(\n+        self,\n+        name: str,\n+        steps: List[Dict[str, Any]],\n+        auto_approve: bool = False,\n+        notification_emails: List[str] = None,\n+    ) -> PublishingWorkflow:\n         \"\"\"Create new publishing workflow.\"\"\"\n-        \n+\n         workflow = PublishingWorkflow(\n             id=str(uuid.uuid4()),\n             name=name,\n             steps=steps,\n             auto_approve=auto_approve,\n             notification_emails=notification_emails or [],\n-            created_at=datetime.now().isoformat()\n-        )\n-        \n+            created_at=datetime.now().isoformat(),\n+        )\n+\n         self.calendar.workflows.append(workflow)\n         self.calendar.updated_at = datetime.now().isoformat()\n-        \n+\n         logger.info(f\"Created publishing workflow: {name}\")\n         return workflow\n-    \n+\n     def apply_workflow(self, content_id: str, workflow_id: str) -> bool:\n         \"\"\"Apply workflow to content.\"\"\"\n-        \n-        workflow = next((w for w in self.calendar.workflows if w.id == workflow_id), None)\n+\n+        workflow = next(\n+            (w for w in self.calendar.workflows if w.id == workflow_id), None\n+        )\n         if not workflow:\n             return False\n-        \n+\n         entry = next((e for e in self.calendar.entries if e.id == content_id), None)\n         if not entry:\n             return False\n-        \n+\n         # Apply workflow steps\n         for step in workflow.steps:\n             if step[\"type\"] == \"status_change\":\n                 entry.status = ContentStatus(step[\"status\"])\n             elif step[\"type\"] == \"assign\":\n                 entry.assigned_to = step[\"assignee\"]\n             elif step[\"type\"] == \"schedule\":\n                 entry.scheduled_date = step[\"date\"]\n                 entry.status = ContentStatus.SCHEDULED\n-        \n+\n         entry.updated_at = datetime.now().isoformat()\n         self.calendar.updated_at = datetime.now().isoformat()\n-        \n+\n         logger.info(f\"Applied workflow {workflow_id} to content {content_id}\")\n         return True\n-    \n-    def track_content_performance(self, content_id: str, performance_data: Dict[str, Any]) -> bool:\n+\n+    def track_content_performance(\n+        self, content_id: str, performance_data: Dict[str, Any]\n+    ) -> bool:\n         \"\"\"Track content performance metrics.\"\"\"\n-        \n+\n         performance = ContentPerformance(\n             content_id=content_id,\n             views=performance_data.get(\"views\", 0),\n             engagement_rate=performance_data.get(\"engagement_rate\", 0.0),\n             bounce_rate=performance_data.get(\"bounce_rate\", 0.0),\n@@ -334,181 +364,193 @@\n             social_shares=performance_data.get(\"social_shares\", 0),\n             backlinks=performance_data.get(\"backlinks\", 0),\n             organic_traffic=performance_data.get(\"organic_traffic\", 0),\n             conversion_rate=performance_data.get(\"conversion_rate\", 0.0),\n             seo_score=performance_data.get(\"seo_score\", 0.0),\n-            last_updated=datetime.now().isoformat()\n-        )\n-        \n+            last_updated=datetime.now().isoformat(),\n+        )\n+\n         self.calendar.performance_data[content_id] = performance\n         self.calendar.updated_at = datetime.now().isoformat()\n-        \n+\n         logger.info(f\"Updated performance data for content {content_id}\")\n         return True\n-    \n+\n     def get_performance_insights(self) -> Dict[str, Any]:\n         \"\"\"Get content performance insights.\"\"\"\n-        \n+\n         if not self.calendar.performance_data:\n             return {\"message\": \"No performance data available\"}\n-        \n+\n         performances = list(self.calendar.performance_data.values())\n-        \n+\n         # Calculate averages\n         avg_views = sum(p.views for p in performances) / len(performances)\n-        avg_engagement = sum(p.engagement_rate for p in performances) / len(performances)\n+        avg_engagement = sum(p.engagement_rate for p in performances) / len(\n+            performances\n+        )\n         avg_seo_score = sum(p.seo_score for p in performances) / len(performances)\n-        \n+\n         # Find top performers\n         top_performers = sorted(performances, key=lambda x: x.views, reverse=True)[:5]\n-        \n+\n         # Find underperformers\n         underperformers = sorted(performances, key=lambda x: x.views)[:5]\n-        \n+\n         return {\n             \"total_content_tracked\": len(performances),\n             \"average_views\": avg_views,\n             \"average_engagement_rate\": avg_engagement,\n             \"average_seo_score\": avg_seo_score,\n             \"top_performers\": [\n                 {\n                     \"content_id\": p.content_id,\n                     \"views\": p.views,\n-                    \"engagement_rate\": p.engagement_rate\n+                    \"engagement_rate\": p.engagement_rate,\n                 }\n                 for p in top_performers\n             ],\n             \"underperformers\": [\n                 {\n                     \"content_id\": p.content_id,\n                     \"views\": p.views,\n-                    \"engagement_rate\": p.engagement_rate\n+                    \"engagement_rate\": p.engagement_rate,\n                 }\n                 for p in underperformers\n-            ]\n+            ],\n         }\n-    \n+\n     def export_calendar(self, format: str = \"json\") -> str:\n         \"\"\"Export calendar data.\"\"\"\n-        \n+\n         if format == \"json\":\n             return json.dumps(asdict(self.calendar), indent=2, default=str)\n         elif format == \"csv\":\n             return self._export_csv()\n         else:\n             raise ValueError(f\"Unsupported format: {format}\")\n-    \n+\n     def _export_csv(self) -> str:\n         \"\"\"Export calendar as CSV.\"\"\"\n-        \n-        csv_lines = [\"id,title,content_type,scheduled_date,status,priority,assigned_to,tags,seo_keywords\"]\n-        \n+\n+        csv_lines = [\n+            \"id,title,content_type,scheduled_date,status,priority,assigned_to,tags,seo_keywords\"\n+        ]\n+\n         for entry in self.calendar.entries:\n-            csv_lines.append(f\"{entry.id},{entry.title},{entry.content_type.value},{entry.scheduled_date},{entry.status.value},{entry.priority.value},{entry.assigned_to or ''},{','.join(entry.tags)},{','.join(entry.seo_keywords)}\")\n-        \n+            csv_lines.append(\n+                f\"{entry.id},{entry.title},{entry.content_type.value},{entry.scheduled_date},{entry.status.value},{entry.priority.value},{entry.assigned_to or ''},{','.join(entry.tags)},{','.join(entry.seo_keywords)}\"\n+            )\n+\n         return \"\\n\".join(csv_lines)\n-    \n+\n     def _load_default_workflows(self):\n         \"\"\"Load default publishing workflows.\"\"\"\n-        \n+\n         # Standard blog post workflow\n         blog_workflow = PublishingWorkflow(\n             id=\"blog_standard\",\n             name=\"Standard Blog Post Workflow\",\n             steps=[\n                 {\"type\": \"status_change\", \"status\": \"draft\"},\n                 {\"type\": \"assign\", \"assignee\": \"content_writer\"},\n                 {\"type\": \"status_change\", \"status\": \"review\"},\n                 {\"type\": \"assign\", \"assignee\": \"editor\"},\n                 {\"type\": \"status_change\", \"status\": \"approved\"},\n-                {\"type\": \"schedule\", \"date\": \"auto\"}\n+                {\"type\": \"schedule\", \"date\": \"auto\"},\n             ],\n             auto_approve=False,\n             notification_emails=[\"content-team@company.com\"],\n-            created_at=datetime.now().isoformat()\n-        )\n-        \n+            created_at=datetime.now().isoformat(),\n+        )\n+\n         # Quick publish workflow\n         quick_workflow = PublishingWorkflow(\n             id=\"quick_publish\",\n             name=\"Quick Publish Workflow\",\n             steps=[\n                 {\"type\": \"status_change\", \"status\": \"draft\"},\n                 {\"type\": \"status_change\", \"status\": \"approved\"},\n-                {\"type\": \"schedule\", \"date\": \"immediate\"}\n+                {\"type\": \"schedule\", \"date\": \"immediate\"},\n             ],\n             auto_approve=True,\n             notification_emails=[],\n-            created_at=datetime.now().isoformat()\n-        )\n-        \n+            created_at=datetime.now().isoformat(),\n+        )\n+\n         self.calendar.workflows.extend([blog_workflow, quick_workflow])\n+\n \n class ContentScheduler:\n     \"\"\"Handles automated content scheduling and publishing.\"\"\"\n-    \n+\n     def __init__(self, calendar_manager: ContentCalendarManager):\n         self.calendar_manager = calendar_manager\n         self.is_running = False\n-    \n+\n     async def start_scheduler(self):\n         \"\"\"Start the content scheduler.\"\"\"\n-        \n+\n         self.is_running = True\n         logger.info(\"Content scheduler started\")\n-        \n+\n         while self.is_running:\n             try:\n                 await self._check_scheduled_content()\n                 await asyncio.sleep(60)  # Check every minute\n             except Exception as e:\n                 logger.error(f\"Scheduler error: {str(e)}\")\n                 await asyncio.sleep(60)\n-    \n+\n     def stop_scheduler(self):\n         \"\"\"Stop the content scheduler.\"\"\"\n-        \n+\n         self.is_running = False\n         logger.info(\"Content scheduler stopped\")\n-    \n+\n     async def _check_scheduled_content(self):\n         \"\"\"Check for content that should be published.\"\"\"\n-        \n+\n         now = datetime.now()\n-        scheduled_content = self.calendar_manager.get_content_by_status(ContentStatus.SCHEDULED)\n-        \n+        scheduled_content = self.calendar_manager.get_content_by_status(\n+            ContentStatus.SCHEDULED\n+        )\n+\n         for entry in scheduled_content:\n             scheduled_time = datetime.fromisoformat(entry.scheduled_date)\n-            \n+\n             if now >= scheduled_time:\n                 # Publish content\n-                success = self.calendar_manager.update_content_status(entry.id, ContentStatus.PUBLISHED)\n-                \n+                success = self.calendar_manager.update_content_status(\n+                    entry.id, ContentStatus.PUBLISHED\n+                )\n+\n                 if success:\n                     logger.info(f\"Published content: {entry.title}\")\n                     # Here you would integrate with actual publishing system\n                     await self._publish_content(entry)\n                 else:\n                     logger.error(f\"Failed to publish content: {entry.title}\")\n-    \n+\n     async def _publish_content(self, entry: ContentCalendarEntry):\n         \"\"\"Publish content to target platform.\"\"\"\n-        \n+\n         # Mock publishing - in production, integrate with CMS/WordPress/etc.\n         logger.info(f\"Publishing {entry.title} to platform...\")\n-        \n+\n         # Simulate publishing delay\n         await asyncio.sleep(1)\n-        \n+\n         logger.info(f\"Successfully published: {entry.title}\")\n+\n \n def main():\n     \"\"\"Main function to demonstrate content calendar.\"\"\"\n-    \n+\n     # Initialize calendar manager\n     calendar_manager = ContentCalendarManager()\n-    \n+\n     # Add sample content entries\n     calendar_manager.add_content_entry(\n         title=\"Complete Guide to SEO Best Practices\",\n         content_type=ContentType.BLOG_POST,\n         scheduled_date=(datetime.now() + timedelta(days=1)).isoformat(),\n@@ -516,46 +558,47 @@\n         assigned_to=\"content_writer\",\n         tags=[\"seo\", \"marketing\", \"guide\"],\n         seo_keywords=[\"seo best practices\", \"search engine optimization\"],\n         target_audience=\"marketing professionals\",\n         estimated_read_time=8,\n-        word_count_target=2500\n+        word_count_target=2500,\n     )\n-    \n+\n     calendar_manager.add_content_entry(\n         title=\"Product Launch: New E-commerce Features\",\n         content_type=ContentType.NEWS_ARTICLE,\n         scheduled_date=(datetime.now() + timedelta(days=3)).isoformat(),\n         priority=ContentPriority.URGENT,\n         assigned_to=\"product_manager\",\n         tags=[\"product\", \"launch\", \"ecommerce\"],\n         seo_keywords=[\"ecommerce features\", \"product launch\"],\n         target_audience=\"customers and prospects\",\n         estimated_read_time=5,\n-        word_count_target=1200\n+        word_count_target=1200,\n     )\n-    \n+\n     # Get calendar analytics\n     analytics = calendar_manager.get_content_analytics()\n     print(\"Content Calendar Analytics:\")\n     print(f\"Total entries: {analytics['total_entries']}\")\n     print(f\"Published: {analytics['published_entries']}\")\n     print(f\"Draft: {analytics['draft_entries']}\")\n     print(f\"Scheduled: {analytics['scheduled_entries']}\")\n     print(f\"Completion rate: {analytics['completion_rate']:.1f}%\")\n-    \n+\n     # Get upcoming content\n     upcoming = calendar_manager.get_upcoming_content(7)\n     print(f\"\\nUpcoming content (next 7 days): {len(upcoming)}\")\n     for entry in upcoming:\n         print(f\"- {entry.title} ({entry.scheduled_date})\")\n-    \n+\n     # Export calendar\n     calendar_json = calendar_manager.export_calendar(\"json\")\n     with open(\"content_calendar.json\", \"w\") as f:\n         f.write(calendar_json)\n-    \n+\n     print(\"\\nFiles created:\")\n     print(\"- content_calendar.json\")\n \n+\n if __name__ == \"__main__\":\n     main()\n--- /home/justin/llama_rag/app/seo-api/automation/seo_performance_tracker.py\t2025-09-29 06:31:23.221240+00:00\n+++ /home/justin/llama_rag/app/seo-api/automation/seo_performance_tracker.py\t2025-09-29 06:34:09.218688+00:00\n@@ -23,35 +23,40 @@\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n class MetricType(Enum):\n     ORGANIC_TRAFFIC = \"organic_traffic\"\n     KEYWORD_RANKINGS = \"keyword_rankings\"\n     CLICK_THROUGH_RATE = \"click_through_rate\"\n     BOUNCE_RATE = \"bounce_rate\"\n     TIME_ON_PAGE = \"time_on_page\"\n     CONVERSION_RATE = \"conversion_rate\"\n     BACKLINKS = \"backlinks\"\n     DOMAIN_AUTHORITY = \"domain_authority\"\n \n+\n @dataclass\n class SEOMetric:\n     \"\"\"SEO performance metric.\"\"\"\n+\n     content_id: str\n     metric_type: MetricType\n     value: float\n     previous_value: Optional[float]\n     change_percentage: float\n     trend: str  # \"up\", \"down\", \"stable\"\n     date: str\n     source: str  # \"google_analytics\", \"search_console\", \"manual\"\n \n+\n @dataclass\n class ContentPerformance:\n     \"\"\"Content performance summary.\"\"\"\n+\n     content_id: str\n     title: str\n     url: str\n     publish_date: str\n     total_views: int\n@@ -66,13 +71,15 @@\n     social_shares: int\n     seo_score: float\n     performance_score: float\n     last_updated: str\n \n+\n @dataclass\n class OptimizationRecommendation:\n     \"\"\"SEO optimization recommendation.\"\"\"\n+\n     content_id: str\n     recommendation_type: str\n     title: str\n     description: str\n     current_value: Any\n@@ -81,49 +88,52 @@\n     priority: str\n     effort_required: str\n     implementation_steps: List[str]\n     created_at: str\n \n+\n @dataclass\n class PerformanceReport:\n     \"\"\"SEO performance report.\"\"\"\n+\n     period_start: str\n     period_end: str\n     total_content: int\n     avg_performance_score: float\n     top_performers: List[ContentPerformance]\n     underperformers: List[ContentPerformance]\n     recommendations: List[OptimizationRecommendation]\n     key_insights: List[str]\n     generated_at: str\n \n+\n class SEOPerformanceTracker:\n     \"\"\"Tracks and analyzes SEO performance.\"\"\"\n-    \n+\n     def __init__(self):\n         self.performance_data: Dict[str, ContentPerformance] = {}\n         self.metrics_history: List[SEOMetric] = []\n         self.recommendations: List[OptimizationRecommendation] = []\n         self.session: Optional[aiohttp.ClientSession] = None\n-    \n+\n     async def __aenter__(self):\n-        self.session = aiohttp.ClientSession(\n-            timeout=aiohttp.ClientTimeout(total=30)\n-        )\n+        self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30))\n         return self\n-    \n+\n     async def __aexit__(self, exc_type, exc_val, exc_tb):\n         if self.session:\n             await self.session.close()\n-    \n-    async def track_content_performance(self, content_id: str, metrics: Dict[str, Any]) -> bool:\n+\n+    async def track_content_performance(\n+        self, content_id: str, metrics: Dict[str, Any]\n+    ) -> bool:\n         \"\"\"Track performance metrics for content.\"\"\"\n-        \n+\n         try:\n             # Calculate performance score\n             performance_score = self._calculate_performance_score(metrics)\n-            \n+\n             # Create or update performance record\n             performance = ContentPerformance(\n                 content_id=content_id,\n                 title=metrics.get(\"title\", \"\"),\n                 url=metrics.get(\"url\", \"\"),\n@@ -138,482 +148,529 @@\n                 conversion_rate=metrics.get(\"conversion_rate\", 0.0),\n                 backlinks=metrics.get(\"backlinks\", 0),\n                 social_shares=metrics.get(\"social_shares\", 0),\n                 seo_score=metrics.get(\"seo_score\", 0.0),\n                 performance_score=performance_score,\n-                last_updated=datetime.now().isoformat()\n-            )\n-            \n+                last_updated=datetime.now().isoformat(),\n+            )\n+\n             # Store performance data\n             self.performance_data[content_id] = performance\n-            \n+\n             # Track metrics history\n             await self._track_metrics_history(content_id, metrics)\n-            \n+\n             # Generate recommendations\n             await self._generate_recommendations(content_id, performance)\n-            \n+\n             logger.info(f\"Tracked performance for content {content_id}\")\n             return True\n-            \n+\n         except Exception as e:\n             logger.error(f\"Error tracking performance for {content_id}: {str(e)}\")\n             return False\n-    \n+\n     async def _track_metrics_history(self, content_id: str, metrics: Dict[str, Any]):\n         \"\"\"Track metrics history for trend analysis.\"\"\"\n-        \n+\n         # Get previous metrics if available\n         previous_metrics = self._get_previous_metrics(content_id)\n-        \n+\n         # Track each metric\n         metric_types = [\n             (MetricType.ORGANIC_TRAFFIC, \"organic_views\"),\n             (MetricType.CLICK_THROUGH_RATE, \"click_through_rate\"),\n             (MetricType.BOUNCE_RATE, \"bounce_rate\"),\n             (MetricType.TIME_ON_PAGE, \"time_on_page\"),\n             (MetricType.CONVERSION_RATE, \"conversion_rate\"),\n-            (MetricType.BACKLINKS, \"backlinks\")\n+            (MetricType.BACKLINKS, \"backlinks\"),\n         ]\n-        \n+\n         for metric_type, metric_key in metric_types:\n             if metric_key in metrics:\n                 current_value = metrics[metric_key]\n-                previous_value = previous_metrics.get(metric_key) if previous_metrics else None\n-                \n+                previous_value = (\n+                    previous_metrics.get(metric_key) if previous_metrics else None\n+                )\n+\n                 # Calculate change percentage\n                 change_percentage = 0.0\n                 if previous_value is not None and previous_value != 0:\n-                    change_percentage = ((current_value - previous_value) / previous_value) * 100\n-                \n+                    change_percentage = (\n+                        (current_value - previous_value) / previous_value\n+                    ) * 100\n+\n                 # Determine trend\n                 trend = \"stable\"\n                 if change_percentage > 5:\n                     trend = \"up\"\n                 elif change_percentage < -5:\n                     trend = \"down\"\n-                \n+\n                 # Create metric record\n                 metric = SEOMetric(\n                     content_id=content_id,\n                     metric_type=metric_type,\n                     value=current_value,\n                     previous_value=previous_value,\n                     change_percentage=change_percentage,\n                     trend=trend,\n                     date=datetime.now().isoformat(),\n-                    source=\"google_analytics\"\n+                    source=\"google_analytics\",\n                 )\n-                \n+\n                 self.metrics_history.append(metric)\n-    \n+\n     def _get_previous_metrics(self, content_id: str) -> Optional[Dict[str, Any]]:\n         \"\"\"Get previous metrics for content.\"\"\"\n-        \n+\n         # Find most recent metrics for this content\n-        recent_metrics = [\n-            m for m in self.metrics_history\n-            if m.content_id == content_id\n-        ]\n-        \n+        recent_metrics = [m for m in self.metrics_history if m.content_id == content_id]\n+\n         if not recent_metrics:\n             return None\n-        \n+\n         # Group by metric type and get latest\n         latest_metrics = {}\n         for metric in recent_metrics:\n             if metric.metric_type not in latest_metrics:\n                 latest_metrics[metric.metric_type] = metric.value\n-        \n+\n         # Convert to dict with metric keys\n         metric_keys = {\n             MetricType.ORGANIC_TRAFFIC: \"organic_views\",\n             MetricType.CLICK_THROUGH_RATE: \"click_through_rate\",\n             MetricType.BOUNCE_RATE: \"bounce_rate\",\n             MetricType.TIME_ON_PAGE: \"time_on_page\",\n             MetricType.CONVERSION_RATE: \"conversion_rate\",\n-            MetricType.BACKLINKS: \"backlinks\"\n+            MetricType.BACKLINKS: \"backlinks\",\n         }\n-        \n+\n         return {\n             metric_keys[metric_type]: value\n             for metric_type, value in latest_metrics.items()\n             if metric_type in metric_keys\n         }\n-    \n+\n     def _calculate_performance_score(self, metrics: Dict[str, Any]) -> float:\n         \"\"\"Calculate overall performance score.\"\"\"\n-        \n+\n         score = 0.0\n         max_score = 100.0\n-        \n+\n         # Organic traffic weight (30%)\n         organic_views = metrics.get(\"organic_views\", 0)\n         if organic_views > 1000:\n             score += 30\n         elif organic_views > 500:\n             score += 20\n         elif organic_views > 100:\n             score += 10\n-        \n+\n         # Click-through rate weight (20%)\n         ctr = metrics.get(\"click_through_rate\", 0)\n         if ctr > 5:\n             score += 20\n         elif ctr > 3:\n             score += 15\n         elif ctr > 1:\n             score += 10\n-        \n+\n         # Bounce rate weight (15%)\n         bounce_rate = metrics.get(\"bounce_rate\", 100)\n         if bounce_rate < 30:\n             score += 15\n         elif bounce_rate < 50:\n             score += 10\n         elif bounce_rate < 70:\n             score += 5\n-        \n+\n         # Time on page weight (15%)\n         time_on_page = metrics.get(\"time_on_page\", 0)\n         if time_on_page > 300:  # 5 minutes\n             score += 15\n         elif time_on_page > 180:  # 3 minutes\n             score += 10\n         elif time_on_page > 60:  # 1 minute\n             score += 5\n-        \n+\n         # Conversion rate weight (10%)\n         conversion_rate = metrics.get(\"conversion_rate\", 0)\n         if conversion_rate > 5:\n             score += 10\n         elif conversion_rate > 2:\n             score += 7\n         elif conversion_rate > 1:\n             score += 5\n-        \n+\n         # Backlinks weight (10%)\n         backlinks = metrics.get(\"backlinks\", 0)\n         if backlinks > 50:\n             score += 10\n         elif backlinks > 20:\n             score += 7\n         elif backlinks > 5:\n             score += 5\n-        \n+\n         return min(max_score, score)\n-    \n-    async def _generate_recommendations(self, content_id: str, performance: ContentPerformance):\n+\n+    async def _generate_recommendations(\n+        self, content_id: str, performance: ContentPerformance\n+    ):\n         \"\"\"Generate optimization recommendations for content.\"\"\"\n-        \n+\n         recommendations = []\n-        \n+\n         # Check organic traffic\n         if performance.organic_views < 100:\n-            recommendations.append(OptimizationRecommendation(\n-                content_id=content_id,\n-                recommendation_type=\"traffic\",\n-                title=\"Improve Organic Traffic\",\n-                description=\"Content has low organic traffic. Focus on keyword optimization and promotion.\",\n-                current_value=performance.organic_views,\n-                recommended_value=500,\n-                expected_improvement=25.0,\n-                priority=\"high\",\n-                effort_required=\"medium\",\n-                implementation_steps=[\n-                    \"Optimize target keywords\",\n-                    \"Improve internal linking\",\n-                    \"Promote on social media\",\n-                    \"Build backlinks\"\n-                ],\n-                created_at=datetime.now().isoformat()\n-            ))\n-        \n+            recommendations.append(\n+                OptimizationRecommendation(\n+                    content_id=content_id,\n+                    recommendation_type=\"traffic\",\n+                    title=\"Improve Organic Traffic\",\n+                    description=\"Content has low organic traffic. Focus on keyword optimization and promotion.\",\n+                    current_value=performance.organic_views,\n+                    recommended_value=500,\n+                    expected_improvement=25.0,\n+                    priority=\"high\",\n+                    effort_required=\"medium\",\n+                    implementation_steps=[\n+                        \"Optimize target keywords\",\n+                        \"Improve internal linking\",\n+                        \"Promote on social media\",\n+                        \"Build backlinks\",\n+                    ],\n+                    created_at=datetime.now().isoformat(),\n+                )\n+            )\n+\n         # Check click-through rate\n         if performance.click_through_rate < 2:\n-            recommendations.append(OptimizationRecommendation(\n-                content_id=content_id,\n-                recommendation_type=\"ctr\",\n-                title=\"Improve Click-Through Rate\",\n-                description=\"Low CTR indicates unappealing title or meta description.\",\n-                current_value=performance.click_through_rate,\n-                recommended_value=3.5,\n-                expected_improvement=15.0,\n-                priority=\"medium\",\n-                effort_required=\"low\",\n-                implementation_steps=[\n-                    \"A/B test different titles\",\n-                    \"Optimize meta description\",\n-                    \"Add compelling call-to-action\",\n-                    \"Improve featured snippets\"\n-                ],\n-                created_at=datetime.now().isoformat()\n-            ))\n-        \n+            recommendations.append(\n+                OptimizationRecommendation(\n+                    content_id=content_id,\n+                    recommendation_type=\"ctr\",\n+                    title=\"Improve Click-Through Rate\",\n+                    description=\"Low CTR indicates unappealing title or meta description.\",\n+                    current_value=performance.click_through_rate,\n+                    recommended_value=3.5,\n+                    expected_improvement=15.0,\n+                    priority=\"medium\",\n+                    effort_required=\"low\",\n+                    implementation_steps=[\n+                        \"A/B test different titles\",\n+                        \"Optimize meta description\",\n+                        \"Add compelling call-to-action\",\n+                        \"Improve featured snippets\",\n+                    ],\n+                    created_at=datetime.now().isoformat(),\n+                )\n+            )\n+\n         # Check bounce rate\n         if performance.bounce_rate > 70:\n-            recommendations.append(OptimizationRecommendation(\n-                content_id=content_id,\n-                recommendation_type=\"bounce_rate\",\n-                title=\"Reduce Bounce Rate\",\n-                description=\"High bounce rate indicates poor content quality or user experience.\",\n-                current_value=performance.bounce_rate,\n-                recommended_value=50,\n-                expected_improvement=20.0,\n-                priority=\"high\",\n-                effort_required=\"high\",\n-                implementation_steps=[\n-                    \"Improve content quality\",\n-                    \"Add internal links\",\n-                    \"Optimize page speed\",\n-                    \"Improve mobile experience\"\n-                ],\n-                created_at=datetime.now().isoformat()\n-            ))\n-        \n+            recommendations.append(\n+                OptimizationRecommendation(\n+                    content_id=content_id,\n+                    recommendation_type=\"bounce_rate\",\n+                    title=\"Reduce Bounce Rate\",\n+                    description=\"High bounce rate indicates poor content quality or user experience.\",\n+                    current_value=performance.bounce_rate,\n+                    recommended_value=50,\n+                    expected_improvement=20.0,\n+                    priority=\"high\",\n+                    effort_required=\"high\",\n+                    implementation_steps=[\n+                        \"Improve content quality\",\n+                        \"Add internal links\",\n+                        \"Optimize page speed\",\n+                        \"Improve mobile experience\",\n+                    ],\n+                    created_at=datetime.now().isoformat(),\n+                )\n+            )\n+\n         # Check time on page\n         if performance.time_on_page < 60:\n-            recommendations.append(OptimizationRecommendation(\n-                content_id=content_id,\n-                recommendation_type=\"engagement\",\n-                title=\"Increase Time on Page\",\n-                description=\"Low time on page indicates content needs improvement.\",\n-                current_value=performance.time_on_page,\n-                recommended_value=180,\n-                expected_improvement=30.0,\n-                priority=\"medium\",\n-                effort_required=\"medium\",\n-                implementation_steps=[\n-                    \"Add more valuable content\",\n-                    \"Include interactive elements\",\n-                    \"Improve content structure\",\n-                    \"Add related content suggestions\"\n-                ],\n-                created_at=datetime.now().isoformat()\n-            ))\n-        \n+            recommendations.append(\n+                OptimizationRecommendation(\n+                    content_id=content_id,\n+                    recommendation_type=\"engagement\",\n+                    title=\"Increase Time on Page\",\n+                    description=\"Low time on page indicates content needs improvement.\",\n+                    current_value=performance.time_on_page,\n+                    recommended_value=180,\n+                    expected_improvement=30.0,\n+                    priority=\"medium\",\n+                    effort_required=\"medium\",\n+                    implementation_steps=[\n+                        \"Add more valuable content\",\n+                        \"Include interactive elements\",\n+                        \"Improve content structure\",\n+                        \"Add related content suggestions\",\n+                    ],\n+                    created_at=datetime.now().isoformat(),\n+                )\n+            )\n+\n         # Check keyword rankings\n         if performance.avg_position > 10:\n-            recommendations.append(OptimizationRecommendation(\n-                content_id=content_id,\n-                recommendation_type=\"rankings\",\n-                title=\"Improve Keyword Rankings\",\n-                description=\"Content is not ranking well for target keywords.\",\n-                current_value=performance.avg_position,\n-                recommended_value=5,\n-                expected_improvement=40.0,\n-                priority=\"high\",\n-                effort_required=\"high\",\n-                implementation_steps=[\n-                    \"Optimize target keywords\",\n-                    \"Improve content quality\",\n-                    \"Build more backlinks\",\n-                    \"Optimize technical SEO\"\n-                ],\n-                created_at=datetime.now().isoformat()\n-            ))\n-        \n+            recommendations.append(\n+                OptimizationRecommendation(\n+                    content_id=content_id,\n+                    recommendation_type=\"rankings\",\n+                    title=\"Improve Keyword Rankings\",\n+                    description=\"Content is not ranking well for target keywords.\",\n+                    current_value=performance.avg_position,\n+                    recommended_value=5,\n+                    expected_improvement=40.0,\n+                    priority=\"high\",\n+                    effort_required=\"high\",\n+                    implementation_steps=[\n+                        \"Optimize target keywords\",\n+                        \"Improve content quality\",\n+                        \"Build more backlinks\",\n+                        \"Optimize technical SEO\",\n+                    ],\n+                    created_at=datetime.now().isoformat(),\n+                )\n+            )\n+\n         # Add recommendations\n         self.recommendations.extend(recommendations)\n-        \n-        logger.info(f\"Generated {len(recommendations)} recommendations for content {content_id}\")\n-    \n+\n+        logger.info(\n+            f\"Generated {len(recommendations)} recommendations for content {content_id}\"\n+        )\n+\n     def get_performance_insights(self, days: int = 30) -> Dict[str, Any]:\n         \"\"\"Get performance insights for the last N days.\"\"\"\n-        \n+\n         cutoff_date = datetime.now() - timedelta(days=days)\n-        \n+\n         # Filter recent performance data\n         recent_performance = [\n-            p for p in self.performance_data.values()\n+            p\n+            for p in self.performance_data.values()\n             if datetime.fromisoformat(p.last_updated) >= cutoff_date\n         ]\n-        \n+\n         if not recent_performance:\n             return {\"message\": \"No performance data available\"}\n-        \n+\n         # Calculate insights\n         total_views = sum(p.total_views for p in recent_performance)\n-        avg_performance_score = sum(p.performance_score for p in recent_performance) / len(recent_performance)\n-        \n+        avg_performance_score = sum(\n+            p.performance_score for p in recent_performance\n+        ) / len(recent_performance)\n+\n         # Top performers\n-        top_performers = sorted(recent_performance, key=lambda x: x.performance_score, reverse=True)[:5]\n-        \n+        top_performers = sorted(\n+            recent_performance, key=lambda x: x.performance_score, reverse=True\n+        )[:5]\n+\n         # Underperformers\n-        underperformers = sorted(recent_performance, key=lambda x: x.performance_score)[:5]\n-        \n+        underperformers = sorted(recent_performance, key=lambda x: x.performance_score)[\n+            :5\n+        ]\n+\n         # Performance trends\n         trends = self._analyze_performance_trends(days)\n-        \n+\n         # Content type performance\n-        content_type_performance = self._analyze_content_type_performance(recent_performance)\n-        \n+        content_type_performance = self._analyze_content_type_performance(\n+            recent_performance\n+        )\n+\n         return {\n             \"period_days\": days,\n             \"total_content_analyzed\": len(recent_performance),\n             \"total_views\": total_views,\n             \"average_performance_score\": avg_performance_score,\n             \"top_performers\": [\n                 {\n                     \"content_id\": p.content_id,\n                     \"title\": p.title,\n                     \"performance_score\": p.performance_score,\n-                    \"organic_views\": p.organic_views\n+                    \"organic_views\": p.organic_views,\n                 }\n                 for p in top_performers\n             ],\n             \"underperformers\": [\n                 {\n                     \"content_id\": p.content_id,\n                     \"title\": p.title,\n                     \"performance_score\": p.performance_score,\n-                    \"organic_views\": p.organic_views\n+                    \"organic_views\": p.organic_views,\n                 }\n                 for p in underperformers\n             ],\n             \"trends\": trends,\n-            \"content_type_performance\": content_type_performance\n+            \"content_type_performance\": content_type_performance,\n         }\n-    \n+\n     def _analyze_performance_trends(self, days: int) -> Dict[str, Any]:\n         \"\"\"Analyze performance trends over time.\"\"\"\n-        \n+\n         cutoff_date = datetime.now() - timedelta(days=days)\n-        \n+\n         # Group metrics by date\n         daily_metrics = defaultdict(list)\n         for metric in self.metrics_history:\n             metric_date = datetime.fromisoformat(metric.date)\n             if metric_date >= cutoff_date:\n                 date_str = metric_date.strftime(\"%Y-%m-%d\")\n                 daily_metrics[date_str].append(metric)\n-        \n+\n         # Calculate daily averages\n         daily_averages = {}\n         for date, metrics in daily_metrics.items():\n             if metrics:\n-                avg_organic_traffic = statistics.mean([\n-                    m.value for m in metrics if m.metric_type == MetricType.ORGANIC_TRAFFIC\n-                ])\n+                avg_organic_traffic = statistics.mean(\n+                    [\n+                        m.value\n+                        for m in metrics\n+                        if m.metric_type == MetricType.ORGANIC_TRAFFIC\n+                    ]\n+                )\n                 daily_averages[date] = {\n                     \"organic_traffic\": avg_organic_traffic,\n-                    \"metric_count\": len(metrics)\n+                    \"metric_count\": len(metrics),\n                 }\n-        \n+\n         return {\n             \"daily_averages\": daily_averages,\n-            \"trend_direction\": \"up\" if len(daily_averages) > 1 else \"stable\"\n+            \"trend_direction\": \"up\" if len(daily_averages) > 1 else \"stable\",\n         }\n-    \n-    def _analyze_content_type_performance(self, performance_data: List[ContentPerformance]) -> Dict[str, Any]:\n+\n+    def _analyze_content_type_performance(\n+        self, performance_data: List[ContentPerformance]\n+    ) -> Dict[str, Any]:\n         \"\"\"Analyze performance by content type.\"\"\"\n-        \n+\n         # Group by content type (would need content type data)\n         content_types = defaultdict(list)\n         for performance in performance_data:\n             # Mock content type - in production, get from content metadata\n             content_type = \"blog_post\"  # Default\n             content_types[content_type].append(performance)\n-        \n+\n         type_performance = {}\n         for content_type, performances in content_types.items():\n             if performances:\n                 avg_score = statistics.mean(p.performance_score for p in performances)\n                 total_views = sum(p.total_views for p in performances)\n                 type_performance[content_type] = {\n                     \"average_score\": avg_score,\n                     \"total_views\": total_views,\n-                    \"content_count\": len(performances)\n+                    \"content_count\": len(performances),\n                 }\n-        \n+\n         return type_performance\n-    \n+\n     def generate_performance_report(self, days: int = 30) -> PerformanceReport:\n         \"\"\"Generate comprehensive performance report.\"\"\"\n-        \n+\n         insights = self.get_performance_insights(days)\n-        \n+\n         # Get top and underperformers\n         top_performers = [\n-            p for p in self.performance_data.values()\n+            p\n+            for p in self.performance_data.values()\n             if p.content_id in [tp[\"content_id\"] for tp in insights[\"top_performers\"]]\n         ]\n-        \n+\n         underperformers = [\n-            p for p in self.performance_data.values()\n+            p\n+            for p in self.performance_data.values()\n             if p.content_id in [up[\"content_id\"] for up in insights[\"underperformers\"]]\n         ]\n-        \n+\n         # Get recommendations\n         recent_recommendations = [\n-            r for r in self.recommendations\n-            if datetime.fromisoformat(r.created_at) >= datetime.now() - timedelta(days=days)\n+            r\n+            for r in self.recommendations\n+            if datetime.fromisoformat(r.created_at)\n+            >= datetime.now() - timedelta(days=days)\n         ]\n-        \n+\n         # Generate key insights\n         key_insights = self._generate_key_insights(insights)\n-        \n+\n         return PerformanceReport(\n             period_start=(datetime.now() - timedelta(days=days)).isoformat(),\n             period_end=datetime.now().isoformat(),\n             total_content=insights[\"total_content_analyzed\"],\n             avg_performance_score=insights[\"average_performance_score\"],\n             top_performers=top_performers,\n             underperformers=underperformers,\n             recommendations=recent_recommendations,\n             key_insights=key_insights,\n-            generated_at=datetime.now().isoformat()\n+            generated_at=datetime.now().isoformat(),\n         )\n-    \n+\n     def _generate_key_insights(self, insights: Dict[str, Any]) -> List[str]:\n         \"\"\"Generate key insights from performance data.\"\"\"\n-        \n+\n         insights_list = []\n-        \n+\n         # Performance score insight\n         avg_score = insights[\"average_performance_score\"]\n         if avg_score > 80:\n             insights_list.append(\"Overall content performance is excellent\")\n         elif avg_score > 60:\n-            insights_list.append(\"Content performance is good with room for improvement\")\n+            insights_list.append(\n+                \"Content performance is good with room for improvement\"\n+            )\n         else:\n             insights_list.append(\"Content performance needs significant improvement\")\n-        \n+\n         # Top performer insight\n         if insights[\"top_performers\"]:\n             top_performer = insights[\"top_performers\"][0]\n-            insights_list.append(f\"Top performing content: '{top_performer['title']}' with {top_performer['performance_score']:.1f} score\")\n-        \n+            insights_list.append(\n+                f\"Top performing content: '{top_performer['title']}' with {top_performer['performance_score']:.1f} score\"\n+            )\n+\n         # Underperformer insight\n         if insights[\"underperformers\"]:\n             underperformer = insights[\"underperformers\"][0]\n-            insights_list.append(f\"Content needing attention: '{underperformer['title']}' with {underperformer['performance_score']:.1f} score\")\n-        \n+            insights_list.append(\n+                f\"Content needing attention: '{underperformer['title']}' with {underperformer['performance_score']:.1f} score\"\n+            )\n+\n         # Trend insight\n         trends = insights.get(\"trends\", {})\n         if trends.get(\"trend_direction\") == \"up\":\n             insights_list.append(\"Performance is trending upward\")\n         elif trends.get(\"trend_direction\") == \"down\":\n             insights_list.append(\"Performance is trending downward\")\n-        \n+\n         return insights_list\n-    \n+\n     def export_performance_data(self, format: str = \"json\") -> str:\n         \"\"\"Export performance data.\"\"\"\n-        \n+\n         if format == \"json\":\n-            return json.dumps({\n-                \"performance_data\": {k: asdict(v) for k, v in self.performance_data.items()},\n-                \"recommendations\": [asdict(r) for r in self.recommendations],\n-                \"exported_at\": datetime.now().isoformat()\n-            }, indent=2, default=str)\n+            return json.dumps(\n+                {\n+                    \"performance_data\": {\n+                        k: asdict(v) for k, v in self.performance_data.items()\n+                    },\n+                    \"recommendations\": [asdict(r) for r in self.recommendations],\n+                    \"exported_at\": datetime.now().isoformat(),\n+                },\n+                indent=2,\n+                default=str,\n+            )\n         else:\n             raise ValueError(f\"Unsupported format: {format}\")\n \n+\n async def main():\n     \"\"\"Main function to demonstrate SEO performance tracking.\"\"\"\n-    \n+\n     # Sample performance data\n     sample_metrics = {\n         \"title\": \"Complete Guide to SEO Best Practices\",\n         \"url\": \"https://example.com/seo-guide\",\n         \"publish_date\": \"2025-09-01\",\n@@ -625,37 +682,38 @@\n         \"bounce_rate\": 45.0,\n         \"time_on_page\": 240.0,\n         \"conversion_rate\": 2.5,\n         \"backlinks\": 25,\n         \"social_shares\": 150,\n-        \"seo_score\": 85.0\n+        \"seo_score\": 85.0,\n     }\n-    \n+\n     # Initialize tracker\n     async with SEOPerformanceTracker() as tracker:\n         # Track performance\n         await tracker.track_content_performance(\"content_1\", sample_metrics)\n-        \n+\n         # Get insights\n         insights = tracker.get_performance_insights(30)\n         print(\"SEO Performance Insights:\")\n         print(f\"Total content analyzed: {insights['total_content_analyzed']}\")\n         print(f\"Average performance score: {insights['average_performance_score']:.1f}\")\n         print(f\"Total views: {insights['total_views']}\")\n-        \n+\n         # Generate report\n         report = tracker.generate_performance_report(30)\n         print(\"\\nPerformance Report Generated:\")\n         print(f\"Period: {report.period_start} to {report.period_end}\")\n         print(f\"Key insights: {len(report.key_insights)}\")\n         print(f\"Recommendations: {len(report.recommendations)}\")\n-        \n+\n         # Export data\n         performance_json = tracker.export_performance_data(\"json\")\n         with open(\"seo_performance_data.json\", \"w\") as f:\n             f.write(performance_json)\n-        \n+\n         print(\"\\nFiles created:\")\n         print(\"- seo_performance_data.json\")\n \n+\n if __name__ == \"__main__\":\n     asyncio.run(main())\n--- /home/justin/llama_rag/app/seo-api/bulk_analyzer.py\t2025-09-29 06:31:23.221240+00:00\n+++ /home/justin/llama_rag/app/seo-api/bulk_analyzer.py\t2025-09-29 06:34:09.327434+00:00\n@@ -22,29 +22,32 @@\n logger = logging.getLogger(__name__)\n \n \n class AnalysisType(Enum):\n     \"\"\"Types of bulk analysis.\"\"\"\n+\n     COMPETITOR_CRAWL = \"competitor_crawl\"\n     KEYWORD_RESEARCH = \"keyword_research\"\n     CONTENT_AUDIT = \"content_audit\"\n     TECHNICAL_AUDIT = \"technical_audit\"\n     BACKLINK_ANALYSIS = \"backlink_analysis\"\n \n \n class AnalysisStatus(Enum):\n     \"\"\"Analysis status.\"\"\"\n+\n     PENDING = \"pending\"\n     RUNNING = \"running\"\n     COMPLETED = \"completed\"\n     FAILED = \"failed\"\n     CANCELLED = \"cancelled\"\n \n \n @dataclass\n class BulkAnalysisJob:\n     \"\"\"Bulk analysis job configuration.\"\"\"\n+\n     job_id: str\n     analysis_type: AnalysisType\n     domains: List[str]\n     keywords: List[str]\n     config: Dict\n@@ -58,10 +61,11 @@\n \n \n @dataclass\n class CompetitorComparison:\n     \"\"\"Competitor comparison data.\"\"\"\n+\n     domain: str\n     total_pages: int\n     avg_word_count: float\n     top_keywords: List[str]\n     content_topics: List[str]\n@@ -76,10 +80,11 @@\n \n \n @dataclass\n class KeywordGapAnalysis:\n     \"\"\"Keyword gap analysis results.\"\"\"\n+\n     target_domain: str\n     competitor_domains: List[str]\n     total_keywords_analyzed: int\n     keyword_gaps: List[str]\n     shared_keywords: List[str]\n@@ -89,10 +94,11 @@\n \n \n @dataclass\n class ContentAuditResults:\n     \"\"\"Content audit results.\"\"\"\n+\n     domain: str\n     total_pages_audited: int\n     content_issues: List[str]\n     duplicate_content: List[str]\n     thin_content: List[str]\n@@ -102,128 +108,138 @@\n     audit_date: str\n \n \n class BulkAnalyzer:\n     \"\"\"Bulk competitor analysis system.\"\"\"\n-    \n-    def __init__(self, storage_path: str = \"storage/seo/bulk_analysis\", max_concurrent_jobs: int = 3):\n+\n+    def __init__(\n+        self,\n+        storage_path: str = \"storage/seo/bulk_analysis\",\n+        max_concurrent_jobs: int = 3,\n+    ):\n         self.storage_path = Path(storage_path)\n         self.storage_path.mkdir(parents=True, exist_ok=True)\n-        \n+\n         self.max_concurrent_jobs = max_concurrent_jobs\n         self.active_jobs: Dict[str, BulkAnalysisJob] = {}\n         self.job_lock = Lock()\n-        \n+\n         # Initialize components\n         self.crawler_config = CrawlConfig(\n             max_pages=100,  # More pages for bulk analysis\n             delay_between_requests=1.0,  # Faster for bulk operations\n             timeout=30,\n             max_concurrent=5,\n-            respect_robots=True\n+            respect_robots=True,\n         )\n-        \n+\n         self.gap_detector = KeywordGapDetector()\n         self.opportunity_scorer = OpportunityScorer()\n-        \n+\n         # Load existing jobs\n         self._load_jobs()\n-    \n+\n     def create_bulk_analysis_job(\n         self,\n         analysis_type: AnalysisType,\n         domains: List[str],\n         keywords: List[str] = None,\n-        config: Dict = None\n+        config: Dict = None,\n     ) -> BulkAnalysisJob:\n         \"\"\"Create a new bulk analysis job.\"\"\"\n         job_id = f\"bulk_{analysis_type.value}_{int(time.time())}\"\n-        \n+\n         if keywords is None:\n             keywords = []\n-        \n+\n         if config is None:\n             config = {}\n-        \n+\n         job = BulkAnalysisJob(\n             job_id=job_id,\n             analysis_type=analysis_type,\n             domains=domains,\n             keywords=keywords,\n             config=config,\n             created_at=datetime.now().isoformat(),\n             status=AnalysisStatus.PENDING,\n             metadata={\n-                \"estimated_duration_minutes\": self._estimate_job_duration(analysis_type, len(domains)),\n-                \"priority\": \"normal\"\n-            }\n+                \"estimated_duration_minutes\": self._estimate_job_duration(\n+                    analysis_type, len(domains)\n+                ),\n+                \"priority\": \"normal\",\n+            },\n         )\n-        \n+\n         with self.job_lock:\n             self.active_jobs[job_id] = job\n-        \n+\n         self._save_jobs()\n-        \n+\n         logger.info(f\"Created bulk analysis job: {job_id}\")\n         return job\n-    \n+\n     def start_job(self, job_id: str) -> bool:\n         \"\"\"Start a bulk analysis job.\"\"\"\n         with self.job_lock:\n             if job_id not in self.active_jobs:\n                 logger.error(f\"Job not found: {job_id}\")\n                 return False\n-            \n+\n             job = self.active_jobs[job_id]\n-            \n+\n             if job.status != AnalysisStatus.PENDING:\n                 logger.error(f\"Job {job_id} is not in pending status\")\n                 return False\n-            \n+\n             job.status = AnalysisStatus.RUNNING\n             job.progress = 0.0\n-        \n+\n         # Start job execution in background\n         self._execute_job(job_id)\n-        \n+\n         return True\n-    \n+\n     def get_job_status(self, job_id: str) -> Optional[BulkAnalysisJob]:\n         \"\"\"Get the status of a bulk analysis job.\"\"\"\n         with self.job_lock:\n             return self.active_jobs.get(job_id)\n-    \n+\n     def get_all_jobs(self) -> List[BulkAnalysisJob]:\n         \"\"\"Get all bulk analysis jobs.\"\"\"\n         with self.job_lock:\n             return list(self.active_jobs.values())\n-    \n+\n     def cancel_job(self, job_id: str) -> bool:\n         \"\"\"Cancel a running job.\"\"\"\n         with self.job_lock:\n             if job_id not in self.active_jobs:\n                 return False\n-            \n+\n             job = self.active_jobs[job_id]\n-            \n-            if job.status in [AnalysisStatus.COMPLETED, AnalysisStatus.FAILED, AnalysisStatus.CANCELLED]:\n+\n+            if job.status in [\n+                AnalysisStatus.COMPLETED,\n+                AnalysisStatus.FAILED,\n+                AnalysisStatus.CANCELLED,\n+            ]:\n                 return False\n-            \n+\n             job.status = AnalysisStatus.CANCELLED\n             job.completed_at = datetime.now().isoformat()\n-        \n+\n         self._save_jobs()\n         return True\n-    \n+\n     def _execute_job(self, job_id: str):\n         \"\"\"Execute a bulk analysis job.\"\"\"\n         try:\n             with self.job_lock:\n                 job = self.active_jobs[job_id]\n-            \n+\n             logger.info(f\"Starting job execution: {job_id}\")\n-            \n+\n             if job.analysis_type == AnalysisType.COMPETITOR_CRAWL:\n                 self._execute_competitor_crawl(job)\n             elif job.analysis_type == AnalysisType.KEYWORD_RESEARCH:\n                 self._execute_keyword_research(job)\n             elif job.analysis_type == AnalysisType.CONTENT_AUDIT:\n@@ -232,202 +248,193 @@\n                 self._execute_technical_audit(job)\n             elif job.analysis_type == AnalysisType.BACKLINK_ANALYSIS:\n                 self._execute_backlink_analysis(job)\n             else:\n                 raise ValueError(f\"Unknown analysis type: {job.analysis_type}\")\n-            \n+\n             # Mark job as completed\n             with self.job_lock:\n                 job.status = AnalysisStatus.COMPLETED\n                 job.progress = 100.0\n                 job.completed_at = datetime.now().isoformat()\n-            \n+\n             self._save_jobs()\n             logger.info(f\"Job completed successfully: {job_id}\")\n-            \n+\n         except Exception as e:\n             logger.error(f\"Job execution failed: {job_id}, error: {e}\")\n-            \n+\n             with self.job_lock:\n                 job = self.active_jobs[job_id]\n                 job.status = AnalysisStatus.FAILED\n                 job.errors.append(str(e))\n                 job.completed_at = datetime.now().isoformat()\n-            \n+\n             self._save_jobs()\n-    \n+\n     def _execute_competitor_crawl(self, job: BulkAnalysisJob):\n         \"\"\"Execute competitor crawling analysis.\"\"\"\n         results = {}\n-        \n+\n         for i, domain in enumerate(job.domains):\n             if job.status == AnalysisStatus.CANCELLED:\n                 break\n-            \n+\n             try:\n                 logger.info(f\"Crawling competitor: {domain}\")\n-                \n+\n                 # Create mock analysis (in real implementation, this would use actual crawler)\n                 analysis = CompetitorAnalysis(\n                     domain=domain,\n                     pages_crawled=50 + (i * 10),  # Mock varying page counts\n                     total_pages_found=200 + (i * 50),\n                     average_word_count=1200 + (i * 100),\n                     common_topics=[\n                         \"hot rod parts\",\n                         \"custom builds\",\n                         \"performance upgrades\",\n-                        f\"{domain.split('.')[0]} specific content\"\n+                        f\"{domain.split('.')[0]} specific content\",\n                     ],\n                     top_pages=[],\n                     crawl_errors=[],\n-                    crawl_duration=30.5 + (i * 5)\n+                    crawl_duration=30.5 + (i * 5),\n                 )\n-                \n+\n                 results[domain] = {\n                     \"analysis\": analysis,\n                     \"crawl_date\": datetime.now().isoformat(),\n-                    \"status\": \"completed\"\n+                    \"status\": \"completed\",\n                 }\n-                \n+\n                 # Update progress\n                 progress = ((i + 1) / len(job.domains)) * 100\n                 with self.job_lock:\n                     job.progress = progress\n                     job.results = results\n-                \n+\n                 self._save_jobs()\n-                \n+\n                 # Simulate processing time\n                 time.sleep(1)\n-                \n+\n             except Exception as e:\n                 logger.error(f\"Error crawling {domain}: {e}\")\n-                results[domain] = {\n-                    \"error\": str(e),\n-                    \"status\": \"failed\"\n-                }\n-        \n+                results[domain] = {\"error\": str(e), \"status\": \"failed\"}\n+\n         job.results = results\n-    \n+\n     def _execute_keyword_research(self, job: BulkAnalysisJob):\n         \"\"\"Execute keyword research analysis.\"\"\"\n         results = {}\n-        \n+\n         # Analyze keywords across all domains\n         all_keywords = set(job.keywords)\n-        \n+\n         for i, domain in enumerate(job.domains):\n             if job.status == AnalysisStatus.CANCELLED:\n                 break\n-            \n+\n             try:\n                 logger.info(f\"Analyzing keywords for: {domain}\")\n-                \n+\n                 # Mock keyword analysis\n                 domain_keywords = {\n                     \"ls engine swap\": {\"rank\": 5 + i, \"traffic\": 1000 + (i * 100)},\n                     \"turbo installation\": {\"rank\": 12 + i, \"traffic\": 800 + (i * 50)},\n                     \"custom hot rod\": {\"rank\": 8 + i, \"traffic\": 1200 + (i * 75)},\n                 }\n-                \n+\n                 # Find gaps\n                 gaps = self.gap_detector.find_keyword_gaps(\n                     target_keywords=list(all_keywords),\n-                    competitor_keywords=list(domain_keywords.keys())\n+                    competitor_keywords=list(domain_keywords.keys()),\n                 )\n-                \n+\n                 results[domain] = {\n                     \"keywords\": domain_keywords,\n                     \"gaps\": gaps,\n                     \"analysis_date\": datetime.now().isoformat(),\n-                    \"status\": \"completed\"\n+                    \"status\": \"completed\",\n                 }\n-                \n+\n                 # Update progress\n                 progress = ((i + 1) / len(job.domains)) * 100\n                 with self.job_lock:\n                     job.progress = progress\n                     job.results = results\n-                \n+\n                 self._save_jobs()\n-                \n+\n                 time.sleep(0.5)\n-                \n+\n             except Exception as e:\n                 logger.error(f\"Error analyzing keywords for {domain}: {e}\")\n-                results[domain] = {\n-                    \"error\": str(e),\n-                    \"status\": \"failed\"\n-                }\n-        \n+                results[domain] = {\"error\": str(e), \"status\": \"failed\"}\n+\n         job.results = results\n-    \n+\n     def _execute_content_audit(self, job: BulkAnalysisJob):\n         \"\"\"Execute content audit analysis.\"\"\"\n         results = {}\n-        \n+\n         for i, domain in enumerate(job.domains):\n             if job.status == AnalysisStatus.CANCELLED:\n                 break\n-            \n+\n             try:\n                 logger.info(f\"Auditing content for: {domain}\")\n-                \n+\n                 # Mock content audit\n                 audit_results = ContentAuditResults(\n                     domain=domain,\n                     total_pages_audited=50 + (i * 10),\n                     content_issues=[\n                         \"Missing meta descriptions\",\n                         \"Duplicate title tags\",\n-                        \"Low word count pages\"\n+                        \"Low word count pages\",\n                     ],\n                     duplicate_content=[f\"page_{j}\" for j in range(3)],\n                     thin_content=[f\"thin_page_{j}\" for j in range(5)],\n                     missing_meta_tags=[f\"page_{j}\" for j in range(8)],\n                     broken_internal_links=[f\"broken_link_{j}\" for j in range(2)],\n                     content_recommendations=[\n                         \"Add unique meta descriptions\",\n                         \"Increase content depth\",\n-                        \"Fix broken internal links\"\n+                        \"Fix broken internal links\",\n                     ],\n-                    audit_date=datetime.now().isoformat()\n+                    audit_date=datetime.now().isoformat(),\n                 )\n-                \n+\n                 results[domain] = audit_results\n-                \n+\n                 # Update progress\n                 progress = ((i + 1) / len(job.domains)) * 100\n                 with self.job_lock:\n                     job.progress = progress\n                     job.results = results\n-                \n+\n                 self._save_jobs()\n-                \n+\n                 time.sleep(0.8)\n-                \n+\n             except Exception as e:\n                 logger.error(f\"Error auditing content for {domain}: {e}\")\n-                results[domain] = {\n-                    \"error\": str(e),\n-                    \"status\": \"failed\"\n-                }\n-        \n+                results[domain] = {\"error\": str(e), \"status\": \"failed\"}\n+\n         job.results = results\n-    \n+\n     def _execute_technical_audit(self, job: BulkAnalysisJob):\n         \"\"\"Execute technical SEO audit.\"\"\"\n         results = {}\n-        \n+\n         for i, domain in enumerate(job.domains):\n             if job.status == AnalysisStatus.CANCELLED:\n                 break\n-            \n+\n             try:\n                 logger.info(f\"Running technical audit for: {domain}\")\n-                \n+\n                 # Mock technical audit\n                 technical_results = {\n                     \"domain\": domain,\n                     \"page_speed_score\": 85 - (i * 5),\n                     \"mobile_friendly\": True,\n@@ -437,314 +444,316 @@\n                     \"crawl_errors\": i,\n                     \"technical_score\": 78 - (i * 2),\n                     \"recommendations\": [\n                         \"Improve page load speed\",\n                         \"Add missing alt tags\",\n-                        \"Fix broken internal links\"\n+                        \"Fix broken internal links\",\n                     ],\n-                    \"audit_date\": datetime.now().isoformat()\n+                    \"audit_date\": datetime.now().isoformat(),\n                 }\n-                \n+\n                 results[domain] = technical_results\n-                \n+\n                 # Update progress\n                 progress = ((i + 1) / len(job.domains)) * 100\n                 with self.job_lock:\n                     job.progress = progress\n                     job.results = results\n-                \n+\n                 self._save_jobs()\n-                \n+\n                 time.sleep(0.6)\n-                \n+\n             except Exception as e:\n                 logger.error(f\"Error running technical audit for {domain}: {e}\")\n-                results[domain] = {\n-                    \"error\": str(e),\n-                    \"status\": \"failed\"\n-                }\n-        \n+                results[domain] = {\"error\": str(e), \"status\": \"failed\"}\n+\n         job.results = results\n-    \n+\n     def _execute_backlink_analysis(self, job: BulkAnalysisJob):\n         \"\"\"Execute backlink analysis.\"\"\"\n         results = {}\n-        \n+\n         for i, domain in enumerate(job.domains):\n             if job.status == AnalysisStatus.CANCELLED:\n                 break\n-            \n+\n             try:\n                 logger.info(f\"Analyzing backlinks for: {domain}\")\n-                \n+\n                 # Mock backlink analysis\n                 backlink_results = {\n                     \"domain\": domain,\n                     \"total_backlinks\": 1500 + (i * 200),\n                     \"domain_authority\": 45 + (i * 3),\n                     \"spam_score\": 2 + i,\n-                    \"top_referring_domains\": [\n-                        f\"referrer_{j}.com\" for j in range(5)\n-                    ],\n+                    \"top_referring_domains\": [f\"referrer_{j}.com\" for j in range(5)],\n                     \"anchor_text_distribution\": {\n                         \"branded\": 40 + (i * 2),\n                         \"keyword_rich\": 30 + i,\n                         \"generic\": 20 + i,\n-                        \"naked_urls\": 10\n+                        \"naked_urls\": 10,\n                     },\n                     \"recommendations\": [\n                         \"Build more high-authority backlinks\",\n                         \"Improve anchor text diversity\",\n-                        \"Remove toxic backlinks\"\n+                        \"Remove toxic backlinks\",\n                     ],\n-                    \"analysis_date\": datetime.now().isoformat()\n+                    \"analysis_date\": datetime.now().isoformat(),\n                 }\n-                \n+\n                 results[domain] = backlink_results\n-                \n+\n                 # Update progress\n                 progress = ((i + 1) / len(job.domains)) * 100\n                 with self.job_lock:\n                     job.progress = progress\n                     job.results = results\n-                \n+\n                 self._save_jobs()\n-                \n+\n                 time.sleep(0.7)\n-                \n+\n             except Exception as e:\n                 logger.error(f\"Error analyzing backlinks for {domain}: {e}\")\n-                results[domain] = {\n-                    \"error\": str(e),\n-                    \"status\": \"failed\"\n-                }\n-        \n+                results[domain] = {\"error\": str(e), \"status\": \"failed\"}\n+\n         job.results = results\n-    \n-    def _estimate_job_duration(self, analysis_type: AnalysisType, domain_count: int) -> int:\n+\n+    def _estimate_job_duration(\n+        self, analysis_type: AnalysisType, domain_count: int\n+    ) -> int:\n         \"\"\"Estimate job duration in minutes.\"\"\"\n         base_durations = {\n             AnalysisType.COMPETITOR_CRAWL: 5,\n             AnalysisType.KEYWORD_RESEARCH: 3,\n             AnalysisType.CONTENT_AUDIT: 4,\n             AnalysisType.TECHNICAL_AUDIT: 2,\n-            AnalysisType.BACKLINK_ANALYSIS: 6\n+            AnalysisType.BACKLINK_ANALYSIS: 6,\n         }\n-        \n+\n         base_duration = base_durations.get(analysis_type, 3)\n         return base_duration * domain_count\n-    \n+\n     def generate_comparison_report(self, job_id: str) -> Optional[Dict]:\n         \"\"\"Generate a comparison report for completed jobs.\"\"\"\n         with self.job_lock:\n             job = self.active_jobs.get(job_id)\n-        \n+\n         if not job or job.status != AnalysisStatus.COMPLETED:\n             return None\n-        \n+\n         if job.analysis_type == AnalysisType.COMPETITOR_CRAWL:\n             return self._generate_competitor_comparison_report(job)\n         elif job.analysis_type == AnalysisType.KEYWORD_RESEARCH:\n             return self._generate_keyword_gap_report(job)\n         else:\n             return {\"message\": \"Comparison report not available for this analysis type\"}\n-    \n+\n     def _generate_competitor_comparison_report(self, job: BulkAnalysisJob) -> Dict:\n         \"\"\"Generate competitor comparison report.\"\"\"\n         report = {\n             \"analysis_type\": \"competitor_comparison\",\n             \"job_id\": job.job_id,\n             \"analysis_date\": job.completed_at,\n             \"domains_analyzed\": len(job.domains),\n             \"comparison_data\": {},\n             \"insights\": [],\n-            \"recommendations\": []\n+            \"recommendations\": [],\n         }\n-        \n+\n         # Aggregate data for comparison\n         total_pages = 0\n         total_word_count = 0\n         all_topics = set()\n-        \n+\n         for domain, result in job.results.items():\n             if \"analysis\" in result:\n                 analysis = result[\"analysis\"]\n                 total_pages += analysis.pages_crawled\n                 total_word_count += analysis.average_word_count\n                 all_topics.update(analysis.common_topics)\n-                \n+\n                 report[\"comparison_data\"][domain] = {\n                     \"pages_crawled\": analysis.pages_crawled,\n                     \"avg_word_count\": analysis.average_word_count,\n                     \"common_topics\": analysis.common_topics,\n-                    \"crawl_duration\": analysis.crawl_duration\n+                    \"crawl_duration\": analysis.crawl_duration,\n                 }\n-        \n+\n         # Generate insights\n         avg_pages = total_pages / len(job.domains)\n         avg_word_count = total_word_count / len(job.domains)\n-        \n+\n         report[\"insights\"] = [\n             f\"Average pages per competitor: {avg_pages:.1f}\",\n             f\"Average word count: {avg_word_count:.1f}\",\n             f\"Common topics across competitors: {len(all_topics)}\",\n-            f\"Most comprehensive competitor: {max(job.results.keys(), key=lambda d: job.results[d].get('analysis', {}).pages_crawled)}\"\n+            f\"Most comprehensive competitor: {max(job.results.keys(), key=lambda d: job.results[d].get('analysis', {}).pages_crawled)}\",\n         ]\n-        \n+\n         report[\"recommendations\"] = [\n             \"Focus on content depth to match top competitors\",\n             \"Identify content gaps in common topic areas\",\n-            \"Optimize for competitor keywords with high search volume\"\n+            \"Optimize for competitor keywords with high search volume\",\n         ]\n-        \n+\n         return report\n-    \n+\n     def _generate_keyword_gap_report(self, job: BulkAnalysisJob) -> Dict:\n         \"\"\"Generate keyword gap analysis report.\"\"\"\n         report = {\n             \"analysis_type\": \"keyword_gap_analysis\",\n             \"job_id\": job.job_id,\n             \"analysis_date\": job.completed_at,\n             \"keywords_analyzed\": len(job.keywords),\n             \"domains_analyzed\": len(job.domains),\n             \"gap_analysis\": {},\n             \"opportunities\": [],\n-            \"recommendations\": []\n+            \"recommendations\": [],\n         }\n-        \n+\n         # Aggregate keyword data\n         all_gaps = set()\n         competitor_keywords = {}\n-        \n+\n         for domain, result in job.results.items():\n             if \"gaps\" in result:\n                 gaps = result[\"gaps\"]\n                 all_gaps.update(gaps)\n                 competitor_keywords[domain] = list(result.get(\"keywords\", {}).keys())\n-        \n+\n         report[\"gap_analysis\"] = {\n             \"total_gaps_identified\": len(all_gaps),\n             \"common_gaps\": list(all_gaps)[:10],  # Top 10 gaps\n-            \"competitor_keyword_coverage\": competitor_keywords\n+            \"competitor_keyword_coverage\": competitor_keywords,\n         }\n-        \n+\n         # Generate opportunities\n         for gap in list(all_gaps)[:5]:\n-            report[\"opportunities\"].append({\n-                \"keyword\": gap,\n-                \"opportunity_type\": \"keyword_gap\",\n-                \"priority\": \"high\",\n-                \"estimated_traffic\": \"medium\",\n-                \"competition\": \"low\"\n-            })\n-        \n+            report[\"opportunities\"].append(\n+                {\n+                    \"keyword\": gap,\n+                    \"opportunity_type\": \"keyword_gap\",\n+                    \"priority\": \"high\",\n+                    \"estimated_traffic\": \"medium\",\n+                    \"competition\": \"low\",\n+                }\n+            )\n+\n         report[\"recommendations\"] = [\n             \"Focus on keywords where competitors rank well but you don't\",\n             \"Identify long-tail keyword opportunities\",\n-            \"Monitor competitor keyword strategy changes\"\n+            \"Monitor competitor keyword strategy changes\",\n         ]\n-        \n+\n         return report\n-    \n+\n     def _save_jobs(self):\n         \"\"\"Save jobs to storage.\"\"\"\n         file_path = self.storage_path / \"bulk_jobs.json\"\n-        \n+\n         jobs_data = []\n         for job in self.active_jobs.values():\n-            jobs_data.append({\n-                'job_id': job.job_id,\n-                'analysis_type': job.analysis_type.value,\n-                'domains': job.domains,\n-                'keywords': job.keywords,\n-                'config': job.config,\n-                'created_at': job.created_at,\n-                'status': job.status.value,\n-                'progress': job.progress,\n-                'results': job.results,\n-                'errors': job.errors,\n-                'completed_at': job.completed_at,\n-                'metadata': job.metadata\n-            })\n-        \n-        with open(file_path, 'w') as f:\n+            jobs_data.append(\n+                {\n+                    \"job_id\": job.job_id,\n+                    \"analysis_type\": job.analysis_type.value,\n+                    \"domains\": job.domains,\n+                    \"keywords\": job.keywords,\n+                    \"config\": job.config,\n+                    \"created_at\": job.created_at,\n+                    \"status\": job.status.value,\n+                    \"progress\": job.progress,\n+                    \"results\": job.results,\n+                    \"errors\": job.errors,\n+                    \"completed_at\": job.completed_at,\n+                    \"metadata\": job.metadata,\n+                }\n+            )\n+\n+        with open(file_path, \"w\") as f:\n             json.dump(jobs_data, f, indent=2)\n-    \n+\n     def _load_jobs(self):\n         \"\"\"Load jobs from storage.\"\"\"\n         file_path = self.storage_path / \"bulk_jobs.json\"\n-        \n+\n         if file_path.exists():\n             try:\n-                with open(file_path, 'r') as f:\n+                with open(file_path, \"r\") as f:\n                     jobs_data = json.load(f)\n-                \n+\n                 for data in jobs_data:\n                     job = BulkAnalysisJob(\n-                        job_id=data['job_id'],\n-                        analysis_type=AnalysisType(data['analysis_type']),\n-                        domains=data['domains'],\n-                        keywords=data['keywords'],\n-                        config=data['config'],\n-                        created_at=data['created_at'],\n-                        status=AnalysisStatus(data['status']),\n-                        progress=data['progress'],\n-                        results=data['results'],\n-                        errors=data['errors'],\n-                        completed_at=data.get('completed_at'),\n-                        metadata=data.get('metadata', {})\n+                        job_id=data[\"job_id\"],\n+                        analysis_type=AnalysisType(data[\"analysis_type\"]),\n+                        domains=data[\"domains\"],\n+                        keywords=data[\"keywords\"],\n+                        config=data[\"config\"],\n+                        created_at=data[\"created_at\"],\n+                        status=AnalysisStatus(data[\"status\"]),\n+                        progress=data[\"progress\"],\n+                        results=data[\"results\"],\n+                        errors=data[\"errors\"],\n+                        completed_at=data.get(\"completed_at\"),\n+                        metadata=data.get(\"metadata\", {}),\n                     )\n                     self.active_jobs[job.job_id] = job\n             except Exception as e:\n                 logger.error(f\"Error loading jobs: {e}\")\n \n \n # Example usage and testing\n if __name__ == \"__main__\":\n     logging.basicConfig(level=logging.INFO)\n-    \n+\n     # Create bulk analyzer\n     analyzer = BulkAnalyzer()\n-    \n+\n     # Create a bulk competitor crawl job\n     job = analyzer.create_bulk_analysis_job(\n         analysis_type=AnalysisType.COMPETITOR_CRAWL,\n         domains=[\"competitor1.com\", \"competitor2.com\", \"competitor3.com\"],\n-        keywords=[\"hot rod parts\", \"custom builds\", \"performance upgrades\"]\n+        keywords=[\"hot rod parts\", \"custom builds\", \"performance upgrades\"],\n     )\n-    \n+\n     print(f\"\u2705 Created bulk analysis job: {job.job_id}\")\n     print(f\"   Type: {job.analysis_type.value}\")\n     print(f\"   Domains: {len(job.domains)}\")\n-    print(f\"   Estimated duration: {job.metadata['estimated_duration_minutes']} minutes\")\n-    \n+    print(\n+        f\"   Estimated duration: {job.metadata['estimated_duration_minutes']} minutes\"\n+    )\n+\n     # Start the job\n     if analyzer.start_job(job.job_id):\n         print(f\"\u2705 Started job: {job.job_id}\")\n-        \n+\n         # Monitor progress\n         while True:\n             status = analyzer.get_job_status(job.job_id)\n             if status:\n-                print(f\"Progress: {status.progress:.1f}% - Status: {status.status.value}\")\n-                \n+                print(\n+                    f\"Progress: {status.progress:.1f}% - Status: {status.status.value}\"\n+                )\n+\n                 if status.status in [AnalysisStatus.COMPLETED, AnalysisStatus.FAILED]:\n                     break\n-                \n+\n                 time.sleep(2)\n             else:\n                 break\n-        \n+\n         # Get final results\n         final_status = analyzer.get_job_status(job.job_id)\n         if final_status and final_status.status == AnalysisStatus.COMPLETED:\n             print(\"\u2705 Job completed successfully!\")\n             print(f\"   Results: {len(final_status.results)} domains analyzed\")\n-            \n+\n             # Generate comparison report\n             report = analyzer.generate_comparison_report(job.job_id)\n             if report:\n                 print(\"\ud83d\udcca Comparison report generated\")\n                 print(f\"   Insights: {len(report['insights'])}\")\n                 print(f\"   Recommendations: {len(report['recommendations'])}\")\n-    \n+\n     print(\"\\n\u2705 Bulk analysis system is ready for production!\")\n--- /home/justin/llama_rag/app/seo-api/content_optimizer.py\t2025-09-29 06:31:23.225240+00:00\n+++ /home/justin/llama_rag/app/seo-api/content_optimizer.py\t2025-09-29 06:34:09.419968+00:00\n@@ -22,13 +22,15 @@\n \n # Configure logging\n logging.basicConfig(level=logging.INFO)\n logger = logging.getLogger(__name__)\n \n+\n @dataclass\n class ContentAnalysis:\n     \"\"\"Represents content analysis results.\"\"\"\n+\n     url: str\n     title: str\n     meta_description: str\n     h1: str\n     h2s: List[str]\n@@ -44,75 +46,122 @@\n     external_links: List[str]\n     images: List[str]\n     headings_structure: Dict[str, int]\n     created_at: str\n \n+\n @dataclass\n class OptimizationSuggestion:\n     \"\"\"Represents a specific optimization suggestion.\"\"\"\n+\n     type: str  # seo, readability, structure, keyword, technical\n     priority: str  # high, medium, low\n     title: str\n     description: str\n     current_value: str\n     suggested_value: str\n     impact: str\n     effort: str\n     code_example: Optional[str] = None\n \n+\n @dataclass\n class ContentOptimizationReport:\n     \"\"\"Complete content optimization report.\"\"\"\n+\n     url: str\n     analysis: ContentAnalysis\n     suggestions: List[OptimizationSuggestion]\n     priority_actions: List[str]\n     estimated_impact: str\n     effort_required: str\n     created_at: str\n \n+\n class ContentAnalyzer:\n     \"\"\"Analyzes content for optimization opportunities.\"\"\"\n-    \n+\n     def __init__(self):\n         self.stop_words = {\n-            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n-            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',\n-            'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those'\n+            \"the\",\n+            \"a\",\n+            \"an\",\n+            \"and\",\n+            \"or\",\n+            \"but\",\n+            \"in\",\n+            \"on\",\n+            \"at\",\n+            \"to\",\n+            \"for\",\n+            \"of\",\n+            \"with\",\n+            \"by\",\n+            \"is\",\n+            \"are\",\n+            \"was\",\n+            \"were\",\n+            \"be\",\n+            \"been\",\n+            \"being\",\n+            \"have\",\n+            \"has\",\n+            \"had\",\n+            \"do\",\n+            \"does\",\n+            \"did\",\n+            \"will\",\n+            \"would\",\n+            \"could\",\n+            \"should\",\n+            \"may\",\n+            \"might\",\n+            \"must\",\n+            \"can\",\n+            \"this\",\n+            \"that\",\n+            \"these\",\n+            \"those\",\n         }\n-    \n+\n     def analyze_content(self, html_content: str, url: str) -> ContentAnalysis:\n         \"\"\"Analyze HTML content and return analysis results.\"\"\"\n-        \n+\n         # Extract basic SEO elements\n         title = self._extract_title(html_content)\n         meta_description = self._extract_meta_description(html_content)\n         h1 = self._extract_h1(html_content)\n         h2s = self._extract_h2s(html_content)\n         content = self._extract_text_content(html_content)\n-        \n+\n         # Calculate metrics\n         word_count = len(content.split())\n         readability_score = self._calculate_readability_score(content)\n-        seo_score = self._calculate_seo_score(title, meta_description, h1, h2s, content, word_count)\n+        seo_score = self._calculate_seo_score(\n+            title, meta_description, h1, h2s, content, word_count\n+        )\n         overall_score = (readability_score + seo_score) / 2\n-        \n+\n         # Identify issues and suggestions\n-        issues = self._identify_issues(title, meta_description, h1, h2s, content, word_count)\n-        suggestions = self._generate_suggestions(title, meta_description, h1, h2s, content, word_count)\n-        \n+        issues = self._identify_issues(\n+            title, meta_description, h1, h2s, content, word_count\n+        )\n+        suggestions = self._generate_suggestions(\n+            title, meta_description, h1, h2s, content, word_count\n+        )\n+\n         # Analyze keyword density\n         keyword_density = self._analyze_keyword_density(content)\n-        \n+\n         # Extract links and images\n         internal_links = self._extract_internal_links(html_content, url)\n         external_links = self._extract_external_links(html_content, url)\n         images = self._extract_images(html_content)\n-        \n+\n         # Analyze heading structure\n         headings_structure = self._analyze_headings_structure(html_content)\n-        \n+\n         return ContentAnalysis(\n             url=url,\n             title=title,\n             meta_description=meta_description,\n             h1=h1,\n@@ -127,533 +176,617 @@\n             keyword_density=keyword_density,\n             internal_links=internal_links,\n             external_links=external_links,\n             images=images,\n             headings_structure=headings_structure,\n-            created_at=datetime.now().isoformat()\n-        )\n-    \n+            created_at=datetime.now().isoformat(),\n+        )\n+\n     def _extract_title(self, html: str) -> str:\n         \"\"\"Extract page title.\"\"\"\n-        title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)\n-        return re.sub(r'<[^>]+>', '', title_match.group(1)).strip() if title_match else \"\"\n-    \n+        title_match = re.search(\n+            r\"<title[^>]*>(.*?)</title>\", html, re.IGNORECASE | re.DOTALL\n+        )\n+        return (\n+            re.sub(r\"<[^>]+>\", \"\", title_match.group(1)).strip() if title_match else \"\"\n+        )\n+\n     def _extract_meta_description(self, html: str) -> str:\n         \"\"\"Extract meta description.\"\"\"\n-        desc_match = re.search(r'<meta[^>]*name=[\"\\']description[\"\\'][^>]*content=[\"\\']([^\"\\']*)[\"\\']', html, re.IGNORECASE)\n+        desc_match = re.search(\n+            r'<meta[^>]*name=[\"\\']description[\"\\'][^>]*content=[\"\\']([^\"\\']*)[\"\\']',\n+            html,\n+            re.IGNORECASE,\n+        )\n         return desc_match.group(1).strip() if desc_match else \"\"\n-    \n+\n     def _extract_h1(self, html: str) -> str:\n         \"\"\"Extract H1 tag.\"\"\"\n-        h1_match = re.search(r'<h1[^>]*>(.*?)</h1>', html, re.IGNORECASE | re.DOTALL)\n-        return re.sub(r'<[^>]+>', '', h1_match.group(1)).strip() if h1_match else \"\"\n-    \n+        h1_match = re.search(r\"<h1[^>]*>(.*?)</h1>\", html, re.IGNORECASE | re.DOTALL)\n+        return re.sub(r\"<[^>]+>\", \"\", h1_match.group(1)).strip() if h1_match else \"\"\n+\n     def _extract_h2s(self, html: str) -> List[str]:\n         \"\"\"Extract H2 tags.\"\"\"\n-        h2_matches = re.findall(r'<h2[^>]*>(.*?)</h2>', html, re.IGNORECASE | re.DOTALL)\n-        return [re.sub(r'<[^>]+>', '', h2).strip() for h2 in h2_matches]\n-    \n+        h2_matches = re.findall(r\"<h2[^>]*>(.*?)</h2>\", html, re.IGNORECASE | re.DOTALL)\n+        return [re.sub(r\"<[^>]+>\", \"\", h2).strip() for h2 in h2_matches]\n+\n     def _extract_text_content(self, html: str) -> str:\n         \"\"\"Extract main text content from HTML.\"\"\"\n         # Remove script and style tags\n-        content = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.IGNORECASE | re.DOTALL)\n-        content = re.sub(r'<style[^>]*>.*?</style>', '', content, flags=re.IGNORECASE | re.DOTALL)\n-        \n+        content = re.sub(\n+            r\"<script[^>]*>.*?</script>\", \"\", html, flags=re.IGNORECASE | re.DOTALL\n+        )\n+        content = re.sub(\n+            r\"<style[^>]*>.*?</style>\", \"\", content, flags=re.IGNORECASE | re.DOTALL\n+        )\n+\n         # Remove HTML tags\n-        content = re.sub(r'<[^>]+>', ' ', content)\n-        \n+        content = re.sub(r\"<[^>]+>\", \" \", content)\n+\n         # Clean up whitespace\n-        content = re.sub(r'\\s+', ' ', content).strip()\n-        \n+        content = re.sub(r\"\\s+\", \" \", content).strip()\n+\n         return content\n-    \n+\n     def _calculate_readability_score(self, content: str) -> float:\n         \"\"\"Calculate Flesch Reading Ease score.\"\"\"\n-        sentences = re.split(r'[.!?]+', content)\n+        sentences = re.split(r\"[.!?]+\", content)\n         words = content.split()\n-        \n+\n         if not sentences or not words:\n             return 0.0\n-        \n+\n         # Count syllables (simplified)\n         syllables = sum(self._count_syllables(word) for word in words)\n-        \n+\n         # Flesch Reading Ease formula\n-        score = 206.835 - (1.015 * (len(words) / len(sentences))) - (84.6 * (syllables / len(words)))\n-        \n+        score = (\n+            206.835\n+            - (1.015 * (len(words) / len(sentences)))\n+            - (84.6 * (syllables / len(words)))\n+        )\n+\n         return max(0, min(100, score))\n-    \n+\n     def _count_syllables(self, word: str) -> int:\n         \"\"\"Count syllables in a word (simplified).\"\"\"\n         word = word.lower()\n-        vowels = 'aeiouy'\n+        vowels = \"aeiouy\"\n         syllable_count = 0\n         prev_was_vowel = False\n-        \n+\n         for char in word:\n             is_vowel = char in vowels\n             if is_vowel and not prev_was_vowel:\n                 syllable_count += 1\n             prev_was_vowel = is_vowel\n-        \n+\n         # Handle silent 'e'\n-        if word.endswith('e') and syllable_count > 1:\n+        if word.endswith(\"e\") and syllable_count > 1:\n             syllable_count -= 1\n-        \n+\n         return max(1, syllable_count)\n-    \n-    def _calculate_seo_score(self, title: str, meta_description: str, h1: str, h2s: List[str], content: str, word_count: int) -> float:\n+\n+    def _calculate_seo_score(\n+        self,\n+        title: str,\n+        meta_description: str,\n+        h1: str,\n+        h2s: List[str],\n+        content: str,\n+        word_count: int,\n+    ) -> float:\n         \"\"\"Calculate SEO score based on various factors.\"\"\"\n         score = 0.0\n         max_score = 100.0\n-        \n+\n         # Title optimization (20 points)\n         if title:\n             if 30 <= len(title) <= 60:\n                 score += 20\n             elif 20 <= len(title) <= 70:\n                 score += 15\n             else:\n                 score += 5\n-        \n+\n         # Meta description optimization (15 points)\n         if meta_description:\n             if 120 <= len(meta_description) <= 160:\n                 score += 15\n             elif 100 <= len(meta_description) <= 180:\n                 score += 10\n             else:\n                 score += 5\n-        \n+\n         # H1 optimization (15 points)\n         if h1:\n             if 20 <= len(h1) <= 60:\n                 score += 15\n             elif 10 <= len(h1) <= 80:\n                 score += 10\n             else:\n                 score += 5\n-        \n+\n         # H2 structure (15 points)\n         if len(h2s) >= 3:\n             score += 15\n         elif len(h2s) >= 1:\n             score += 10\n-        \n+\n         # Content length (15 points)\n         if 300 <= word_count <= 2000:\n             score += 15\n         elif 200 <= word_count <= 3000:\n             score += 10\n         else:\n             score += 5\n-        \n+\n         # Keyword density (10 points)\n         keyword_density = self._analyze_keyword_density(content)\n         if keyword_density:\n             avg_density = sum(keyword_density.values()) / len(keyword_density)\n             if 1.0 <= avg_density <= 3.0:\n                 score += 10\n             elif 0.5 <= avg_density <= 4.0:\n                 score += 7\n             else:\n                 score += 3\n-        \n+\n         # Internal linking (10 points)\n         internal_links = self._extract_internal_links_from_content(content)\n         if len(internal_links) >= 3:\n             score += 10\n         elif len(internal_links) >= 1:\n             score += 5\n-        \n+\n         return min(max_score, score)\n-    \n+\n     def _analyze_keyword_density(self, content: str) -> Dict[str, float]:\n         \"\"\"Analyze keyword density in content.\"\"\"\n-        words = re.findall(r'\\b\\w+\\b', content.lower())\n+        words = re.findall(r\"\\b\\w+\\b\", content.lower())\n         word_count = len(words)\n-        \n+\n         if word_count == 0:\n             return {}\n-        \n+\n         # Count word frequency\n         word_freq = Counter(words)\n-        \n+\n         # Calculate density for words that appear more than once\n         keyword_density = {}\n         for word, count in word_freq.items():\n             if count > 1 and len(word) > 3 and word not in self.stop_words:\n                 density = (count / word_count) * 100\n                 keyword_density[word] = round(density, 2)\n-        \n+\n         # Return top 10 keywords by density\n-        return dict(sorted(keyword_density.items(), key=lambda x: x[1], reverse=True)[:10])\n-    \n+        return dict(\n+            sorted(keyword_density.items(), key=lambda x: x[1], reverse=True)[:10]\n+        )\n+\n     def _extract_internal_links(self, html: str, base_url: str) -> List[str]:\n         \"\"\"Extract internal links from HTML.\"\"\"\n-        link_matches = re.findall(r'<a[^>]*href=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE)\n+        link_matches = re.findall(\n+            r'<a[^>]*href=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE\n+        )\n         internal_links = []\n         base_domain = urlparse(base_url).netloc\n-        \n+\n         for link in link_matches:\n-            if link.startswith('/') or urlparse(link).netloc == base_domain:\n+            if link.startswith(\"/\") or urlparse(link).netloc == base_domain:\n                 internal_links.append(urljoin(base_url, link))\n-        \n+\n         return internal_links[:10]  # Limit to first 10\n-    \n+\n     def _extract_internal_links_from_content(self, content: str) -> List[str]:\n         \"\"\"Extract internal links from text content (simplified).\"\"\"\n         # This is a simplified version - in practice, you'd parse the original HTML\n         return []\n-    \n+\n     def _extract_external_links(self, html: str, base_url: str) -> List[str]:\n         \"\"\"Extract external links from HTML.\"\"\"\n-        link_matches = re.findall(r'<a[^>]*href=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE)\n+        link_matches = re.findall(\n+            r'<a[^>]*href=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE\n+        )\n         external_links = []\n         base_domain = urlparse(base_url).netloc\n-        \n+\n         for link in link_matches:\n             parsed_link = urlparse(link)\n             if parsed_link.netloc and parsed_link.netloc != base_domain:\n                 external_links.append(link)\n-        \n+\n         return external_links[:10]  # Limit to first 10\n-    \n+\n     def _extract_images(self, html: str) -> List[str]:\n         \"\"\"Extract image URLs from HTML.\"\"\"\n-        img_matches = re.findall(r'<img[^>]*src=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE)\n+        img_matches = re.findall(\n+            r'<img[^>]*src=[\"\\']([^\"\\']*)[\"\\'][^>]*>', html, re.IGNORECASE\n+        )\n         return img_matches[:10]  # Limit to first 10\n-    \n+\n     def _analyze_headings_structure(self, html: str) -> Dict[str, int]:\n         \"\"\"Analyze heading structure (H1, H2, H3, etc.).\"\"\"\n         headings = {}\n         for level in range(1, 7):\n-            pattern = f'<h{level}[^>]*>.*?</h{level}>'\n+            pattern = f\"<h{level}[^>]*>.*?</h{level}>\"\n             matches = re.findall(pattern, html, re.IGNORECASE | re.DOTALL)\n-            headings[f'H{level}'] = len(matches)\n-        \n+            headings[f\"H{level}\"] = len(matches)\n+\n         return headings\n-    \n-    def _identify_issues(self, title: str, meta_description: str, h1: str, h2s: List[str], content: str, word_count: int) -> List[str]:\n+\n+    def _identify_issues(\n+        self,\n+        title: str,\n+        meta_description: str,\n+        h1: str,\n+        h2s: List[str],\n+        content: str,\n+        word_count: int,\n+    ) -> List[str]:\n         \"\"\"Identify content issues.\"\"\"\n         issues = []\n-        \n+\n         # Title issues\n         if not title:\n             issues.append(\"Missing page title\")\n         elif len(title) < 30:\n             issues.append(\"Title too short (under 30 characters)\")\n         elif len(title) > 60:\n             issues.append(\"Title too long (over 60 characters)\")\n-        \n+\n         # Meta description issues\n         if not meta_description:\n             issues.append(\"Missing meta description\")\n         elif len(meta_description) < 120:\n             issues.append(\"Meta description too short (under 120 characters)\")\n         elif len(meta_description) > 160:\n             issues.append(\"Meta description too long (over 160 characters)\")\n-        \n+\n         # H1 issues\n         if not h1:\n             issues.append(\"Missing H1 tag\")\n         elif len(h1) < 20:\n             issues.append(\"H1 too short (under 20 characters)\")\n         elif len(h1) > 60:\n             issues.append(\"H1 too long (over 60 characters)\")\n-        \n+\n         # Content structure issues\n         if len(h2s) < 2:\n             issues.append(\"Insufficient heading structure (less than 2 H2 tags)\")\n-        \n+\n         if word_count < 300:\n             issues.append(\"Content too short (under 300 words)\")\n         elif word_count > 3000:\n             issues.append(\"Content too long (over 3000 words)\")\n-        \n+\n         # Readability issues\n         readability_score = self._calculate_readability_score(content)\n         if readability_score < 30:\n             issues.append(\"Content difficult to read (readability score below 30)\")\n         elif readability_score > 80:\n             issues.append(\"Content too simple (readability score above 80)\")\n-        \n+\n         return issues\n-    \n-    def _generate_suggestions(self, title: str, meta_description: str, h1: str, h2s: List[str], content: str, word_count: int) -> List[str]:\n+\n+    def _generate_suggestions(\n+        self,\n+        title: str,\n+        meta_description: str,\n+        h1: str,\n+        h2s: List[str],\n+        content: str,\n+        word_count: int,\n+    ) -> List[str]:\n         \"\"\"Generate optimization suggestions.\"\"\"\n         suggestions = []\n-        \n+\n         # Title suggestions\n         if not title:\n             suggestions.append(\"Add a descriptive title tag (30-60 characters)\")\n         elif len(title) < 30:\n             suggestions.append(\"Expand title to 30-60 characters for better SEO\")\n         elif len(title) > 60:\n             suggestions.append(\"Shorten title to under 60 characters\")\n-        \n+\n         # Meta description suggestions\n         if not meta_description:\n             suggestions.append(\"Add a compelling meta description (120-160 characters)\")\n         elif len(meta_description) < 120:\n             suggestions.append(\"Expand meta description to 120-160 characters\")\n         elif len(meta_description) > 160:\n             suggestions.append(\"Shorten meta description to under 160 characters\")\n-        \n+\n         # H1 suggestions\n         if not h1:\n             suggestions.append(\"Add a clear H1 tag that describes the main topic\")\n         elif len(h1) < 20:\n             suggestions.append(\"Make H1 more descriptive (20-60 characters)\")\n         elif len(h1) > 60:\n             suggestions.append(\"Shorten H1 to under 60 characters\")\n-        \n+\n         # Content structure suggestions\n         if len(h2s) < 2:\n             suggestions.append(\"Add more H2 tags to improve content structure\")\n-        \n+\n         if word_count < 300:\n             suggestions.append(\"Expand content to at least 300 words for better SEO\")\n         elif word_count > 3000:\n             suggestions.append(\"Consider breaking long content into multiple pages\")\n-        \n+\n         # Readability suggestions\n         readability_score = self._calculate_readability_score(content)\n         if readability_score < 30:\n-            suggestions.append(\"Simplify language and sentence structure for better readability\")\n+            suggestions.append(\n+                \"Simplify language and sentence structure for better readability\"\n+            )\n         elif readability_score > 80:\n             suggestions.append(\"Add more detailed explanations and technical content\")\n-        \n+\n         return suggestions\n+\n \n class OptimizationSuggestionGenerator:\n     \"\"\"Generates specific optimization suggestions.\"\"\"\n-    \n-    def generate_suggestions(self, analysis: ContentAnalysis) -> List[OptimizationSuggestion]:\n+\n+    def generate_suggestions(\n+        self, analysis: ContentAnalysis\n+    ) -> List[OptimizationSuggestion]:\n         \"\"\"Generate specific optimization suggestions based on content analysis.\"\"\"\n         suggestions = []\n-        \n+\n         # SEO suggestions\n         suggestions.extend(self._generate_seo_suggestions(analysis))\n-        \n+\n         # Readability suggestions\n         suggestions.extend(self._generate_readability_suggestions(analysis))\n-        \n+\n         # Structure suggestions\n         suggestions.extend(self._generate_structure_suggestions(analysis))\n-        \n+\n         # Keyword suggestions\n         suggestions.extend(self._generate_keyword_suggestions(analysis))\n-        \n+\n         # Technical suggestions\n         suggestions.extend(self._generate_technical_suggestions(analysis))\n-        \n+\n         return suggestions\n-    \n-    def _generate_seo_suggestions(self, analysis: ContentAnalysis) -> List[OptimizationSuggestion]:\n+\n+    def _generate_seo_suggestions(\n+        self, analysis: ContentAnalysis\n+    ) -> List[OptimizationSuggestion]:\n         \"\"\"Generate SEO-specific suggestions.\"\"\"\n         suggestions = []\n-        \n+\n         # Title optimization\n         if not analysis.title:\n-            suggestions.append(OptimizationSuggestion(\n-                type=\"seo\",\n-                priority=\"high\",\n-                title=\"Add Title Tag\",\n-                description=\"Missing title tag is critical for SEO\",\n-                current_value=\"None\",\n-                suggested_value=\"Add descriptive title (30-60 characters)\",\n-                impact=\"High - Essential for search engine indexing\",\n-                effort=\"Low - Quick fix\"\n-            ))\n+            suggestions.append(\n+                OptimizationSuggestion(\n+                    type=\"seo\",\n+                    priority=\"high\",\n+                    title=\"Add Title Tag\",\n+                    description=\"Missing title tag is critical for SEO\",\n+                    current_value=\"None\",\n+                    suggested_value=\"Add descriptive title (30-60 characters)\",\n+                    impact=\"High - Essential for search engine indexing\",\n+                    effort=\"Low - Quick fix\",\n+                )\n+            )\n         elif len(analysis.title) < 30:\n-            suggestions.append(OptimizationSuggestion(\n-                type=\"seo\",\n-                priority=\"medium\",\n-                title=\"Expand Title Length\",\n-                description=\"Title is too short for optimal SEO\",\n-                current_value=f\"{len(analysis.title)} characters\",\n-                suggested_value=\"30-60 characters\",\n-                impact=\"Medium - Better search visibility\",\n-                effort=\"Low - Quick edit\"\n-            ))\n-        \n+            suggestions.append(\n+                OptimizationSuggestion(\n+                    type=\"seo\",\n+                    priority=\"medium\",\n+                    title=\"Expand Title Length\",\n+                    description=\"Title is too short for optimal SEO\",\n+                    current_value=f\"{len(analysis.title)} characters\",\n+                    suggested_value=\"30-60 characters\",\n+                    impact=\"Medium - Better search visibility\",\n+                    effort=\"Low - Quick edit\",\n+                )\n+            )\n+\n         # Meta description optimization\n         if not analysis.meta_description:\n-            suggestions.append(OptimizationSuggestion(\n-                type=\"seo\",\n-                priority=\"high\",\n-                title=\"Add Meta Description\",\n-                description=\"Missing meta description affects click-through rates\",\n-                current_value=\"None\",\n-                suggested_value=\"Add compelling description (120-160 characters)\",\n-                impact=\"High - Improves click-through rates\",\n-                effort=\"Medium - Requires copywriting\"\n-            ))\n-        \n+            suggestions.append(\n+                OptimizationSuggestion(\n+                    type=\"seo\",\n+                    priority=\"high\",\n+                    title=\"Add Meta Description\",\n+                    description=\"Missing meta description affects click-through rates\",\n+                    current_value=\"None\",\n+                    suggested_value=\"Add compelling description (120-160 characters)\",\n+                    impact=\"High - Improves click-through rates\",\n+                    effort=\"Medium - Requires copywriting\",\n+                )\n+            )\n+\n         return suggestions\n-    \n-    def _generate_readability_suggestions(self, analysis: ContentAnalysis) -> List[OptimizationSuggestion]:\n+\n+    def _generate_readability_suggestions(\n+        self, analysis: ContentAnalysis\n+    ) -> List[OptimizationSuggestion]:\n         \"\"\"Generate readability suggestions.\"\"\"\n         suggestions = []\n-        \n+\n         if analysis.readability_score < 30:\n-            suggestions.append(OptimizationSuggestion(\n-                type=\"readability\",\n-                priority=\"high\",\n-                title=\"Improve Readability\",\n-                description=\"Content is difficult to read\",\n-                current_value=f\"Score: {analysis.readability_score:.1f}\",\n-                suggested_value=\"Score: 50-70 (easier to read)\",\n-                impact=\"High - Better user experience\",\n-                effort=\"High - Requires content rewriting\",\n-                code_example=\"Use shorter sentences, simpler words, and active voice\"\n-            ))\n-        \n+            suggestions.append(\n+                OptimizationSuggestion(\n+                    type=\"readability\",\n+                    priority=\"high\",\n+                    title=\"Improve Readability\",\n+                    description=\"Content is difficult to read\",\n+                    current_value=f\"Score: {analysis.readability_score:.1f}\",\n+                    suggested_value=\"Score: 50-70 (easier to read)\",\n+                    impact=\"High - Better user experience\",\n+                    effort=\"High - Requires content rewriting\",\n+                    code_example=\"Use shorter sentences, simpler words, and active voice\",\n+                )\n+            )\n+\n         return suggestions\n-    \n-    def _generate_structure_suggestions(self, analysis: ContentAnalysis) -> List[OptimizationSuggestion]:\n+\n+    def _generate_structure_suggestions(\n+        self, analysis: ContentAnalysis\n+    ) -> List[OptimizationSuggestion]:\n         \"\"\"Generate content structure suggestions.\"\"\"\n         suggestions = []\n-        \n+\n         if len(analysis.h2s) < 2:\n-            suggestions.append(OptimizationSuggestion(\n-                type=\"structure\",\n-                priority=\"medium\",\n-                title=\"Improve Heading Structure\",\n-                description=\"Insufficient heading structure for good SEO\",\n-                current_value=f\"{len(analysis.h2s)} H2 tags\",\n-                suggested_value=\"At least 2-3 H2 tags\",\n-                impact=\"Medium - Better content organization\",\n-                effort=\"Medium - Requires content restructuring\"\n-            ))\n-        \n+            suggestions.append(\n+                OptimizationSuggestion(\n+                    type=\"structure\",\n+                    priority=\"medium\",\n+                    title=\"Improve Heading Structure\",\n+                    description=\"Insufficient heading structure for good SEO\",\n+                    current_value=f\"{len(analysis.h2s)} H2 tags\",\n+                    suggested_value=\"At least 2-3 H2 tags\",\n+                    impact=\"Medium - Better content organization\",\n+                    effort=\"Medium - Requires content restructuring\",\n+                )\n+            )\n+\n         return suggestions\n-    \n-    def _generate_keyword_suggestions(self, analysis: ContentAnalysis) -> List[OptimizationSuggestion]:\n+\n+    def _generate_keyword_suggestions(\n+        self, analysis: ContentAnalysis\n+    ) -> List[OptimizationSuggestion]:\n         \"\"\"Generate keyword optimization suggestions.\"\"\"\n         suggestions = []\n-        \n+\n         if not analysis.keyword_density:\n-            suggestions.append(OptimizationSuggestion(\n-                type=\"keyword\",\n-                priority=\"medium\",\n-                title=\"Add Target Keywords\",\n-                description=\"No clear keyword focus detected\",\n-                current_value=\"No keywords identified\",\n-                suggested_value=\"Add 2-3 target keywords\",\n-                impact=\"Medium - Better search targeting\",\n-                effort=\"Medium - Requires keyword research and content updates\"\n-            ))\n-        \n+            suggestions.append(\n+                OptimizationSuggestion(\n+                    type=\"keyword\",\n+                    priority=\"medium\",\n+                    title=\"Add Target Keywords\",\n+                    description=\"No clear keyword focus detected\",\n+                    current_value=\"No keywords identified\",\n+                    suggested_value=\"Add 2-3 target keywords\",\n+                    impact=\"Medium - Better search targeting\",\n+                    effort=\"Medium - Requires keyword research and content updates\",\n+                )\n+            )\n+\n         return suggestions\n-    \n-    def _generate_technical_suggestions(self, analysis: ContentAnalysis) -> List[OptimizationSuggestion]:\n+\n+    def _generate_technical_suggestions(\n+        self, analysis: ContentAnalysis\n+    ) -> List[OptimizationSuggestion]:\n         \"\"\"Generate technical SEO suggestions.\"\"\"\n         suggestions = []\n-        \n+\n         if len(analysis.internal_links) < 3:\n-            suggestions.append(OptimizationSuggestion(\n-                type=\"technical\",\n-                priority=\"low\",\n-                title=\"Add Internal Links\",\n-                description=\"Insufficient internal linking\",\n-                current_value=f\"{len(analysis.internal_links)} internal links\",\n-                suggested_value=\"At least 3-5 internal links\",\n-                impact=\"Low - Improves site structure\",\n-                effort=\"Low - Quick additions\"\n-            ))\n-        \n+            suggestions.append(\n+                OptimizationSuggestion(\n+                    type=\"technical\",\n+                    priority=\"low\",\n+                    title=\"Add Internal Links\",\n+                    description=\"Insufficient internal linking\",\n+                    current_value=f\"{len(analysis.internal_links)} internal links\",\n+                    suggested_value=\"At least 3-5 internal links\",\n+                    impact=\"Low - Improves site structure\",\n+                    effort=\"Low - Quick additions\",\n+                )\n+            )\n+\n         return suggestions\n+\n \n class ContentOptimizer:\n     \"\"\"Main class for content optimization.\"\"\"\n-    \n+\n     def __init__(self):\n         self.analyzer = ContentAnalyzer()\n         self.suggestion_generator = OptimizationSuggestionGenerator()\n-    \n-    def optimize_content(self, html_content: str, url: str) -> ContentOptimizationReport:\n+\n+    def optimize_content(\n+        self, html_content: str, url: str\n+    ) -> ContentOptimizationReport:\n         \"\"\"Analyze content and generate optimization report.\"\"\"\n-        \n+\n         # Analyze content\n         analysis = self.analyzer.analyze_content(html_content, url)\n-        \n+\n         # Generate suggestions\n         suggestions = self.suggestion_generator.generate_suggestions(analysis)\n-        \n+\n         # Generate priority actions\n         priority_actions = self._generate_priority_actions(suggestions)\n-        \n+\n         # Estimate impact and effort\n         estimated_impact = self._estimate_impact(suggestions)\n         effort_required = self._estimate_effort(suggestions)\n-        \n+\n         return ContentOptimizationReport(\n             url=url,\n             analysis=analysis,\n             suggestions=suggestions,\n             priority_actions=priority_actions,\n             estimated_impact=estimated_impact,\n             effort_required=effort_required,\n-            created_at=datetime.now().isoformat()\n-        )\n-    \n-    def _generate_priority_actions(self, suggestions: List[OptimizationSuggestion]) -> List[str]:\n+            created_at=datetime.now().isoformat(),\n+        )\n+\n+    def _generate_priority_actions(\n+        self, suggestions: List[OptimizationSuggestion]\n+    ) -> List[str]:\n         \"\"\"Generate priority action list.\"\"\"\n         high_priority = [s for s in suggestions if s.priority == \"high\"]\n         medium_priority = [s for s in suggestions if s.priority == \"medium\"]\n-        \n+\n         actions = []\n-        \n+\n         if high_priority:\n             actions.append(f\"Address {len(high_priority)} high-priority issues first\")\n-        \n+\n         if medium_priority:\n             actions.append(f\"Plan {len(medium_priority)} medium-priority improvements\")\n-        \n+\n         actions.append(\"Monitor performance after implementing changes\")\n-        \n+\n         return actions\n-    \n+\n     def _estimate_impact(self, suggestions: List[OptimizationSuggestion]) -> str:\n         \"\"\"Estimate overall impact of suggestions.\"\"\"\n         high_impact = len([s for s in suggestions if s.impact == \"High\"])\n         medium_impact = len([s for s in suggestions if s.impact == \"Medium\"])\n-        \n+\n         if high_impact >= 3:\n             return \"High - Significant SEO improvements expected\"\n         elif high_impact >= 1 or medium_impact >= 3:\n             return \"Medium - Moderate improvements expected\"\n         else:\n             return \"Low - Minor improvements expected\"\n-    \n+\n     def _estimate_effort(self, suggestions: List[OptimizationSuggestion]) -> str:\n         \"\"\"Estimate effort required for suggestions.\"\"\"\n         high_effort = len([s for s in suggestions if s.effort == \"High\"])\n         medium_effort = len([s for s in suggestions if s.effort == \"Medium\"])\n-        \n+\n         if high_effort >= 2:\n             return \"High - Requires significant content work\"\n         elif high_effort >= 1 or medium_effort >= 3:\n             return \"Medium - Moderate effort required\"\n         else:\n             return \"Low - Quick fixes and minor updates\"\n-    \n-    def export_report(self, report: ContentOptimizationReport, format: str = \"json\") -> str:\n+\n+    def export_report(\n+        self, report: ContentOptimizationReport, format: str = \"json\"\n+    ) -> str:\n         \"\"\"Export optimization report in specified format.\"\"\"\n         if format == \"json\":\n             return json.dumps(asdict(report), indent=2)\n         elif format == \"markdown\":\n             return self._generate_markdown_report(report)\n         else:\n             raise ValueError(f\"Unsupported format: {format}\")\n-    \n+\n     def _generate_markdown_report(self, report: ContentOptimizationReport) -> str:\n         \"\"\"Generate markdown report.\"\"\"\n         analysis = report.analysis\n-        \n+\n         md_content = f\"\"\"# Content Optimization Report\n \n ## URL: {report.url}\n **Generated**: {report.created_at}\n \n@@ -674,11 +807,11 @@\n ## Issues Identified\n \n \"\"\"\n         for issue in analysis.issues:\n             md_content += f\"- \u274c {issue}\\n\"\n-        \n+\n         md_content += \"\"\"\n ## Optimization Suggestions\n \n ### High Priority\n \"\"\"\n@@ -687,29 +820,29 @@\n             md_content += f\"- **{suggestion.title}**: {suggestion.description}\\n\"\n             md_content += f\"  - Current: {suggestion.current_value}\\n\"\n             md_content += f\"  - Suggested: {suggestion.suggested_value}\\n\"\n             md_content += f\"  - Impact: {suggestion.impact}\\n\"\n             md_content += f\"  - Effort: {suggestion.effort}\\n\\n\"\n-        \n+\n         md_content += \"\"\"\n ### Medium Priority\n \"\"\"\n         medium_priority = [s for s in report.suggestions if s.priority == \"medium\"]\n         for suggestion in medium_priority:\n             md_content += f\"- **{suggestion.title}**: {suggestion.description}\\n\"\n             md_content += f\"  - Current: {suggestion.current_value}\\n\"\n             md_content += f\"  - Suggested: {suggestion.suggested_value}\\n\"\n             md_content += f\"  - Impact: {suggestion.impact}\\n\"\n             md_content += f\"  - Effort: {suggestion.effort}\\n\\n\"\n-        \n+\n         md_content += \"\"\"\n ## Priority Actions\n \n \"\"\"\n         for action in report.priority_actions:\n             md_content += f\"- {action}\\n\"\n-        \n+\n         md_content += f\"\"\"\n ## Impact & Effort Summary\n \n - **Estimated Impact**: {report.estimated_impact}\n - **Effort Required**: {report.effort_required}\n@@ -719,16 +852,17 @@\n 1. Address high-priority issues first\n 2. Implement medium-priority improvements\n 3. Monitor performance metrics\n 4. Track improvements over time\n \"\"\"\n-        \n+\n         return md_content\n+\n \n def main():\n     \"\"\"Main function to demonstrate content optimization.\"\"\"\n-    \n+\n     # Sample HTML content for testing\n     sample_html = \"\"\"\n     <!DOCTYPE html>\n     <html>\n     <head>\n@@ -744,33 +878,34 @@\n         <h2>Conclusion</h2>\n         <p>Follow these tips to improve your SEO.</p>\n     </body>\n     </html>\n     \"\"\"\n-    \n+\n     # Initialize optimizer\n     optimizer = ContentOptimizer()\n-    \n+\n     # Analyze content\n     report = optimizer.optimize_content(sample_html, \"https://example.com/seo-tips\")\n-    \n+\n     # Export report\n     json_report = optimizer.export_report(report, \"json\")\n     markdown_report = optimizer.export_report(report, \"markdown\")\n-    \n+\n     # Save reports\n     with open(\"content_optimization_report.json\", \"w\") as f:\n         f.write(json_report)\n-    \n+\n     with open(\"content_optimization_report.md\", \"w\") as f:\n         f.write(markdown_report)\n-    \n+\n     print(\"Content optimization analysis complete!\")\n     print(f\"Overall Score: {report.analysis.overall_score:.1f}/100\")\n     print(f\"Issues Found: {len(report.analysis.issues)}\")\n     print(f\"Suggestions Generated: {len(report.suggestions)}\")\n     print(\"\\nFiles created:\")\n     print(\"- content_optimization_report.json\")\n     print(\"- content_optimization_report.md\")\n \n+\n if __name__ == \"__main__\":\n     main()\n--- /home/justin/llama_rag/app/seo-api/performance_tracker.py\t2025-09-29 06:31:23.221240+00:00\n+++ /home/justin/llama_rag/app/seo-api/performance_tracker.py\t2025-09-29 06:34:09.511417+00:00\n@@ -17,150 +17,158 @@\n logger = logging.getLogger(__name__)\n \n \n class MetricType(Enum):\n     \"\"\"Types of performance metrics.\"\"\"\n+\n     RANKING = \"ranking\"\n     TRAFFIC = \"traffic\"\n     CONVERSION = \"conversion\"\n     ENGAGEMENT = \"engagement\"\n     TECHNICAL = \"technical\"\n \n \n class PerformancePeriod(Enum):\n     \"\"\"Performance tracking periods.\"\"\"\n+\n     DAILY = \"daily\"\n     WEEKLY = \"weekly\"\n     MONTHLY = \"monthly\"\n     QUARTERLY = \"quarterly\"\n \n \n @dataclass\n class PerformanceMetric:\n     \"\"\"Individual performance metric data point.\"\"\"\n+\n     id: str\n     metric_type: MetricType\n     value: float\n     timestamp: str\n     metadata: Dict = field(default_factory=dict)\n \n \n @dataclass\n class ContentPerformance:\n     \"\"\"Content performance tracking data.\"\"\"\n+\n     content_id: str\n     url: str\n     title: str\n     target_keywords: List[str]\n     created_at: str\n     last_updated: str\n-    \n+\n     # Performance metrics\n     organic_traffic: List[PerformanceMetric]\n     ranking_positions: List[PerformanceMetric]\n     conversions: List[PerformanceMetric]\n     engagement_metrics: List[PerformanceMetric]\n-    \n+\n     # ROI calculations\n     estimated_value: float\n     implementation_cost: float\n     roi_percentage: float\n-    \n+\n     # Trend analysis\n     traffic_trend: str  # up, down, stable\n     ranking_trend: str\n     conversion_trend: str\n \n \n @dataclass\n class OpportunityPerformance:\n     \"\"\"SEO opportunity performance tracking.\"\"\"\n+\n     opportunity_id: str\n     title: str\n     target_keyword: str\n     implemented_at: str\n-    \n+\n     # Baseline metrics (before implementation)\n     baseline_traffic: float\n     baseline_ranking: Optional[int]\n     baseline_conversions: float\n-    \n+\n     # Current metrics\n     current_traffic: float\n     current_ranking: Optional[int]\n     current_conversions: float\n-    \n+\n     # Performance calculations\n     traffic_growth: float  # percentage\n     ranking_improvement: Optional[int]  # position improvement\n     conversion_growth: float  # percentage\n-    \n+\n     # ROI metrics\n     implementation_cost: float\n     revenue_generated: float\n     roi_percentage: float\n     payback_period_days: Optional[int]\n-    \n+\n     # Success indicators\n     is_successful: bool\n     success_factors: List[str]\n     areas_for_improvement: List[str]\n \n \n @dataclass\n class PerformanceSummary:\n     \"\"\"Overall performance summary.\"\"\"\n+\n     period_start: str\n     period_end: str\n-    \n+\n     # Overall metrics\n     total_opportunities_tracked: int\n     successful_opportunities: int\n     success_rate: float\n-    \n+\n     # Traffic metrics\n     total_traffic_growth: float\n     total_organic_traffic: float\n     traffic_growth_percentage: float\n-    \n+\n     # Ranking metrics\n     average_ranking_improvement: float\n     keywords_ranking_top_10: int\n     keywords_ranking_top_3: int\n-    \n+\n     # ROI metrics\n     total_investment: float\n     total_revenue: float\n     overall_roi: float\n-    \n+\n     # Top performers\n     top_performing_content: List[str]\n     top_performing_keywords: List[str]\n-    \n+\n     # Recommendations\n     recommendations: List[str]\n \n \n class PerformanceTracker:\n     \"\"\"SEO performance tracking and ROI measurement system.\"\"\"\n-    \n+\n     def __init__(self, storage_path: str = \"storage/seo/performance\"):\n         self.storage_path = Path(storage_path)\n         self.storage_path.mkdir(parents=True, exist_ok=True)\n-        \n+\n         # Performance data\n         self.content_performance: Dict[str, ContentPerformance] = {}\n         self.opportunity_performance: Dict[str, OpportunityPerformance] = {}\n-        \n+\n         # Load existing data\n         self._load_performance_data()\n-    \n-    def track_content_performance(self, content_id: str, metrics: Dict[str, float], timestamp: str = None):\n+\n+    def track_content_performance(\n+        self, content_id: str, metrics: Dict[str, float], timestamp: str = None\n+    ):\n         \"\"\"Track performance metrics for content.\"\"\"\n         if timestamp is None:\n             timestamp = datetime.now().isoformat()\n-        \n+\n         if content_id not in self.content_performance:\n             # Create new content performance record\n             self.content_performance[content_id] = ContentPerformance(\n                 content_id=content_id,\n                 url=f\"https://example.com/{content_id}\",\n@@ -175,187 +183,200 @@\n                 estimated_value=0.0,\n                 implementation_cost=0.0,\n                 roi_percentage=0.0,\n                 traffic_trend=\"stable\",\n                 ranking_trend=\"stable\",\n-                conversion_trend=\"stable\"\n-            )\n-        \n+                conversion_trend=\"stable\",\n+            )\n+\n         content = self.content_performance[content_id]\n         content.last_updated = timestamp\n-        \n+\n         # Add metrics\n         for metric_name, value in metrics.items():\n             metric = PerformanceMetric(\n                 id=f\"{content_id}_{metric_name}_{timestamp}\",\n                 metric_type=self._get_metric_type(metric_name),\n                 value=value,\n-                timestamp=timestamp\n-            )\n-            \n-            if metric_name in ['organic_traffic', 'page_views', 'unique_visitors']:\n+                timestamp=timestamp,\n+            )\n+\n+            if metric_name in [\"organic_traffic\", \"page_views\", \"unique_visitors\"]:\n                 content.organic_traffic.append(metric)\n-            elif metric_name in ['ranking_position', 'serp_position']:\n+            elif metric_name in [\"ranking_position\", \"serp_position\"]:\n                 content.ranking_positions.append(metric)\n-            elif metric_name in ['conversions', 'leads', 'sales']:\n+            elif metric_name in [\"conversions\", \"leads\", \"sales\"]:\n                 content.conversions.append(metric)\n             else:\n                 content.engagement_metrics.append(metric)\n-        \n+\n         # Calculate trends\n         self._calculate_trends(content)\n-        \n+\n         # Save data\n         self._save_performance_data()\n-    \n-    def track_opportunity_performance(self, opportunity_id: str, performance_data: Dict):\n+\n+    def track_opportunity_performance(\n+        self, opportunity_id: str, performance_data: Dict\n+    ):\n         \"\"\"Track performance of implemented SEO opportunities.\"\"\"\n         opportunity = OpportunityPerformance(\n             opportunity_id=opportunity_id,\n-            title=performance_data.get('title', f'Opportunity {opportunity_id}'),\n-            target_keyword=performance_data.get('target_keyword', ''),\n-            implemented_at=performance_data.get('implemented_at', datetime.now().isoformat()),\n-            \n+            title=performance_data.get(\"title\", f\"Opportunity {opportunity_id}\"),\n+            target_keyword=performance_data.get(\"target_keyword\", \"\"),\n+            implemented_at=performance_data.get(\n+                \"implemented_at\", datetime.now().isoformat()\n+            ),\n             # Baseline metrics\n-            baseline_traffic=performance_data.get('baseline_traffic', 0.0),\n-            baseline_ranking=performance_data.get('baseline_ranking'),\n-            baseline_conversions=performance_data.get('baseline_conversions', 0.0),\n-            \n+            baseline_traffic=performance_data.get(\"baseline_traffic\", 0.0),\n+            baseline_ranking=performance_data.get(\"baseline_ranking\"),\n+            baseline_conversions=performance_data.get(\"baseline_conversions\", 0.0),\n             # Current metrics\n-            current_traffic=performance_data.get('current_traffic', 0.0),\n-            current_ranking=performance_data.get('current_ranking'),\n-            current_conversions=performance_data.get('current_conversions', 0.0),\n-            \n+            current_traffic=performance_data.get(\"current_traffic\", 0.0),\n+            current_ranking=performance_data.get(\"current_ranking\"),\n+            current_conversions=performance_data.get(\"current_conversions\", 0.0),\n             # Performance calculations\n             traffic_growth=0.0,\n             ranking_improvement=None,\n             conversion_growth=0.0,\n-            \n             # ROI metrics\n-            implementation_cost=performance_data.get('implementation_cost', 0.0),\n-            revenue_generated=performance_data.get('revenue_generated', 0.0),\n+            implementation_cost=performance_data.get(\"implementation_cost\", 0.0),\n+            revenue_generated=performance_data.get(\"revenue_generated\", 0.0),\n             roi_percentage=0.0,\n             payback_period_days=None,\n-            \n             # Success indicators\n             is_successful=False,\n             success_factors=[],\n-            areas_for_improvement=[]\n+            areas_for_improvement=[],\n         )\n-        \n+\n         # Calculate performance metrics\n         self._calculate_opportunity_metrics(opportunity)\n-        \n+\n         self.opportunity_performance[opportunity_id] = opportunity\n         self._save_performance_data()\n-        \n+\n         return opportunity\n-    \n+\n     def _calculate_opportunity_metrics(self, opportunity: OpportunityPerformance):\n         \"\"\"Calculate performance metrics for an opportunity.\"\"\"\n         # Traffic growth\n         if opportunity.baseline_traffic > 0:\n             opportunity.traffic_growth = (\n-                (opportunity.current_traffic - opportunity.baseline_traffic) / \n-                opportunity.baseline_traffic * 100\n-            )\n-        \n+                (opportunity.current_traffic - opportunity.baseline_traffic)\n+                / opportunity.baseline_traffic\n+                * 100\n+            )\n+\n         # Ranking improvement\n         if opportunity.baseline_ranking and opportunity.current_ranking:\n-            opportunity.ranking_improvement = opportunity.baseline_ranking - opportunity.current_ranking\n-        \n+            opportunity.ranking_improvement = (\n+                opportunity.baseline_ranking - opportunity.current_ranking\n+            )\n+\n         # Conversion growth\n         if opportunity.baseline_conversions > 0:\n             opportunity.conversion_growth = (\n-                (opportunity.current_conversions - opportunity.baseline_conversions) / \n-                opportunity.baseline_conversions * 100\n-            )\n-        \n+                (opportunity.current_conversions - opportunity.baseline_conversions)\n+                / opportunity.baseline_conversions\n+                * 100\n+            )\n+\n         # ROI calculation\n         if opportunity.implementation_cost > 0:\n             opportunity.roi_percentage = (\n-                (opportunity.revenue_generated - opportunity.implementation_cost) / \n-                opportunity.implementation_cost * 100\n-            )\n-        \n+                (opportunity.revenue_generated - opportunity.implementation_cost)\n+                / opportunity.implementation_cost\n+                * 100\n+            )\n+\n         # Payback period\n         if opportunity.revenue_generated > 0 and opportunity.implementation_cost > 0:\n             daily_revenue = opportunity.revenue_generated / 365  # Assume annual revenue\n             if daily_revenue > 0:\n-                opportunity.payback_period_days = int(opportunity.implementation_cost / daily_revenue)\n-        \n+                opportunity.payback_period_days = int(\n+                    opportunity.implementation_cost / daily_revenue\n+                )\n+\n         # Success determination\n         opportunity.is_successful = (\n-            opportunity.traffic_growth > 20 and  # 20% traffic growth\n-            (opportunity.ranking_improvement and opportunity.ranking_improvement > 0) and\n-            opportunity.conversion_growth > 10 and  # 10% conversion growth\n-            opportunity.roi_percentage > 0\n+            opportunity.traffic_growth > 20  # 20% traffic growth\n+            and (\n+                opportunity.ranking_improvement and opportunity.ranking_improvement > 0\n+            )\n+            and opportunity.conversion_growth > 10  # 10% conversion growth\n+            and opportunity.roi_percentage > 0\n         )\n-        \n+\n         # Success factors\n         success_factors = []\n         if opportunity.traffic_growth > 50:\n             success_factors.append(\"High traffic growth\")\n         if opportunity.ranking_improvement and opportunity.ranking_improvement > 5:\n             success_factors.append(\"Significant ranking improvement\")\n         if opportunity.conversion_growth > 25:\n             success_factors.append(\"Strong conversion growth\")\n         if opportunity.roi_percentage > 100:\n             success_factors.append(\"Excellent ROI\")\n-        \n+\n         opportunity.success_factors = success_factors\n-        \n+\n         # Areas for improvement\n         improvements = []\n         if opportunity.traffic_growth < 10:\n             improvements.append(\"Improve content optimization\")\n         if opportunity.ranking_improvement and opportunity.ranking_improvement <= 0:\n             improvements.append(\"Focus on ranking improvement\")\n         if opportunity.conversion_growth < 5:\n             improvements.append(\"Optimize conversion funnel\")\n         if opportunity.roi_percentage < 50:\n             improvements.append(\"Reduce implementation costs\")\n-        \n+\n         opportunity.areas_for_improvement = improvements\n-    \n+\n     def _calculate_trends(self, content: ContentPerformance):\n         \"\"\"Calculate trend indicators for content performance.\"\"\"\n         # Traffic trend\n         if len(content.organic_traffic) >= 2:\n             recent_traffic = content.organic_traffic[-1].value\n             previous_traffic = content.organic_traffic[-2].value\n             if recent_traffic > previous_traffic * 1.1:\n                 content.traffic_trend = \"up\"\n             elif recent_traffic < previous_traffic * 0.9:\n                 content.traffic_trend = \"down\"\n-        \n+\n         # Ranking trend\n         if len(content.ranking_positions) >= 2:\n             recent_ranking = content.ranking_positions[-1].value\n             previous_ranking = content.ranking_positions[-2].value\n             if recent_ranking < previous_ranking:  # Lower number is better ranking\n                 content.ranking_trend = \"up\"\n             elif recent_ranking > previous_ranking:\n                 content.ranking_trend = \"down\"\n-        \n+\n         # Conversion trend\n         if len(content.conversions) >= 2:\n             recent_conversions = content.conversions[-1].value\n             previous_conversions = content.conversions[-2].value\n             if recent_conversions > previous_conversions * 1.05:\n                 content.conversion_trend = \"up\"\n             elif recent_conversions < previous_conversions * 0.95:\n                 content.conversion_trend = \"down\"\n-    \n+\n     def _get_metric_type(self, metric_name: str) -> MetricType:\n         \"\"\"Determine metric type from metric name.\"\"\"\n-        traffic_metrics = ['organic_traffic', 'page_views', 'unique_visitors', 'sessions']\n-        ranking_metrics = ['ranking_position', 'serp_position', 'keyword_rank']\n-        conversion_metrics = ['conversions', 'leads', 'sales', 'revenue']\n-        engagement_metrics = ['bounce_rate', 'time_on_page', 'pages_per_session']\n-        technical_metrics = ['page_speed', 'core_web_vitals', 'mobile_friendly']\n-        \n+        traffic_metrics = [\n+            \"organic_traffic\",\n+            \"page_views\",\n+            \"unique_visitors\",\n+            \"sessions\",\n+        ]\n+        ranking_metrics = [\"ranking_position\", \"serp_position\", \"keyword_rank\"]\n+        conversion_metrics = [\"conversions\", \"leads\", \"sales\", \"revenue\"]\n+        engagement_metrics = [\"bounce_rate\", \"time_on_page\", \"pages_per_session\"]\n+        technical_metrics = [\"page_speed\", \"core_web_vitals\", \"mobile_friendly\"]\n+\n         if metric_name in traffic_metrics:\n             return MetricType.TRAFFIC\n         elif metric_name in ranking_metrics:\n             return MetricType.RANKING\n         elif metric_name in conversion_metrics:\n@@ -364,267 +385,329 @@\n             return MetricType.ENGAGEMENT\n         elif metric_name in technical_metrics:\n             return MetricType.TECHNICAL\n         else:\n             return MetricType.ENGAGEMENT  # Default\n-    \n-    def generate_performance_summary(self, period_start: str, period_end: str) -> PerformanceSummary:\n+\n+    def generate_performance_summary(\n+        self, period_start: str, period_end: str\n+    ) -> PerformanceSummary:\n         \"\"\"Generate comprehensive performance summary for a period.\"\"\"\n         start_date = datetime.fromisoformat(period_start)\n         end_date = datetime.fromisoformat(period_end)\n-        \n+\n         # Filter opportunities in period\n         period_opportunities = [\n-            opp for opp in self.opportunity_performance.values()\n+            opp\n+            for opp in self.opportunity_performance.values()\n             if start_date <= datetime.fromisoformat(opp.implemented_at) <= end_date\n         ]\n-        \n-        successful_opportunities = [opp for opp in period_opportunities if opp.is_successful]\n-        \n+\n+        successful_opportunities = [\n+            opp for opp in period_opportunities if opp.is_successful\n+        ]\n+\n         # Calculate summary metrics\n         total_traffic_growth = sum(opp.traffic_growth for opp in period_opportunities)\n         total_organic_traffic = sum(opp.current_traffic for opp in period_opportunities)\n         total_investment = sum(opp.implementation_cost for opp in period_opportunities)\n         total_revenue = sum(opp.revenue_generated for opp in period_opportunities)\n-        \n+\n         # Calculate averages\n-        avg_ranking_improvement = statistics.mean([\n-            opp.ranking_improvement for opp in period_opportunities \n-            if opp.ranking_improvement is not None\n-        ]) if period_opportunities else 0\n-        \n+        avg_ranking_improvement = (\n+            statistics.mean(\n+                [\n+                    opp.ranking_improvement\n+                    for opp in period_opportunities\n+                    if opp.ranking_improvement is not None\n+                ]\n+            )\n+            if period_opportunities\n+            else 0\n+        )\n+\n         # Count ranking achievements\n-        keywords_top_10 = len([opp for opp in period_opportunities if opp.current_ranking and opp.current_ranking <= 10])\n-        keywords_top_3 = len([opp for opp in period_opportunities if opp.current_ranking and opp.current_ranking <= 3])\n-        \n+        keywords_top_10 = len(\n+            [\n+                opp\n+                for opp in period_opportunities\n+                if opp.current_ranking and opp.current_ranking <= 10\n+            ]\n+        )\n+        keywords_top_3 = len(\n+            [\n+                opp\n+                for opp in period_opportunities\n+                if opp.current_ranking and opp.current_ranking <= 3\n+            ]\n+        )\n+\n         # Top performers\n         top_performing_content = sorted(\n-            period_opportunities, \n-            key=lambda x: x.traffic_growth, \n-            reverse=True\n+            period_opportunities, key=lambda x: x.traffic_growth, reverse=True\n         )[:5]\n-        \n+\n         top_performing_keywords = sorted(\n-            period_opportunities,\n-            key=lambda x: x.roi_percentage,\n-            reverse=True\n+            period_opportunities, key=lambda x: x.roi_percentage, reverse=True\n         )[:5]\n-        \n+\n         # Generate recommendations\n         recommendations = self._generate_recommendations(period_opportunities)\n-        \n+\n         return PerformanceSummary(\n             period_start=period_start,\n             period_end=period_end,\n             total_opportunities_tracked=len(period_opportunities),\n             successful_opportunities=len(successful_opportunities),\n-            success_rate=len(successful_opportunities) / len(period_opportunities) * 100 if period_opportunities else 0,\n+            success_rate=(\n+                len(successful_opportunities) / len(period_opportunities) * 100\n+                if period_opportunities\n+                else 0\n+            ),\n             total_traffic_growth=total_traffic_growth,\n             total_organic_traffic=total_organic_traffic,\n-            traffic_growth_percentage=total_traffic_growth / len(period_opportunities) if period_opportunities else 0,\n+            traffic_growth_percentage=(\n+                total_traffic_growth / len(period_opportunities)\n+                if period_opportunities\n+                else 0\n+            ),\n             average_ranking_improvement=avg_ranking_improvement,\n             keywords_ranking_top_10=keywords_top_10,\n             keywords_ranking_top_3=keywords_top_3,\n             total_investment=total_investment,\n             total_revenue=total_revenue,\n-            overall_roi=(total_revenue - total_investment) / total_investment * 100 if total_investment > 0 else 0,\n+            overall_roi=(\n+                (total_revenue - total_investment) / total_investment * 100\n+                if total_investment > 0\n+                else 0\n+            ),\n             top_performing_content=[opp.title for opp in top_performing_content],\n-            top_performing_keywords=[opp.target_keyword for opp in top_performing_keywords],\n-            recommendations=recommendations\n+            top_performing_keywords=[\n+                opp.target_keyword for opp in top_performing_keywords\n+            ],\n+            recommendations=recommendations,\n         )\n-    \n-    def _generate_recommendations(self, opportunities: List[OpportunityPerformance]) -> List[str]:\n+\n+    def _generate_recommendations(\n+        self, opportunities: List[OpportunityPerformance]\n+    ) -> List[str]:\n         \"\"\"Generate actionable recommendations based on performance data.\"\"\"\n         recommendations = []\n-        \n+\n         # Analyze common success factors\n         success_factors = {}\n         for opp in opportunities:\n             if opp.is_successful:\n                 for factor in opp.success_factors:\n                     success_factors[factor] = success_factors.get(factor, 0) + 1\n-        \n+\n         if success_factors:\n             top_success_factor = max(success_factors, key=success_factors.get)\n-            recommendations.append(f\"Focus on {top_success_factor} - this was the most common success factor\")\n-        \n+            recommendations.append(\n+                f\"Focus on {top_success_factor} - this was the most common success factor\"\n+            )\n+\n         # Analyze common improvement areas\n         improvement_areas = {}\n         for opp in opportunities:\n             for area in opp.areas_for_improvement:\n                 improvement_areas[area] = improvement_areas.get(area, 0) + 1\n-        \n+\n         if improvement_areas:\n             top_improvement = max(improvement_areas, key=improvement_areas.get)\n-            recommendations.append(f\"Address {top_improvement} - this was the most common improvement area\")\n-        \n+            recommendations.append(\n+                f\"Address {top_improvement} - this was the most common improvement area\"\n+            )\n+\n         # ROI recommendations\n-        low_roi_opportunities = [opp for opp in opportunities if opp.roi_percentage < 50]\n+        low_roi_opportunities = [\n+            opp for opp in opportunities if opp.roi_percentage < 50\n+        ]\n         if low_roi_opportunities:\n-            recommendations.append(\"Review low-ROI opportunities and consider reallocating resources\")\n-        \n+            recommendations.append(\n+                \"Review low-ROI opportunities and consider reallocating resources\"\n+            )\n+\n         # Traffic growth recommendations\n         low_traffic_growth = [opp for opp in opportunities if opp.traffic_growth < 10]\n         if low_traffic_growth:\n-            recommendations.append(\"Improve content optimization for opportunities with low traffic growth\")\n-        \n+            recommendations.append(\n+                \"Improve content optimization for opportunities with low traffic growth\"\n+            )\n+\n         return recommendations\n-    \n+\n     def get_performance_insights(self) -> Dict[str, any]:\n         \"\"\"Get key performance insights and trends.\"\"\"\n         if not self.opportunity_performance:\n             return {\"message\": \"No performance data available\"}\n-        \n+\n         opportunities = list(self.opportunity_performance.values())\n-        \n+\n         insights = {\n             \"total_opportunities\": len(opportunities),\n-            \"success_rate\": len([opp for opp in opportunities if opp.is_successful]) / len(opportunities) * 100,\n-            \"average_traffic_growth\": statistics.mean([opp.traffic_growth for opp in opportunities]),\n-            \"average_roi\": statistics.mean([opp.roi_percentage for opp in opportunities]),\n-            \"best_performing_keyword\": max(opportunities, key=lambda x: x.traffic_growth).target_keyword,\n-            \"highest_roi_opportunity\": max(opportunities, key=lambda x: x.roi_percentage).title,\n-            \"quickest_payback\": min([opp for opp in opportunities if opp.payback_period_days], \n-                                  key=lambda x: x.payback_period_days, default=None)\n+            \"success_rate\": len([opp for opp in opportunities if opp.is_successful])\n+            / len(opportunities)\n+            * 100,\n+            \"average_traffic_growth\": statistics.mean(\n+                [opp.traffic_growth for opp in opportunities]\n+            ),\n+            \"average_roi\": statistics.mean(\n+                [opp.roi_percentage for opp in opportunities]\n+            ),\n+            \"best_performing_keyword\": max(\n+                opportunities, key=lambda x: x.traffic_growth\n+            ).target_keyword,\n+            \"highest_roi_opportunity\": max(\n+                opportunities, key=lambda x: x.roi_percentage\n+            ).title,\n+            \"quickest_payback\": min(\n+                [opp for opp in opportunities if opp.payback_period_days],\n+                key=lambda x: x.payback_period_days,\n+                default=None,\n+            ),\n         }\n-        \n+\n         return insights\n-    \n+\n     def _save_performance_data(self):\n         \"\"\"Save performance data to storage.\"\"\"\n         # Save content performance\n         content_file = self.storage_path / \"content_performance.json\"\n         content_data = {}\n         for content_id, content in self.content_performance.items():\n             content_data[content_id] = {\n-                'content_id': content.content_id,\n-                'url': content.url,\n-                'title': content.title,\n-                'target_keywords': content.target_keywords,\n-                'created_at': content.created_at,\n-                'last_updated': content.last_updated,\n-                'estimated_value': content.estimated_value,\n-                'implementation_cost': content.implementation_cost,\n-                'roi_percentage': content.roi_percentage,\n-                'traffic_trend': content.traffic_trend,\n-                'ranking_trend': content.ranking_trend,\n-                'conversion_trend': content.conversion_trend\n+                \"content_id\": content.content_id,\n+                \"url\": content.url,\n+                \"title\": content.title,\n+                \"target_keywords\": content.target_keywords,\n+                \"created_at\": content.created_at,\n+                \"last_updated\": content.last_updated,\n+                \"estimated_value\": content.estimated_value,\n+                \"implementation_cost\": content.implementation_cost,\n+                \"roi_percentage\": content.roi_percentage,\n+                \"traffic_trend\": content.traffic_trend,\n+                \"ranking_trend\": content.ranking_trend,\n+                \"conversion_trend\": content.conversion_trend,\n             }\n-        \n-        with open(content_file, 'w') as f:\n+\n+        with open(content_file, \"w\") as f:\n             json.dump(content_data, f, indent=2)\n-        \n+\n         # Save opportunity performance\n         opportunity_file = self.storage_path / \"opportunity_performance.json\"\n         opportunity_data = {}\n         for opp_id, opp in self.opportunity_performance.items():\n             opportunity_data[opp_id] = {\n-                'opportunity_id': opp.opportunity_id,\n-                'title': opp.title,\n-                'target_keyword': opp.target_keyword,\n-                'implemented_at': opp.implemented_at,\n-                'baseline_traffic': opp.baseline_traffic,\n-                'baseline_ranking': opp.baseline_ranking,\n-                'baseline_conversions': opp.baseline_conversions,\n-                'current_traffic': opp.current_traffic,\n-                'current_ranking': opp.current_ranking,\n-                'current_conversions': opp.current_conversions,\n-                'traffic_growth': opp.traffic_growth,\n-                'ranking_improvement': opp.ranking_improvement,\n-                'conversion_growth': opp.conversion_growth,\n-                'implementation_cost': opp.implementation_cost,\n-                'revenue_generated': opp.revenue_generated,\n-                'roi_percentage': opp.roi_percentage,\n-                'payback_period_days': opp.payback_period_days,\n-                'is_successful': opp.is_successful,\n-                'success_factors': opp.success_factors,\n-                'areas_for_improvement': opp.areas_for_improvement\n+                \"opportunity_id\": opp.opportunity_id,\n+                \"title\": opp.title,\n+                \"target_keyword\": opp.target_keyword,\n+                \"implemented_at\": opp.implemented_at,\n+                \"baseline_traffic\": opp.baseline_traffic,\n+                \"baseline_ranking\": opp.baseline_ranking,\n+                \"baseline_conversions\": opp.baseline_conversions,\n+                \"current_traffic\": opp.current_traffic,\n+                \"current_ranking\": opp.current_ranking,\n+                \"current_conversions\": opp.current_conversions,\n+                \"traffic_growth\": opp.traffic_growth,\n+                \"ranking_improvement\": opp.ranking_improvement,\n+                \"conversion_growth\": opp.conversion_growth,\n+                \"implementation_cost\": opp.implementation_cost,\n+                \"revenue_generated\": opp.revenue_generated,\n+                \"roi_percentage\": opp.roi_percentage,\n+                \"payback_period_days\": opp.payback_period_days,\n+                \"is_successful\": opp.is_successful,\n+                \"success_factors\": opp.success_factors,\n+                \"areas_for_improvement\": opp.areas_for_improvement,\n             }\n-        \n-        with open(opportunity_file, 'w') as f:\n+\n+        with open(opportunity_file, \"w\") as f:\n             json.dump(opportunity_data, f, indent=2)\n-    \n+\n     def _load_performance_data(self):\n         \"\"\"Load performance data from storage.\"\"\"\n         # Load content performance\n         content_file = self.storage_path / \"content_performance.json\"\n         if content_file.exists():\n             try:\n-                with open(content_file, 'r') as f:\n+                with open(content_file, \"r\") as f:\n                     content_data = json.load(f)\n-                \n+\n                 for content_id, data in content_data.items():\n                     self.content_performance[content_id] = ContentPerformance(\n-                        content_id=data['content_id'],\n-                        url=data['url'],\n-                        title=data['title'],\n-                        target_keywords=data['target_keywords'],\n-                        created_at=data['created_at'],\n-                        last_updated=data['last_updated'],\n+                        content_id=data[\"content_id\"],\n+                        url=data[\"url\"],\n+                        title=data[\"title\"],\n+                        target_keywords=data[\"target_keywords\"],\n+                        created_at=data[\"created_at\"],\n+                        last_updated=data[\"last_updated\"],\n                         organic_traffic=[],\n                         ranking_positions=[],\n                         conversions=[],\n                         engagement_metrics=[],\n-                        estimated_value=data['estimated_value'],\n-                        implementation_cost=data['implementation_cost'],\n-                        roi_percentage=data['roi_percentage'],\n-                        traffic_trend=data['traffic_trend'],\n-                        ranking_trend=data['ranking_trend'],\n-                        conversion_trend=data['conversion_trend']\n+                        estimated_value=data[\"estimated_value\"],\n+                        implementation_cost=data[\"implementation_cost\"],\n+                        roi_percentage=data[\"roi_percentage\"],\n+                        traffic_trend=data[\"traffic_trend\"],\n+                        ranking_trend=data[\"ranking_trend\"],\n+                        conversion_trend=data[\"conversion_trend\"],\n                     )\n             except Exception as e:\n                 logger.error(f\"Error loading content performance data: {e}\")\n-        \n+\n         # Load opportunity performance\n         opportunity_file = self.storage_path / \"opportunity_performance.json\"\n         if opportunity_file.exists():\n             try:\n-                with open(opportunity_file, 'r') as f:\n+                with open(opportunity_file, \"r\") as f:\n                     opportunity_data = json.load(f)\n-                \n+\n                 for opp_id, data in opportunity_data.items():\n                     self.opportunity_performance[opp_id] = OpportunityPerformance(\n-                        opportunity_id=data['opportunity_id'],\n-                        title=data['title'],\n-                        target_keyword=data['target_keyword'],\n-                        implemented_at=data['implemented_at'],\n-                        baseline_traffic=data['baseline_traffic'],\n-                        baseline_ranking=data['baseline_ranking'],\n-                        baseline_conversions=data['baseline_conversions'],\n-                        current_traffic=data['current_traffic'],\n-                        current_ranking=data['current_ranking'],\n-                        current_conversions=data['current_conversions'],\n-                        traffic_growth=data['traffic_growth'],\n-                        ranking_improvement=data['ranking_improvement'],\n-                        conversion_growth=data['conversion_growth'],\n-                        implementation_cost=data['implementation_cost'],\n-                        revenue_generated=data['revenue_generated'],\n-                        roi_percentage=data['roi_percentage'],\n-                        payback_period_days=data['payback_period_days'],\n-                        is_successful=data['is_successful'],\n-                        success_factors=data['success_factors'],\n-                        areas_for_improvement=data['areas_for_improvement']\n+                        opportunity_id=data[\"opportunity_id\"],\n+                        title=data[\"title\"],\n+                        target_keyword=data[\"target_keyword\"],\n+                        implemented_at=data[\"implemented_at\"],\n+                        baseline_traffic=data[\"baseline_traffic\"],\n+                        baseline_ranking=data[\"baseline_ranking\"],\n+                        baseline_conversions=data[\"baseline_conversions\"],\n+                        current_traffic=data[\"current_traffic\"],\n+                        current_ranking=data[\"current_ranking\"],\n+                        current_conversions=data[\"current_conversions\"],\n+                        traffic_growth=data[\"traffic_growth\"],\n+                        ranking_improvement=data[\"ranking_improvement\"],\n+                        conversion_growth=data[\"conversion_growth\"],\n+                        implementation_cost=data[\"implementation_cost\"],\n+                        revenue_generated=data[\"revenue_generated\"],\n+                        roi_percentage=data[\"roi_percentage\"],\n+                        payback_period_days=data[\"payback_period_days\"],\n+                        is_successful=data[\"is_successful\"],\n+                        success_factors=data[\"success_factors\"],\n+                        areas_for_improvement=data[\"areas_for_improvement\"],\n                     )\n             except Exception as e:\n                 logger.error(f\"Error loading opportunity performance data: {e}\")\n \n \n # Example usage and testing\n if __name__ == \"__main__\":\n     logging.basicConfig(level=logging.INFO)\n-    \n+\n     # Create performance tracker\n     tracker = PerformanceTracker()\n-    \n+\n     # Test content performance tracking\n-    tracker.track_content_performance(\"content_1\", {\n-        \"organic_traffic\": 1500,\n-        \"ranking_position\": 8,\n-        \"conversions\": 25,\n-        \"bounce_rate\": 0.45\n-    })\n-    \n+    tracker.track_content_performance(\n+        \"content_1\",\n+        {\n+            \"organic_traffic\": 1500,\n+            \"ranking_position\": 8,\n+            \"conversions\": 25,\n+            \"bounce_rate\": 0.45,\n+        },\n+    )\n+\n     # Test opportunity performance tracking\n     opportunity_data = {\n         \"title\": \"LS Engine Swap Guide\",\n         \"target_keyword\": \"LS engine swap guide\",\n         \"implemented_at\": \"2025-01-01T00:00:00\",\n@@ -633,21 +716,21 @@\n         \"baseline_conversions\": 2,\n         \"current_traffic\": 800,\n         \"current_ranking\": 5,\n         \"current_conversions\": 15,\n         \"implementation_cost\": 500,\n-        \"revenue_generated\": 2500\n+        \"revenue_generated\": 2500,\n     }\n-    \n+\n     opportunity = tracker.track_opportunity_performance(\"opp_1\", opportunity_data)\n     print(f\"\u2705 Opportunity tracked: {opportunity.title}\")\n     print(f\"   Traffic growth: {opportunity.traffic_growth:.1f}%\")\n     print(f\"   ROI: {opportunity.roi_percentage:.1f}%\")\n     print(f\"   Success: {opportunity.is_successful}\")\n-    \n+\n     # Generate performance insights\n     insights = tracker.get_performance_insights()\n     print(\"\\n\ud83d\udcca Performance Insights:\")\n     for key, value in insights.items():\n         print(f\"   {key}: {value}\")\n-    \n+\n     print(\"\\n\u2705 Performance tracking system is ready for production!\")\n"
    },
    "ruff": {
      "status": "FAIL",
      "details": "Linting issues: \u001b[1m\u001b[91mF841 \u001b[0m\u001b[1mLocal variable `index` is assigned to but never used\u001b[0m\n   \u001b[1m\u001b[94m-->\u001b[0m app/rag_api/main_optimized.py:202:9\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[94m200 |\u001b[0m             vector_store=vector_store, persist_dir=persist_dir\n\u001b[1m\u001b[94m201 |\u001b[0m         )\n\u001b[1m\u001b[94m202 |\u001b[0m         index = load_index_from_storage(storage, index_id=INDEX_ID)\n    \u001b[1m\u001b[94m|\u001b[0m         \u001b[1m\u001b[91m^^^^^\u001b[0m\n\u001b[1m\u001b[94m203 |\u001b[0m         index_status = \"loaded\"\n\u001b[1m\u001b[94m204 |\u001b[0m     except Exception as e:\n    \u001b[1m\u001b[94m|\u001b[0m\n\u001b[1m\u001b[96mhelp\u001b[0m: \u001b[1mRemove assignment to unused variable `index`\u001b[0m\n\nFound 1 error.\nNo fixes available (1 hidden fix can be enabled with the `--unsafe-fixes` option).\n"
    },
    "mypy": {
      "status": "WARN",
      "details": "Type checking warnings: app/assistants/main.py: error: Duplicate module named \"main\" (also at \"app/approval-app/main.py\")\napp/assistants/main.py: note: See https://mypy.readthedocs.io/en/stable/running_mypy.html#mapping-file-paths-to-modules for more info\napp/assistants/main.py: note: Common resolutions include: a) using `--exclude` to avoid checking one of them, b) adding `__init__.py` somewhere, c) using `--explicit-package-bases` or adjusting MYPYPATH\nFound 1 error in 1 file (errors prevented further checking)\n"
    }
  },
  "node_quality": {
    "tests": {
      "status": "FAIL",
      "details": "Tests failed: \u001b[33mThe CJS build of Vite's Node API is deprecated. See https://vite.dev/guide/troubleshooting.html#vite-cjs-node-api-deprecated for more details.\u001b[39m\n\u23af\u23af\u23af\u23af\u23af\u23af\u23af Failed Tests 1 \u23af\u23af\u23af\u23af\u23af\u23af\u23af\n\n FAIL  app/tests/unit/inventory-analytics.test.ts > VendorPerformanceAnalyzer > should analyze vendor performance\nAssertionError: expected 0 to be greater than 0\n \u276f app/tests/unit/inventory-analytics.test.ts:101:39\n     99|     expect(performance).toHaveLength(2);\n    100|     expect(performance[0]?.vendorId).toBe(\"vendor-1\");\n    101|     expect(performance[0]?.totalSkus).toBeGreaterThan(0);\n       |                                       ^\n    102|     expect(performance[0]?.overallScore).toBeGreaterThan(0);\n    103|     expect(performance[0]?.overallScore).toBeLessThanOrEqual(1);\n\n\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af\u23af[1/1]\u23af\n\n"
    },
    "lint": {
      "status": "PASS",
      "details": "Linting passed"
    },
    "build": {
      "status": "PASS",
      "details": "Build successful"
    }
  },
  "system_health": {
    "disk_space": {
      "status": "OK",
      "details": "Disk usage: 15%"
    },
    "memory": {
      "status": "OK",
      "details": "Memory: 2.9Gi/12Gi used"
    },
    "processes": {
      "status": "OK",
      "details": "Critical processes running: python, node"
    }
  },
  "security": {
    "dependencies": {
      "status": "UNKNOWN",
      "details": ""
    },
    "secrets": {
      "status": "CRITICAL",
      "details": "Potential hardcoded secrets found: 76 instances"
    },
    "permissions": {
      "status": "OK",
      "details": "File permissions look secure"
    }
  },
  "critical_issues": [
    "Python code formatting issues detected",
    "Python linting issues detected",
    "Node.js tests failed",
    "Potential hardcoded secrets detected"
  ],
  "overall_status": "CRITICAL"
}